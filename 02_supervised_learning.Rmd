# æ•™å¸«ã‚ã‚Šå­¦ç¿’

å‚™ãˆã¾ã™ã€‚

```{r setup, echo=FALSE}
knitr::opts_chunk$set(
  engine.path = "/usr/bin/python3",
  engine = "python",
  collapse = TRUE,
#  cache = TRUE,
  comment = " ##"
  )
```


```{python}
import numpy as np
import scipy as sp
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rc('font', family='IPAexGothic') # æ—¥æœ¬èªãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
import mglearn
```

## ã‚¯ãƒ©ã‚¹åˆ†é¡ã¨å›å¸°

æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¯ã•ã‚‰ã«2ã¤ã«åˆ†ã‘ã‚‰ã‚Œã‚‹ã€‚

- **ã‚¯ãƒ©ã‚¹åˆ†é¡**: ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã™ã‚‹å•é¡Œã€‚
    - 2ã‚¯ãƒ©ã‚¹åˆ†é¡ (binary classification): Yes/Noã¿ãŸã„ãª2æŠã€‚
        - ç‰‡æ–¹ã‚’**é™½æ€§** (positive)ã€ã‚‚ã†ç‰‡æ–¹ã‚’**é™°æ€§** (negative)ã¨ã™ã‚‹å ´åˆãŒã—ã°ã—ã°ã‚ã‚‹ã€‚
    - ä»–ã‚¯ãƒ©ã‚¹åˆ†é¡ (multiclass classification): ã‚‚ã£ã¨é¸æŠè‚¢å¤šã„ã‚„ã¤ã€‚
- **å›å¸°**: é€£ç¶šå€¤ã‚’äºˆæ¸¬ã™ã‚‹å•é¡Œã€‚

2ã¤ã‚’åŒºåˆ¥ã™ã‚‹ã®ã¯**å‡ºåŠ›**ãŒé€£ç¶šã‹ã©ã†ã‹ã€‚**å…¥åŠ›**ã¯ã©ã¡ã‚‰ã®å•é¡Œã§ã‚‚é€£ç¶šã®å ´åˆã‚‚é›¢æ•£çš„ãªå ´åˆã‚‚ã‚ã‚‹ã€‚

## æ±åŒ–ã€éå‰°é©åˆã€é©åˆä¸è¶³

- **æ±åŒ–èƒ½åŠ›**: æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿(è¨“ç·´ã«ä½¿ã£ã¦ãªã„ãƒ‡ãƒ¼ã‚¿)ã«å¯¾ã™ã‚‹æ­£ã—ã„å€¤ã‚’äºˆæ¸¬ã™ã‚‹èƒ½åŠ›ã€‚
- **éå‰°é©åˆ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã£ã¡ã‚ƒæ­£ç¢ºã«äºˆæ¸¬ã§ãã‚‹ã‘ã©æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã¦ã‚“ã§ãƒ€ãƒ¡ã¨ã„ã†çŠ¶æ…‹ã€‚
- **é©åˆä¸è¶³**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã™ã‚‰ã¡ã‚ƒã‚“ã¨äºˆæ¸¬ã§ãã¦ãªã„ã¨ã„ã†çŠ¶æ…‹ã€‚

ä¸€èˆ¬çš„ã«ã¯**ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡é›‘ã«ã™ã‚‹**ã»ã©è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«é©åˆã—ã¦ã„ãã€‚é©åˆä¸è¶³ã§ãªãã€éå‰°é©åˆã«ãªã‚‰ãªã„é©åº¦ãªãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã®æ™‚ã«æ±åŒ–èƒ½åŠ›ãŒæœ€å¤§ã«ãªã‚‹ã€‚ãã“ã‚’ç›®æŒ‡ãã†ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤§ãã•

- ãƒ¢ãƒ‡ãƒ«ãŒè¤‡é›‘ã§ã‚‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¤§ãã‘ã‚Œã°éå‰°é©åˆã‚’é¿ã‘ã‚‰ã‚Œã‚‹ã€‚
- é©åº¦ãªè¤‡é›‘ã•ã®ãƒ¢ãƒ‡ãƒ«ã¨ååˆ†ã«å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã†ã“ã¨ãŒæˆåŠŸã®ãƒã‚¤ãƒ³ãƒˆã€‚

## æ•™å¸«ã‚ã‚Šæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

### ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

- **äººå·¥çš„ãªå˜ç´”ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**ã¨ã€**å®Ÿä¸–ç•Œã®å‰²ã¨è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**ã‚’ä½¿ã†ã€‚

#### äººå·¥çš„ãªå˜ç´”ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

å˜ç´”ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯**mglearn**ã§ç”Ÿæˆã™ã‚‹ã€‚

- **forge**: `mglearn.datasets.make_forge()`ã§ç”Ÿæˆã™ã‚‹2ã‚¯ãƒ©ã‚¹åˆ†é¡å‘ã‘ãƒ‡ãƒ¼ã‚¿ã€‚
    - 2ã¤ã®ç‰¹å¾´é‡ã¨1ã¤ã®2å€¤ç›®çš„å¤‰æ•°ã‚’ã‚‚ã¤ã€‚

```{python}
X, y = mglearn.datasets.make_forge()
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.legend(["Class 0", "Class 1"], loc = 4) # å‡¡ä¾‹
plt.xlabel("ç¬¬1ç‰¹å¾´é‡")
plt.ylabel("ç¬¬2ç‰¹å¾´é‡")
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- **wave**: `mglearn.datasets.make_wave`ã§ç”Ÿæˆã™ã‚‹å›å¸°å‘ã‘ãƒ‡ãƒ¼ã‚¿ã€‚
    - 1ã¤ã®ç‰¹å¾´é‡ã¨1ã¤ã®ç›®çš„å¤‰æ•°ã‚’æŒã¤ã€‚

```{python}
X, y = mglearn.datasets.make_wave(n_samples = 40)
plt.plot(X, y, 'o')
plt.xlabel("ç‰¹å¾´é‡")
plt.ylabel("ç›®çš„å¤‰æ•°")
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

#### å®Ÿãƒ‡ãƒ¼ã‚¿

å®Ÿãƒ‡ãƒ¼ã‚¿ã¯**scikit-learn**ã«å…¥ã£ã¦ã‚‹ã‚‚ã®ã‚’ä½¿ã†ã€‚ç¬¬1ç« ã§ã‚‚èª¬æ˜ã—ãŸBunchã‚¯ãƒ©ã‚¹ã«ãªã£ã¦ã„ã‚‹ã€‚

- **cancer**: ã‚¦ã‚£ã‚¹ã‚³ãƒ³ã‚·ãƒ³ä¹³ç™Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    - ç›®çš„å¤‰æ•°ã¯è‰¯æ€§(benign)ã¨æ‚ªæ€§(malignant)ã®2å€¤ã€‚
    - ç‰¹å¾´é‡ã¯30ã€‚
    - ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯569ç‚¹ã€‚

```{python}
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
print(cancer.keys())
print(cancer.data.shape)
print(cancer.target_names)
print(np.bincount(cancer.target))
```

- **boston_housing**: 1970å¹´ä»£ã®ãƒœã‚¹ãƒˆãƒ³è¿‘éƒŠã®ä½å®…ä¾¡æ ¼ã€‚
    - ä½å®…ä¾¡æ ¼ã®ä¸­å¤®å€¤ãŒç›®çš„å¤‰æ•°ã€‚
    - ç‰¹å¾´é‡ã¯13ã€‚
    - ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯506ç‚¹ã€‚

```{python}
from sklearn.datasets import load_boston
boston = load_boston()
print(boston.data.shape)
print(boston.feature_names)
```

- ç‰¹å¾´é‡åŒå£«ã®ç©ã‚’æ±‚ã‚ãŸã‚Šã—ã¦ã€æ–°ã—ã„ç‰¹å¾´é‡ã‚’å°å‡ºã™ã‚‹ã“ã¨ã‚’**ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**ã¨å‘¼ã¶ã€‚
- **boston_housing**ã«å¯¾ã—ã€é‡è¤‡ã‚ã‚Šã§2ã¤ã®ç‰¹å¾´é‡ã®ç©ã‚’æ±‚ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ‹¡å¼µã‚’è©¦ã¿ã‚‹ã€‚
    - ä½œæ¥­ãŒé¢å€’ãªã®ã§æ—¢ã«æ‹¡å¼µã—ãŸã‚‚ã®ãŒ`mglearn.datasets.load_extended_boston()`ã§èª­ã¿è¾¼ã‚ã¾ã™ã€‚

```{python}
X, y = mglearn.datasets.load_extended_boston()
print(X.shape)
```


## ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 1 $k$-æœ€è¿‘å‚æ³•

- a.k.a. $k$-NN
- è¿‘ã„ã‚„ã¤ã¯å¤§ä½“ãŠã‚“ãªã˜ã€‚

### $k$-æœ€è¿‘å‚æ³•ã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹åˆ†é¡

- $k$ã¯å‚è€ƒã«ã™ã‚‹è¿‘å‚ç‚¹ã®å€‹æ•°ã€‚
- 1-NNã®ä¾‹ã€‚

```{python}
mglearn.plots.plot_knn_classification(n_neighbors=1)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- 3-NNã®ä¾‹
    - è¿‘å‚ç‚¹ãŒè¤‡æ•°ã®ã¨ãã¯å¤šæ•°æ±ºã§æ±ºã‚ã‚‹ã€‚

```{python}
mglearn.plots.plot_knn_classification(n_neighbors=3)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- **scikit-learn**ã§ã‚„ã‚‹ã€‚

```{python}
from sklearn.model_selection import train_test_split
X, y = mglearn.datasets.make_forge()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)
print(clf.score(X_test, y_test))
```

### KNeighborsClassifierã®è§£æ

- ç‰¹å¾´é‡ãŒ2ã¤ã—ã‹ãªã‘ã‚Œã°ã€æ•£å¸ƒå›³ãŒæã‘ã‚‹ã€‚
- æ•£å¸ƒå›³ä¸Šã®ã‚ã‚‰ã‚†ã‚‹ç®‡æ‰€ã«ã¤ã„ã¦**ã‚‚ã—ãã®å ´æ‰€ã«ç‚¹ãŒã‚ã£ãŸã‚‰**ã¨è€ƒãˆã¦åˆ¤åˆ¥ãŒã§ãã‚‹ã€‚
- ã¤ã¾ã‚Šã€ç‰¹å¾´é‡ãŒã¤ãã‚‹å¹³é¢ã‚’åˆ†é¡ã‚¯ãƒ©ã‚¹ã§å¡—ã‚Šåˆ†ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
- å¢ƒç•Œç·šã‚’**æ±ºå®šå¢ƒç•Œ**ã¨å‘¼ã¶ã€‚

```{python}
fig, axes = plt.subplots(1, 3, figsize = (10, 3))

for n, ax in zip([1, 3, 9], axes):
    clf = KNeighborsClassifier(n_neighbors = n).fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps = 0.5, ax = ax, alpha = .4)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax = ax)
    ax.set_title("{} neighbor(s)".format(n))
    ax.set_xlabel("ç‰¹å¾´é‡ 0")
    ax.set_ylabel("ç‰¹å¾´é‡ 1")

axes[0].legend(loc=3)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- è¿‘å‚ç‚¹ãŒå¤šã„ã»ã©å¢ƒç•ŒãŒãªã‚ã‚‰ã‹ = ãƒ¢ãƒ‡ãƒ«ã¯å˜ç´”ã«ãªã‚‹ã€‚
    - è¿‘å‚ç‚¹1 = æœ€ã‚‚è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«
    - è¿‘å‚ç‚¹æ•° = ãƒ‡ãƒ¼ã‚¿æ•° -> ãŸã ã®å¤šæ•°æ±º
- ã¨ã„ã†ã“ã¨ã¯è¿‘å‚ç‚¹æ•°ã®æ•°ã‚’å¢—ã‚„ã—ã¦ã„ãã¨ã€ã©ã“ã‹ã§æ±åŒ–èƒ½åŠ›ã®ãƒ”ãƒ¼ã‚¯ãŒâ€¦ï¼Ÿ
- **cancer**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©¦ã—ã¦ã¿ã‚‹ã€‚

```{python}
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
  cancer.data, cancer.target, stratify = cancer.target, random_state = 66
)

training_accuracy = []
test_accuracy = []
n_settings = range(1, 11)

for n in n_settings:
  clf = KNeighborsClassifier(n_neighbors = n).fit(X_train, y_train)
  training_accuracy.append(clf.score(X_train, y_train))
  test_accuracy.append(clf.score(X_test, y_test))

plt.plot(n_settings, training_accuracy, label = "è¨“ç·´ã‚»ãƒƒãƒˆç²¾åº¦")
plt.plot(n_settings, test_accuracy, label = "ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆç²¾åº¦")
plt.ylabel("ç²¾åº¦")
plt.xlabel("è¿‘å‚ç‚¹æ•°")
plt.legend()
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

### $k$-è¿‘å‚å›å¸°

- kNNã¯å›å¸°ã‚‚ã§ãã‚‹ã€‚
- 1-NNã§ã¯è¿‘å‚ç‚¹ã®å€¤ãŒæ–°ã—ã„è¦³æ¸¬å€¤ã«å¯¾å¿œã™ã‚‹å€¤ã ã¨è€ƒãˆã‚‹ã€‚

```{python}
mglearn.plots.plot_knn_regression(n_neighbors = 1)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

  
- è¿‘å‚ç‚¹ãŒè¤‡æ•°ã®æ™‚ã¯å¹³å‡å€¤ã‚’ä½¿ã†ã€‚

```{python}
mglearn.plots.plot_knn_regression(n_neighbors = 3)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- **scikit-learn**ã§ã¯ã€**KNeighborsRegressor**ã‚¯ãƒ©ã‚¹ã«å®Ÿè£…ã•ã‚Œã¦ã‚‹ã€‚

```{python}
from sklearn.neighbors import KNeighborsRegressor

X, y = mglearn.datasets.make_wave(n_samples = 40)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

reg = KNeighborsRegressor(n_neighbors = 3).fit(X_train, y_train)

print(reg.score(X_test, y_test))
```

### KNeighborsRegressorã®è§£æ

- 1æ¬¡å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹äºˆæ¸¬å€¤ã¯ã€è¿‘å‚ç‚¹æ•°$k$ã«å¯¾ã—ã¦ã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ï¼Ÿ

```{python}
# ãƒ—ãƒ­ãƒƒãƒˆå…ˆã‚’3ã¤ä½œã‚‹
fig, axes = plt.subplots(1, 3, figsize = (15, 4))
# -3ã€œ3ã¾ã§ã®é–“ã«ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’1000ç‚¹ä½œã‚‹
line = np.linspace(-3, 3, 1000).reshape(-1, 1)
for n_neighbors, ax in zip([1, 3, 9], axes):
  reg = KNeighborsRegressor(n_neighbors = n_neighbors)
  reg.fit(X_train, y_train)
  ax.plot(line, reg.predict(line))
  ax.plot(X_train, y_train, '^')
  ax.plot(X_test, y_test, 'v')
  ax.set_title(
    "{} è¿‘å‚ç‚¹\n è¨“ç·´ã‚¹ã‚³ã‚¢: {:.2f} ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢{:.2f}".format(
      n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test)))
  ax.set_xlabel("ç‰¹å¾´é‡")
  ax.set_ylabel("ç›®çš„å¤‰æ•°")
  
axes[0].legend(["ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬å€¤", "è¨“ç·´ãƒ‡ãƒ¼ã‚¿", "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿"], loc="best")
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- $k=1$ã®å ´åˆã¯äºˆæ¸¬å€¤ãŒå…¨ã¦ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’é€šã‚‹ã®ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒä¸å®‰å®šã«ãªã‚‹ã€‚
- è¿‘å‚ç‚¹ã‚’å¢—ã‚„ã—ã¦ã„ãã¨äºˆæ¸¬ã¯æ»‘ã‚‰ã‹ã«ãªã‚‹ãŒã€ãã®åé¢è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¸ã®é©åˆåº¦ãŒä¸‹ãŒã‚‹ã€‚

### åˆ©ç‚¹ã¨æ¬ ç‚¹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- åˆ©ç‚¹
    - ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã—ã‚„ã™ã„ã€‚
    - ã‚ã¾ã‚Šèª¿æ•´ã—ãªãã¦ã‚‚æ€§èƒ½ãŒå‡ºã‚„ã™ã„ã€‚
    - ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¯é«˜é€Ÿ
- æ¬ ç‚¹
    - è¨“ç·´ã‚»ãƒƒãƒˆãŒå¤§ãããªã‚‹ã¨äºˆæ¸¬ãŒé…ããªã‚‹ã€‚
        - å®Ÿéš›ã«ä½¿ã†å‰ã«ã¯å‰å‡¦ç†ã‚’è¡Œã†ã“ã¨ãŒé‡è¦ã€‚
    - ç–ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ç‰¹å¾´é‡ã®å¤šããŒ0ã§ã‚ã‚‹)ã«å¯¾ã—ã¦ã¯ååˆ†ãªæ€§èƒ½ãŒå‡ºã«ãã„ã€‚

ä¸Šè¨˜ã®ç†ç”±ã‹ã‚‰ã€kNNã¯å®Ÿéš›ã«ä½¿ã‚ã‚Œã‚‹ã“ã¨ã¯å°‘ãªã„ã€‚

## ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 2 ç·šå½¢ãƒ¢ãƒ‡ãƒ«

### ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å›å¸°

ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬å¼ã¯...

$$\hat{y} = w[0]\times x[0] + w[1]\times x[1] + ... + w[p]\times x[p] + b$$

- $\hat{y}$ã¯äºˆæ¸¬å€¤ã§ã€$w$ã¨$b$ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚$x$ã¯ã‚ã‚‹ä¸€ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å¾´é‡ã€‚
- äºˆæ¸¬å€¤ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’é©å½“ã«é‡ã¿ä»˜ã‘ã—ãŸã‚‚ã®ã€ã¨è¦‹ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã€‚

**wave**ã«ç·šå½¢å›å¸°ã‚’é©ç”¨ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã‚ˆã†ã€‚

```{python}
mglearn.plots.plot_linear_regression_wave()
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ãŸå›å¸°ã«ã¯ã„ã‚ã„ã‚ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒã‚ã£ã¦ã€ãã‚Œãã‚Œä»¥ä¸‹ã®ç‚¹ã§ç•°ãªã£ã¦ã„ã‚‹ã€‚

- ã©ã®ã‚ˆã†ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$w$ã¨$b$ã‚’å­¦ç¿’ã™ã‚‹ã‹ã€‚
- ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã‚’ã©ã®ã‚ˆã†ã«åˆ¶å¾¡ã™ã‚‹ã®ã‹ã€‚

### ç·šå½¢å›å¸°(é€šå¸¸æœ€å°äºŒä¹—æ³•)

- äºˆæ¸¬å€¤ã¨çœŸå€¤ã®**å¹³å‡äºŒä¹—èª¤å·®** (mean squared error) ã‚’æœ€å°ã«ã™ã‚‹ã‚ˆã†ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±‚ã‚ã‚‹ã€‚
- ç·šå½¢å›å¸°ã«ã¯è¤‡é›‘ã•ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãªã„ã€‚ã§ããªã„ã€‚

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
X, y = mglearn.datasets.make_wave(n_samples = 60)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)
lr = LinearRegression().fit(X_train, y_train)
```

- $w$ã¯**ä¿‚æ•°** (coefficient)ã¨å‘¼ã°ã‚Œã€`coef_`ã«æ ¼ç´ã•ã‚Œã‚‹ã€‚
- $b$ã¯**åˆ‡ç‰‡** (intercept)ã¨å‘¼ã°ã‚Œã€`intercept_`ã«æ ¼ç´ã•ã‚Œã‚‹ã€‚

```{python}
print(lr.coef_)
print(lr.intercept_)
```

- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸå±æ€§ã«ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ä»˜ã‘ã‚‹ã®ã¯**scikit-learn**ã®æ…£ç¿’ã§ã‚ã‚‹ã€‚
- `coef_`ã¯ç‰¹å¾´é‡1ã¤ã«å¯¾ã—ã¦1ã¤ã®å€¤ã‚’ã‚‚ã¤NumPyé…åˆ—ã¨ãªã‚‹ã€‚
- ç·šå½¢å›å¸°ã®æ€§èƒ½ã¯æ±ºå®šä¿‚æ•°$R^2$ã¨ã—ã¦æ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚

```{python}
print(lr.score(X_train, y_train))
print(lr.score(X_test, y_test))
```

ã“ã“ã§è¨“ç·´ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®$R^2$ãŒã‚ã‚“ã¾ã‚Šé•ã‚ãªã„ã®ã¯ï¼ˆäºˆæ¸¬æ€§èƒ½ã¯ã¨ã‚‚ã‹ãï¼‰éå‰°é©åˆã—ã¦ã„ãªã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚é€šå¸¸ã€ç‰¹å¾´é‡ãŒå¤šã„ã»ã©éå‰°é©åˆã®ãƒªã‚¹ã‚¯ãŒé«˜ã¾ã‚‹ã€‚æ‹¡å¼µã—ãŸ**boston_housing**ã§ç¢ºèªã—ã¦ã¿ã‚ˆã†ã€‚

```{python}
X, y = mglearn.datasets.load_extended_boston()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
lr = LinearRegression().fit(X_train, y_train)
```

$R^2$ã‚’è¨“ç·´ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æ¯”è¼ƒã—ã¦ã¿ã‚ˆã†ã€‚

```{python}
print(lr.score(X_train, y_train))
print(lr.score(X_test, y_test))
```

ä¸¡è€…ã«ä¹–é›¢ãŒè¦‹ã‚‰ã‚Œã‚‹ã®ã¯ã€éå‰°é©åˆã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã‚’åˆ¶å¾¡ã§ãã‚Œã°è‰¯ã„ã®ã ãŒã€ç·šå½¢å›å¸°ã«ã¯ãã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãªã„ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å°å…¥ã™ã‚‹æ–¹æ³•ã¨ã—ã¦**ãƒªãƒƒã‚¸å›å¸°**ãŒã‚ã‚‹ã€‚

### ãƒªãƒƒã‚¸å›å¸°

- ä¿‚æ•°ãŒå¤šã„ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãŒè¤‡é›‘ã«ãªã‚‹ã€‚
- ä¿‚æ•°ãŒ0ï¼ãã®ä¿‚æ•°ã‚’è€ƒæ…®ã—ãªã„ã€‚
- ä¿‚æ•°ãŒå°ã•ã‘ã‚Œã°ãƒ¢ãƒ‡ãƒ«ã¯å˜ç´”ã«ãªã‚‹ã®ã§ã¯ğŸ¤”
    - æ¥µç«¯ãªè©±ä¿‚æ•°ãŒå…¨éƒ¨ã‚¼ãƒ­ãªã‚‰å…¥åŠ›ã«é–¢ã‚ã‚‰ãšä¸€å®šã®å€¤(å¹³å‡ã¨ã‹)ã‚’å‡ºåŠ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ãªã‚‹ã€‚
- ä¿‚æ•°ãƒ™ã‚¯ãƒˆãƒ«ã®é•·ã•ã‚’æœ€å°åŒ–ã—ã‚ˆã†ï¼â†’ãƒªãƒƒã‚¸å›å¸°

```{python}
from sklearn.linear_model import Ridge
ridge = Ridge().fit(X_train, y_train) # ãƒ‡ãƒ¼ã‚¿ã¯æ‹¡å¼µBoston housingã®ã¾ã¾
print(ridge.score(X_train, y_train))
print(ridge.score(X_test, y_test))
```

- è¨“ç·´ã‚»ãƒƒãƒˆã¸ã®äºˆæ¸¬èƒ½åŠ›ãŒä¸‹ãŒã£ãŸã‘ã©ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã¸ã®äºˆæ¸¬èƒ½åŠ›ãŒä¸ŠãŒã£ãŸï¼
    - ãƒ¢ãƒ‡ãƒ«ã‚’å˜ç´”ã«ã™ã‚‹ã“ã¨ã§æ±åŒ–èƒ½åŠ›ãŒä¸ŠãŒã£ã¦ã„ã‚‹ã€‚
- ãƒªãƒƒã‚¸å›å¸°ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å˜ç´”ã•ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: $\alpha$
    - å¤§ãã„ã»ã©åˆ¶ç´„ãŒå¼·ã„ = ãƒ¢ãƒ‡ãƒ«ãŒå˜ç´”ã«ãªã‚‹
    - sklearnã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0
    - ä½•ãŒè‰¯ã„ã‹ã¯ãƒ‡ãƒ¼ã‚¿æ¬¡ç¬¬ã§ã€è‡ªå‹•çš„ã«ã¯èª¿æ•´ã•ã‚Œãªã„ï¼ˆå¾Œã§å¤šåˆ†ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ãŒå‡ºã¦æ¥ã‚‹ï¼‰ã€‚
    
```{python}
### alphaã‚’10å€ã«ã—ã¦ã¿ã‚‹ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆæ™‚ã«æŒ‡å®š
ridge10 = Ridge(alpha = 10).fit(X_train, y_train)
print(ridge10.score(X_train, y_train))
print(ridge10.score(X_test, y_test))

### alphaã‚’0.1å€ã«ã—ã¦ã¿ã‚‹ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆæ™‚ã«æŒ‡å®š
ridge01 = Ridge(alpha = .1).fit(X_train, y_train)
print(ridge01.score(X_train, y_train))
print(ridge01.score(X_test, y_test))
```

$\alpha$ã®å¤§ãã•ã¨ä¿‚æ•°ã®é–¢ä¿‚ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã‚‹ã€‚$\alpha$ãŒå¤§ãã„ã»ã©ä¿‚æ•°ã®çµ¶å¯¾å€¤ã¯å°ã•ããªã‚‹ã¯ãšâ€¦

```{python}
plt.plot(ridge.coef_, 's', label="Ridge alpha=1")
plt.plot(ridge10.coef_, '^', label="Ridge alpha=10")
plt.plot(ridge01.coef_, 'v', label="Ridge alpha=0.1")
plt.plot(lr.coef_, 'o', label="LinearRegression")
plt.xlabel("ä¿‚æ•°ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹")
plt.ylabel("ä¿‚æ•°ã®å€¤")
plt.hlines(0, 0, len(lr.coef_))
plt.ylim(-25, 25)
plt.legend()
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã—ã¦ã„ãã¨ã‚¹ã‚³ã‚¢ã¯ã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ï¼Ÿ
    - **å­¦ç¿’æ›²ç·š** (learning curve): ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã¨ã®é–¢ä¿‚ã§è¡¨ã—ãŸã‚‚ã®ã€‚
    - ãƒªãƒƒã‚¸å›å¸°ã¯æ­£å‰‡åŒ–ã®å½±éŸ¿ã§å¸¸ã«ç·šå½¢å›å¸°ã‚ˆã‚Šè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¸ã®é©åˆãŒä½ã„ã€‚
    - ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã¸ã®é©åˆã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºãŒå°ã•ã„ã†ã¡ã¯ãƒªãƒƒã‚¸å›å¸°ã®æ–¹ãŒå„ªã‚Œã‚‹ã€‚
    - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã¨ã€ãƒªãƒƒã‚¸å›å¸°ã¨ç·šå½¢å›å¸°ã®å·®ã¯ãªããªã‚‹ã€‚
        - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã¨ã€(å˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã§ã¯)éå‰°é©åˆã™ã‚‹ã“ã¨ãŒé›£ã—ããªã‚‹ã€‚

```{python}
mglearn.plots.plot_ridge_n_samples()
plt.xlabel("è¨“ç·´ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚º")
plt.ylabel("ã‚¹ã‚³ã‚¢(RÂ²)")
plt.legend(labels=["ãƒªãƒƒã‚¸ è¨“ç·´ã‚»ãƒƒãƒˆ", "ãƒªãƒƒã‚¸ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ", "ç·šå½¢å›å¸° è¨“ç·´ã‚»ãƒƒãƒˆ", "ç·šå½¢å›å¸° ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ"])
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

### Lasso

- Ridgeã¨ã¯ç•°ãªã‚‹å½¢ã§ä¿‚æ•°ã«åˆ¶ç´„ã‚’ã‹ã‘ã‚‹ç·šå½¢å›å¸°ã€‚
    - L1æ­£å‰‡åŒ–: L1ãƒãƒ«ãƒ ã€ã¤ã¾ã‚Šä¿‚æ•°ã®çµ¶å¯¾å€¤ã®å’Œã«åˆ¶ç´„ã‚’ã‹ã‘ã‚‹ã€‚
- **ã„ãã¤ã‹ã®ä¿‚æ•°ãŒå®Œå…¨ã«0ã«ãªã‚‹å ´åˆãŒã‚ã‚‹**ã¨ã„ã†ç‚¹ãŒRidgeã¨å¤§ããç•°ãªã‚‹ã€‚
    - ä¿‚æ•°ãŒå®Œå…¨ã«0=ä¿‚æ•°ã‚’é™¤å¤–ã—ã¦ã„ã‚‹ã¨ã„ã†ã“ã¨ãªã®ã§ã€**è‡ªå‹•çš„ãªå¤‰æ•°é¸æŠ**ã¨ã‚‚ã¿ãªã›ã‚‹ã€‚
    - å¤‰æ•°ãŒæ¸›ã‚Œã°ãƒ¢ãƒ‡ãƒ«ã‚’è§£é‡ˆã—ã‚„ã™ããªã‚‹ã¨ã„ã†åˆ©ç‚¹ã‚‚ã‚ã‚‹ã€‚
    
Lassoã‚’**boston_housing**ã«é©ç”¨ã™ã‚‹ã€‚

```{python}
from sklearn.linear_model import Lasso

lasso = Lasso().fit(X_train, y_train)
print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢: {:.2f}".format(lasso.score(X_train, y_train)))
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢: {:.2f}".format(lasso.score(X_test, y_test)))
print("é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {}".format(np.sum(lasso.coef_ != 0)))
```

- ã‚¹ã‚³ã‚¢ãŒéå¸¸ã«æ‚ªã„ã®ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…¨ããƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ãªã„ã“ã¨ã«ã‚ˆã‚‹ã€‚
- Lassoã«ã¯è¤‡é›‘ã•ã®åº¦åˆã„ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿`alpha`ãŒã‚ã‚‹ã€‚`alpha`ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã§ã€å°ã•ãã™ã‚‹ã»ã©è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã«ãªã‚‹ã€‚
- `alpha`ã‚’æ‰‹å‹•ã§æ¸›ã‚‰ã™éš›ã«ã¯ã€åˆã‚ã›ã¦`max_iter`ã‚’å¢—ã‚„ã—ã¦ã‚„ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

```{python}
lasso001 = Lasso(alpha = 0.01, max_iter=100000).fit(X_train, y_train)
print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢: {:.2f}".format(lasso001.score(X_train, y_train)))
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢: {:.2f}".format(lasso001.score(X_test, y_test)))
print("é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {}".format(np.sum(lasso001.coef_ != 0)))
```

- `alpha`ã‚’å°ã•ãã—ã™ãã‚‹ã¨éå‰°é©åˆã™ã‚‹ã€‚

```{python}
lasso00001 = Lasso(alpha = 0.0001, max_iter=100000).fit(X_train, y_train)
print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢: {:.2f}".format(lasso00001.score(X_train, y_train)))
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢: {:.2f}".format(lasso00001.score(X_test, y_test)))
print("é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {}".format(np.sum(lasso00001.coef_ != 0)))
```


Ridgeã§ã‚„ã£ãŸã‚ˆã†ã«ä¿‚æ•°ã®å¤§ãã•ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã‚ˆã†ã€‚

```{python}
plt.plot(lasso.coef_, 's', label = "Lasso alpha = 1")
plt.plot(lasso001.coef_, '^', label = "Lasso alpha = 0.01")
plt.plot(lasso00001.coef_, 'v', label = "Lasso alpha = 0.0001")
plt.plot(ridge01.coef_, 'o', label = "Ridge alpha = 0.1")
plt.legend(ncol = 2, loc = (0, 1.05))
plt.ylim = (-25, 25)
plt.xlabel("ä¿‚æ•°ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹")
plt.ylabel("ä¿‚æ•°ã®å¤§ãã•")
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

- åˆã‚ã›ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ãŸRidge($\alpha=0.1$)ã¯ã€Lasso($\alpha=0.01$)ã¨åŒã˜ãã‚‰ã„ã®æ€§èƒ½ã§ã‚ã‚‹ãŒã€Ridgeã§ã¯å¤§ãã•ãŒå°ã•ã„ãªãŒã‚‰ã‚‚ä¿‚æ•°ã®å€¤ã¯0ã«ã¯ãªã£ã¦ã„ãªã„ã‚‚ã®ãŒå¤šã„ã®ã«å¯¾ã—ã¦ã€Lassoã§ã¯å¤§ãã•ãŒ0ã®ä¿‚æ•°ãŒç›®ç«‹ã¤ã€‚
- å®Ÿéš›ã«ã¯ã¾ãšRidgeã‚’è©¦ã™ã¨è‰¯ã„ã€‚
- ä¿‚æ•°ãŒãŸãã•ã‚“ã‚ã£ã¦é‡è¦ãªã®ã¯ãã®ã†ã¡ã®å¹¾ã¤ã‹å°‘æ•°ã§ã‚ã‚‹ã¨äºˆæƒ³ã•ã‚Œã‚‹ã®ã§ã‚ã‚Œã°ã€Lassoã‚’è©¦ã™ã¨è‰¯ã„ã€‚
- Ridgeã¨Lassoã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’çµ„åˆã›ãŸã‚‚ã®ã¨ã—ã¦ElasticNetãŒã‚ã‚‹ã€‚çµæœã¯è‰¯å¥½ã§ã‚ã‚‹ãŒã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¢—ãˆã‚‹ã¨ã„ã†æ¬ ç‚¹ãŒã‚ã‚‹ã€‚

### ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ãŸã‚ã®ç·šå½¢ãƒ¢ãƒ‡ãƒ«

ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’è¡Œã†å ´åˆã¯ä»¥ä¸‹ã®å¼ã‚’ç”¨ã„ã‚‹ã€‚

$$\hat{y} = w[0]\times x[0] + w[1]\times x[1] + \dots + w[p]\times x[p] + b > 0$$

- å‡ºåŠ›$y$ãŒ0ã‚’è¶…ãˆã‚‹ã‹ã©ã†ã‹ã§åˆ¤åˆ¥ã™ã‚‹ã€‚
- å‡ºåŠ›$y$ã¯ç‰¹å¾´é‡ã®ç·šå½¢é–¢æ•°ã§ã‚ã‚Šã€2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚’ç›´ç·šã‚„å¹³é¢ã€è¶…å¹³é¢ã§åˆ†å‰²ã™ã‚‹**æ±ºå®šå¢ƒç•Œ**ã¨ãªã‚‹ã€‚
- ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰åˆ†é¡ã•ã‚Œã‚‹ã€‚
    - ã©ã®ã‚ˆã†ãªå°ºåº¦ã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¸ã®é©åˆåº¦ã‚’æ¸¬ã‚‹ã‹ã€‚
    - æ­£å‰‡åŒ–ã‚’è¡Œã†ã‹ã€‚è¡Œã†ãªã‚‰ã©ã®ã‚ˆã†ãªæ–¹æ³•ã‹ã€‚
- **ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°**ã¨**ç·šå½¢ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³**ã¯ä¸€èˆ¬çš„ãªç·šå½¢ã‚¯ãƒ©ã‚¹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã€‚

**LogisticRegression**ã¨**LinearSVC**ã«ã‚ˆã‚Š**forge**ã‚’åˆ†é¡ã™ã‚‹æ±ºå®šå¢ƒç•Œã‚’å¯è¦–åŒ–ã™ã‚‹ã€‚

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize = (10, 3))

for model, ax in zip([LinearSVC(), LogisticRegression()], axes):
  clf = model.fit(X, y)
  mglearn.plots.plot_2d_separator(clf, X, fill = False, eps = 0.5, ax = ax, alpha = 0.7)
  mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax = ax)
  ax.set_title("{}".format(clf.__class__.__name__))
  ax.set_xlabel("ç‰¹å¾´é‡ 0")
  ax.set_ylabel("ç‰¹å¾´é‡ 1")
  
axes[0].legend()
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

- 2ã¤ã®ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨ã¯ã„ãšã‚Œã‚‚æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Cã‚’æŒã¤ã€‚Cã¯å¤§ãã„ã»ã©æ­£å‰‡åŒ–ãŒå¼±ããªã‚‹ã€‚
- CãŒã¯å°ã•ã„ã¨ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®å¤šæ•°æ´¾ã«é©åˆã—ã‚ˆã†ã¨ã™ã‚‹ãŒã€å¤§ããã™ã‚‹ã¨å€‹ã€…ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æ­£ç¢ºã«åˆ†é¡ã—ã‚ˆã†ã¨ã™ã‚‹ã€‚

```{python}
mglearn.plots.plot_linear_svc_regularization()
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

- ä¸Šè¨˜ã®ä¾‹ã§ã¯ã€Cã‚’å¤§ããã™ã‚‹ã¨èª¤åˆ†é¡ã—ãŸå°‘æ•°ã®ç‚¹ã«æ±ºå®šå¢ƒç•ŒãŒå¤§ããå½±éŸ¿ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
- ä½æ¬¡å…ƒã®å ´åˆã¯ç·šå½¢åˆ†é¡ã¯åˆ¶ç´„ãŒå¼·ã„ã‚ˆã†ã«æ€ãˆã‚‹ãŒã€æ¬¡å…ƒæ•°ãŒå¤§ãããªã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ã¯å¼·åŠ›ã«ãªã‚Šã€ã‚€ã—ã‚éå‰°é©åˆã‚’ã„ã‹ã«é¿ã‘ã‚‹ã‹ãŒãƒã‚¤ãƒ³ãƒˆã«ãªã‚‹ã€‚

**cancer**ã«**LogisticRegression**ã‚’é©ç”¨ã—ã¦ã¿ã‚‹ã€‚

```{python}
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
  cancer.data, cancer.target, stratify = cancer.target, random_state = 42
)
logreg = LogisticRegression().fit(X_train, y_train)
print("ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¹ã‚³ã‚¢: {:.3f}".format(logreg.score(X_train, y_train)))
print("è¨“ç·´ã‚»ãƒƒãƒˆã‚¹ã‚³ã‚¢: {:.3f}".format(logreg.score(X_test, y_test)))
```

- **è¨“ç·´ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã‚¹ã‚³ã‚¢ãŒè¿‘ã„å ´åˆã¯é©åˆä¸è¶³ã‚’ç–‘ã†ã€‚**

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Cã‚’å¤§ããã—ã¦ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã‚’ä¸Šã’ã‚‹ã€‚

```{python}
logreg100 = LogisticRegression(C=100).fit(X_train, y_train)
print("ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¹ã‚³ã‚¢: {:.3f}".format(logreg100.score(X_train, y_train)))
print("è¨“ç·´ã‚»ãƒƒãƒˆã‚¹ã‚³ã‚¢: {:.3f}".format(logreg100.score(X_test, y_test)))
```

ç²¾åº¦ãŒä¸ŠãŒã£ãŸã€‚ä»Šåº¦ã¯é€†ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Cã‚’å°ã•ãã—ã¦ã¿ã‚‹ã€‚

```{python}
logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)
print("ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¹ã‚³ã‚¢: {:.3f}".format(logreg001.score(X_train, y_train)))
print("è¨“ç·´ã‚»ãƒƒãƒˆã‚¹ã‚³ã‚¢: {:.3f}".format(logreg001.score(X_test, y_test)))
```

ç²¾åº¦ãŒä¸‹ãŒã£ãŸã€‚æœ€å¾Œã«ã€3ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã¤ã„ã¦ä¿‚æ•°ã‚’å¯è¦–åŒ–ã—ã¦ã¿ã‚‹ã€‚

```{python}
plt.plot(logreg.coef_.T, 'o', label = "C=1")
plt.plot(logreg100.coef_.T, '^', label = "C=100")
plt.plot(logreg001.coef_.T, 'v', label = "C=0.01")
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
plt.hlines(0, 0, cancer.data.shape[1])
plt.xlabel("ç‰¹å¾´é‡")
plt.ylabel("ä¿‚æ•°ã®å¤§ãã•")
plt.legend()
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯**LogisticRegression**ã¯L2æ­£å‰‡åŒ–ã‚’è¡Œã†ã€‚
- `penalty="l1"`ã®æŒ‡å®šã§L1æ­£å‰‡åŒ–ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ã‚ˆã‚Šå˜ç´”ãªãƒ¢ãƒ‡ãƒ«ãŒæ¬²ã—ã‘ã‚Œã°ã“ã¡ã‚‰ã‚’è©¦ã™ã¨è‰¯ã„ã€‚

```{python}
for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):
  lr_l1 = LogisticRegression(C = C, penalty = "l1").fit(X_train, y_train)
  print("è¨“ç·´ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹ç²¾åº¦(C={:.3f}): {:.2f}".format(C, lr_l1.score(X_train, y_train)))
  print("ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹ç²¾åº¦(C={:.3f}): {:.2f}".format(C, lr_l1.score(X_test, y_test)))
  plt.plot(lr_l1.coef_.T, marker, label = "C={:.3f}".format(C))

plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation = 90)
plt.hlines(0, 0, cancer.data.shape[1])
plt.xlabel("ç‰¹å¾´é‡")
plt.ylabel("ä¿‚æ•°ã®å¤§ãã•")
plt.legend(loc = 3)
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

### ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å¤šã‚¯ãƒ©ã‚¹åˆ†é¡

- å¤§æŠµã®ç·šå½¢ã‚¯ãƒ©ã‚¹åˆ†é¡ã¯2ã‚¯ãƒ©ã‚¹åˆ†é¡ã«ã—ã‹å¯¾å¿œã—ã¦ãŠã‚‰ãšã€ãã®ã¾ã¾ã§ã¯å¤šã‚¯ãƒ©ã‚¹ã«æ‹¡å¼µã™ã‚‹ã“ã¨ã¯ã§ããªã„ã€‚
    - ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¯ä¾‹å¤–
- æ‹¡å¼µã™ã‚‹ãŸã‚ã®æ–¹æ³•ã¨ã—ã¦**1å¯¾ãã®ä»–(one-vs.-rest)**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚‹ã€‚
    - **1ã¤ã®ã‚¯ãƒ©ã‚¹ã¨ãã®ä»–ã®ã‚¯ãƒ©ã‚¹**ã¨ã„ã†2ã‚¯ãƒ©ã‚¹åˆ†é¡ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚
    - ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«å¯¾ã—ã¦ã¯å…¨ã¦ã®2ã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    - **ä¸€ç•ªé«˜ã„ã‚¹ã‚³ã‚¢ã®ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨**ã®åˆ†é¡çµæœã‚’äºˆæ¸¬çµæœã¨ã™ã‚‹ã€‚
    - ã‚¯ãƒ©ã‚¹ã”ã¨ã«2ã‚¯ãƒ©ã‚¹åˆ†é¡ãŒå­˜åœ¨ã™ã‚‹ã¨ã„ã†ã“ã¨ãªã®ã§ã€ã‚¯ãƒ©ã‚¹ã”ã¨ã«ä»¥ä¸‹ã®å¼ã§è¡¨ã™ç¢ºä¿¡åº¦ãŒå­˜åœ¨ã—ã€ç¢ºä¿¡åº¦ãŒæœ€ã‚‚å¤§ãã„ã‚¯ãƒ©ã‚¹ãŒã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã¨ãªã‚‹ã€‚
    
$$ w[0] \times x[0] + w[1] \times x[1] + \dots + w[p] \times x[p] + b$$

- å¤šã‚¯ãƒ©ã‚¹ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨1å¯¾å¤šã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯å¤šå°‘ç•°ãªã‚‹ãŒã€1ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šä¿‚æ•°ãƒ™ã‚¯ãƒˆãƒ«ã¨åˆ‡ç‰‡ãŒã§ãã‚‹ã¨ã„ã†ç‚¹ã¯å…±é€šã—ã¦ã„ã‚‹ã€‚

3ã‚¯ãƒ©ã‚¹åˆ†é¡ã«å¯¾ã—ã¦1å¯¾å¤šã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã¯ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸ2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã™ã‚‹ã€‚

```{python}
from sklearn.datasets import make_blobs

X, y = make_blobs(random_state = 42)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("ç‰¹å¾´é‡0")
plt.ylabel("ç‰¹å¾´é‡1")
plt.legend(["ã‚¯ãƒ©ã‚¹0", "ã‚¯ãƒ©ã‚¹1", "ã‚¯ãƒ©ã‚¹2"])
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§**LinearSVC**ã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚

```{python}
linear_svm = LinearSVC().fit(X, y)
print("ä¿‚æ•°ãƒ™ã‚¯ãƒˆãƒ«ã®å½¢çŠ¶", linear_svm.coef_.shape)
print("åˆ‡ç‰‡ãƒ™ã‚¯ãƒˆãƒ«ã®å½¢çŠ¶", linear_svm.intercept_.shape)
```

- ä¿‚æ•°ãƒ™ã‚¯ãƒˆãƒ«ã®å½¢çŠ¶ãŒ3è¡Œ2åˆ—ã¨ã„ã†ã“ã¨ã¯ã€å„è¡Œã«å„ã‚¯ãƒ©ã‚¹ã«å¯¾å¿œã™ã‚‹2æ¬¡å…ƒã®ä¿‚æ•°ãƒ™ã‚¯ãƒˆãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚
- åˆ‡ç‰‡ãƒ™ã‚¯ãƒˆãƒ«ã¯ã‚¯ãƒ©ã‚¹ã®æ•°ã«å¯¾å¿œã—ã¦ã„ã‚‹ã€‚
- ä¸Šè¨˜2ç‚¹ã‚’ã¾ã¨ã‚ã‚‹ã¨ã€3ã¤ã®ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚

3ã¤ã®ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨ãŒä½œã‚‹æ±ºå®šå¢ƒç•Œã‚’å¯è¦–åŒ–ã™ã‚‹ã€‚

```{python}
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
line = np.linspace(-15, 15)
for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, ['b', 'r', 'g']):
  plt.plot(line, -(line * coef[0] + intercept) / coef[1], c = color)
plt.xlabel("ç‰¹å¾´é‡0")
plt.ylabel("ç‰¹å¾´é‡1")
plt.legend(['ã‚¯ãƒ©ã‚¹0', 'ã‚¯ãƒ©ã‚¹1', 'ã‚¯ãƒ©ã‚¹2', 'ã‚¯ãƒ©ã‚¹0ã®æ±ºå®šå¢ƒç•Œ', 'ã‚¯ãƒ©ã‚¹1ã®æ±ºå®šå¢ƒç•Œ', 'ã‚¯ãƒ©ã‚¹2ã®æ±ºå®šå¢ƒç•Œ'],
  loc = (1.01, 0.3))
```

```{python}
plt.show()
plt.close()
```

