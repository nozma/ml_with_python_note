# 教師あり学習

備えます。

```{r setup, echo=FALSE}
knitr::opts_chunk$set(
  engine.path = "/usr/bin/python3",
  engine = "python",
  collapse = TRUE,
#  cache = TRUE,
  comment = " ##"
  )
```


```{python}
import numpy as np
import scipy as sp
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rc('font', family='IPAexGothic') # 日本語プロット設定
import mglearn
```

## クラス分類と回帰

教師あり学習はさらに2つに分けられる。

- **クラス分類**: クラスラベルを予測する問題。
    - 2クラス分類 (binary classification): Yes/Noみたいな2択。
        - 片方を**陽性** (positive)、もう片方を**陰性** (negative)とする場合がしばしばある。
    - 他クラス分類 (multiclass classification): もっと選択肢多いやつ。
- **回帰**: 連続値を予測する問題。

2つを区別するのは**出力**が連続かどうか。**入力**はどちらの問題でも連続の場合も離散的な場合もある。

## 汎化、過剰適合、適合不足

- **汎化能力**: 未知のデータ(訓練に使ってないデータ)に対する正しい値を予測する能力。
- **過剰適合**: 訓練データはめっちゃ正確に予測できるけど新しいデータはてんでダメという状態。
- **適合不足**: 訓練データすらちゃんと予測できてないという状態。

一般的には**モデルを複雑にする**ほど訓練データに適合していく。適合不足でなく、過剰適合にならない適度なモデルの複雑さの時に汎化能力が最大になる。そこを目指そう。

### モデルの複雑さとデータセットの大きさ

- モデルが複雑でも、データセットが大きければ過剰適合を避けられる。
- 適度な複雑さのモデルと十分に大きなデータセットを使うことが成功のポイント。

## 教師あり機械学習アルゴリズム

### サンプルデータセット

- **人工的な単純なデータセット**と、**実世界の割と複雑なデータセット**を使う。

#### 人工的な単純なデータセット

単純なデータセットは**mglearn**で生成する。

- **forge**: `mglearn.datasets.make_forge()`で生成する2クラス分類向けデータ。
    - 2つの特徴量と1つの2値目的変数をもつ。

```{python}
X, y = mglearn.datasets.make_forge()
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.legend(["Class 0", "Class 1"], loc = 4) # 凡例
plt.xlabel("第1特徴量")
plt.ylabel("第2特徴量")
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- **wave**: `mglearn.datasets.make_wave`で生成する回帰向けデータ。
    - 1つの特徴量と1つの目的変数を持つ。

```{python}
X, y = mglearn.datasets.make_wave(n_samples = 40)
plt.plot(X, y, 'o')
plt.xlabel("特徴量")
plt.ylabel("目的変数")
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

#### 実データ

実データは**scikit-learn**に入ってるものを使う。第1章でも説明したBunchクラスになっている。

- **cancer**: ウィスコンシン乳癌データセット
    - 目的変数は良性(benign)と悪性(malignant)の2値。
    - 特徴量は30。
    - データポイントは569点。

```{python}
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
print(cancer.keys())
print(cancer.data.shape)
print(cancer.target_names)
print(np.bincount(cancer.target))
```

- **boston_housing**: 1970年代のボストン近郊の住宅価格。
    - 住宅価格の中央値が目的変数。
    - 特徴量は13。
    - データポイントは506点。

```{python}
from sklearn.datasets import load_boston
boston = load_boston()
print(boston.data.shape)
print(boston.feature_names)
```

- 特徴量同士の積を求めたりして、新しい特徴量を導出することを**特徴量エンジニアリング**と呼ぶ。
- **boston_housing**に対し、重複ありで2つの特徴量の積を求め、データセットの拡張を試みる。
    - 作業が面倒なので既に拡張したものが`mglearn.datasets.load_extended_boston()`で読み込めます。

```{python}
X, y = mglearn.datasets.load_extended_boston()
print(X.shape)
```


## アルゴリズム1 $k$-最近傍法

- a.k.a. $k$-NN
- 近いやつは大体おんなじ。

### $k$-最近傍法によるクラス分類

- $k$は参考にする近傍点の個数。
- 1-NNの例。

```{python}
mglearn.plots.plot_knn_classification(n_neighbors=1)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- 3-NNの例
    - 近傍点が複数のときは多数決で決める。

```{python}
mglearn.plots.plot_knn_classification(n_neighbors=3)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- **scikit-learn**でやる。

```{python}
from sklearn.model_selection import train_test_split
X, y = mglearn.datasets.make_forge()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)
print(clf.score(X_test, y_test))
```

### KNeighborsClassifierの解析

- 特徴量が2つしかなければ、散布図が描ける。
- 散布図上のあらゆる箇所について**もしその場所に点があったら**と考えて判別ができる。
- つまり、特徴量がつくる平面を分類クラスで塗り分けることができる。
- 境界線を**決定境界**と呼ぶ。

```{python}
fig, axes = plt.subplots(1, 3, figsize = (10, 3))

for n, ax in zip([1, 3, 9], axes):
    clf = KNeighborsClassifier(n_neighbors = n).fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps = 0.5, ax = ax, alpha = .4)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax = ax)
    ax.set_title("{} neighbor(s)".format(n))
    ax.set_xlabel("特徴量 0")
    ax.set_ylabel("特徴量 1")

axes[0].legend(loc=3)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- 近傍点が多いほど境界がなめらか = モデルは単純になる。
    - 近傍点1 = 最も複雑なモデル
    - 近傍点数 = データ数 -> ただの多数決
- ということは近傍点数の数を増やしていくと、どこかで汎化能力のピークが…？
- **cancer**データセットで試してみる。

```{python}
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
  cancer.data, cancer.target, stratify = cancer.target, random_state = 66
)

training_accuracy = []
test_accuracy = []
n_settings = range(1, 11)

for n in n_settings:
  clf = KNeighborsClassifier(n_neighbors = n).fit(X_train, y_train)
  training_accuracy.append(clf.score(X_train, y_train))
  test_accuracy.append(clf.score(X_test, y_test))

plt.plot(n_settings, training_accuracy, label = "訓練セット精度")
plt.plot(n_settings, test_accuracy, label = "テストセット精度")
plt.ylabel("精度")
plt.xlabel("近傍点数")
plt.legend()
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

### $k$-近傍回帰

- kNNは回帰もできる。
- 1-NNでは近傍点の値が新しい観測値に対応する値だと考える。

```{python}
mglearn.plots.plot_knn_regression(n_neighbors = 1)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

  
- 近傍点が複数の時は平均値を使う。

```{python}
mglearn.plots.plot_knn_regression(n_neighbors = 3)
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- **scikit-learn**では、**KNeighborsRegressor**クラスに実装されてる。

```{python}
from sklearn.neighbors import KNeighborsRegressor

X, y = mglearn.datasets.make_wave(n_samples = 40)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

reg = KNeighborsRegressor(n_neighbors = 3).fit(X_train, y_train)

print(reg.score(X_test, y_test))
```

### KNeighborsRegressorの解析

- 1次元のデータセットに対する予測値は、近傍点数$k$に対してどのように変化するか？

```{python}
# プロット先を3つ作る
fig, axes = plt.subplots(1, 3, figsize = (15, 4))
# -3〜3までの間にデータポイントを1000点作る
line = np.linspace(-3, 3, 1000).reshape(-1, 1)
for n_neighbors, ax in zip([1, 3, 9], axes):
  reg = KNeighborsRegressor(n_neighbors = n_neighbors)
  reg.fit(X_train, y_train)
  ax.plot(line, reg.predict(line))
  ax.plot(X_train, y_train, '^')
  ax.plot(X_test, y_test, 'v')
  ax.set_title(
    "{} 近傍点\n 訓練スコア: {:.2f} テストスコア{:.2f}".format(
      n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test)))
  ax.set_xlabel("特徴量")
  ax.set_ylabel("目的変数")
  
axes[0].legend(["モデルによる予測値", "訓練データ", "テストデータ"], loc="best")
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- $k=1$の場合は予測値が全ての訓練データを通るので、モデルが不安定になる。
- 近傍点を増やしていくと予測は滑らかになるが、その反面訓練データへの適合度が下がる。

### 利点と欠点とパラメータ

- 利点
    - モデルが理解しやすい。
    - あまり調整しなくても性能が出やすい。
    - モデル構築は高速
- 欠点
    - 訓練セットが大きくなると予測が遅くなる。
        - 実際に使う前には前処理を行うことが重要。
    - 疎なデータセット(特徴量の多くが0である)に対しては十分な性能が出にくい。

上記の理由から、kNNは実際に使われることは少ない。

## アルゴリズム2 線形モデル

### 線形モデルによる回帰

線形モデルによる予測式は...

$$\hat{y} = w[0]\times x[0] + w[1]\times x[1] + ... + w[p]\times x[p] + b$$

- $\hat{y}$は予測値で、$w$と$b$はモデルのパラメータ。$x$はある一つのデータポイントの特徴量。
- 予測値は、データポイントを適当に重み付けしたもの、と見ることもできる。

**wave**に線形回帰を適用してプロットしてみよう。

```{python}
mglearn.plots.plot_linear_regression_wave()
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

線形モデルを利用した回帰にはいろいろなアルゴリズムがあって、それぞれ以下の点で異なっている。

- どのようにパラメータ$w$と$b$を学習するか。
- モデルの複雑さをどのように制御するのか。

### 線形回帰(通常最小二乗法)

- 予測値と真値の**平均二乗誤差** (mean squared error) を最小にするようなパラメータを求める。
- 線形回帰には複雑さを制御するパラメータがない。できない。

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
X, y = mglearn.datasets.make_wave(n_samples = 60)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)
lr = LinearRegression().fit(X_train, y_train)
```

- $w$は**係数** (coefficient)と呼ばれ、`coef_`に格納される。
- $b$は**切片** (intercept)と呼ばれ、`intercept_`に格納される。

```{python}
print(lr.coef_)
print(lr.intercept_)
```

- 訓練データから得られた属性にアンダースコアを付けるのは**scikit-learn**の慣習である。
- `coef_`は特徴量1つに対して1つの値をもつNumPy配列となる。
- 線形回帰の性能は決定係数$R^2$として求められる。

```{python}
print(lr.score(X_train, y_train))
print(lr.score(X_test, y_test))
```

ここで訓練セットとテストセットの$R^2$があんまり違わないのは（予測性能はともかく）過剰適合していないことを示している。通常、特徴量が多いほど過剰適合のリスクが高まる。拡張した**boston_housing**で確認してみよう。

```{python}
X, y = mglearn.datasets.load_extended_boston()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
lr = LinearRegression().fit(X_train, y_train)
```

$R^2$を訓練セットとテストセットで比較してみよう。

```{python}
print(lr.score(X_train, y_train))
print(lr.score(X_test, y_test))
```

両者に乖離が見られるのは、過剰適合している可能性がある。

モデルの複雑さを制御できれば良いのだが、線形回帰にはそのためのパラメータがない。パラメータを導入する方法として**リッジ回帰**がある。

### リッジ回帰

- 係数が多いからモデルが複雑になる。
- 係数が0＝その係数を考慮しない。
- 係数が小さければモデルは単純になるのでは🤔
    - 極端な話係数が全部ゼロなら入力に関わらず一定の値(平均とか)を出力するモデルになる。
- 係数ベクトルの長さを最小化しよう！→リッジ回帰

```{python}
from sklearn.linear_model import Ridge
ridge = Ridge().fit(X_train, y_train) # データは拡張Boston housingのまま
print(ridge.score(X_train, y_train))
print(ridge.score(X_test, y_test))
```

- 訓練セットへの予測能力が下がったけどテストセットへの予測能力が上がった！
    - モデルを単純にすることで汎化能力が上がっている。
- リッジ回帰におけるモデルの単純さを制御するパラメータ: $\alpha$
    - 大きいほど制約が強い = モデルが単純になる
    - sklearnのデフォルトは1.0
    - 何が良いかはデータ次第で、自動的には調整されない（後で多分チューニング方法が出て来る）。
    
```{python}
### alphaを10倍にしてみる パラメータはオブジェクト生成時に指定
ridge10 = Ridge(alpha = 10).fit(X_train, y_train)
print(ridge10.score(X_train, y_train))
print(ridge10.score(X_test, y_test))

### alphaを0.1倍にしてみる パラメータはオブジェクト生成時に指定
ridge01 = Ridge(alpha = .1).fit(X_train, y_train)
print(ridge01.score(X_train, y_train))
print(ridge01.score(X_test, y_test))
```

$\alpha$の大きさと係数の関係をプロットしてみる。$\alpha$が大きいほど係数の絶対値は小さくなるはず…

```{python}
plt.plot(ridge.coef_, 's', label="Ridge alpha=1")
plt.plot(ridge10.coef_, '^', label="Ridge alpha=10")
plt.plot(ridge01.coef_, 'v', label="Ridge alpha=0.1")
plt.plot(lr.coef_, 'o', label="LinearRegression")
plt.xlabel("係数のインデックス")
plt.ylabel("係数の値")
plt.hlines(0, 0, len(lr.coef_))
plt.ylim(-25, 25)
plt.legend()
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

- データサイズを増やしていくとスコアはどのように変化するか？
    - **学習曲線** (learning curve): モデルの性能をデータセットサイズとの関係で表したもの。
    - リッジ回帰は正則化の影響で常に線形回帰より訓練データへの適合が低い。
    - テストセットへの適合はデータセットサイズが小さいうちはリッジ回帰の方が優れる。
    - データセットサイズが大きくなると、リッジ回帰と線形回帰の差はなくなる。
        - データセットサイズが大きくなると、(単純なモデルでは)過剰適合することが難しくなる。

```{python}
mglearn.plots.plot_ridge_n_samples()
plt.xlabel("訓練セットのサイズ")
plt.ylabel("スコア(R²)")
plt.legend(labels=["リッジ 訓練セット", "リッジ テストセット", "線形回帰 訓練セット", "線形回帰 テストセット"])
```

```{python, echo=FALSE}
plt.show()
plt.close()
```

### Lasso

- Ridgeとは異なる形で係数に制約をかける線形回帰。
    - L1正則化: L1ノルム、つまり係数の絶対値の和に制約をかける。
- **いくつかの係数が完全に0になる場合がある**という点がRidgeと大きく異なる。
    - 係数が完全に0=係数を除外しているということなので、**自動的な変数選択**ともみなせる。
    - 変数が減ればモデルを解釈しやすくなるという利点もある。
    
Lassoを**boston_housing**に適用する。

```{python}
from sklearn.linear_model import Lasso

lasso = Lasso().fit(X_train, y_train)
print("訓練データスコア: {:.2f}".format(lasso.score(X_train, y_train)))
print("テストデータスコア: {:.2f}".format(lasso.score(X_test, y_test)))
print("選択された特徴量数: {}".format(np.sum(lasso.coef_ != 0)))
```

- スコアが非常に悪いのは、パラメータを全くチューニングしていないことによる。
- Lassoには複雑さの度合いを制御するパラメータ`alpha`がある。`alpha`のデフォルトは1.0で、小さくするほど複雑なモデルになる。
- `alpha`を手動で減らす際には、合わせて`max_iter`を増やしてやる必要がある。

```{python}
lasso001 = Lasso(alpha = 0.01, max_iter=100000).fit(X_train, y_train)
print("訓練データスコア: {:.2f}".format(lasso001.score(X_train, y_train)))
print("テストデータスコア: {:.2f}".format(lasso001.score(X_test, y_test)))
print("選択された特徴量数: {}".format(np.sum(lasso001.coef_ != 0)))
```

- `alpha`を小さくしすぎると過剰適合する。

```{python}
lasso00001 = Lasso(alpha = 0.0001, max_iter=100000).fit(X_train, y_train)
print("訓練データスコア: {:.2f}".format(lasso00001.score(X_train, y_train)))
print("テストデータスコア: {:.2f}".format(lasso00001.score(X_test, y_test)))
print("選択された特徴量数: {}".format(np.sum(lasso00001.coef_ != 0)))
```


Ridgeでやったように係数の大きさをプロットしてみよう。

```{python}
plt.plot(lasso.coef_, 's', label = "Lasso alpha = 1")
plt.plot(lasso001.coef_, '^', label = "Lasso alpha = 0.01")
plt.plot(lasso00001.coef_, 'v', label = "Lasso alpha = 0.0001")
plt.plot(ridge01.coef_, 'o', label = "Ridge alpha = 0.1")
plt.legend(ncol = 2, loc = (0, 1.05))
plt.ylim = (-25, 25)
plt.xlabel("係数のインデックス")
plt.ylabel("係数の大きさ")
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

- 合わせてプロットしたRidge($\alpha=0.1$)は、Lasso($\alpha=0.01$)と同じくらいの性能であるが、Ridgeでは大きさが小さいながらも係数の値は0にはなっていないものが多いのに対して、Lassoでは大きさが0の係数が目立つ。
- 実際にはまずRidgeを試すと良い。
- 係数がたくさんあって重要なのはそのうちの幾つか少数であると予想されるのであれば、Lassoを試すと良い。
- RidgeとLassoのペナルティを組合せたものとしてElasticNetがある。結果は良好であるが、チューニングすべきパラメータが増えるという欠点がある。

### クラス分類のための線形モデル

線形モデルでクラス分類を行う場合は以下の式を用いる。

$$\hat{y} = w[0]\times x[0] + w[1]\times x[1] + \dots + w[p]\times x[p] + b > 0$$

- 出力$y$が0を超えるかどうかで判別する。
- 出力$y$は特徴量の線形関数であり、2つのクラスを直線や平面、超平面で分割する**決定境界**となる。
- 線形モデルを学習するアルゴリズムは以下の観点から分類される。
    - どのような尺度で訓練データへの適合度を測るか。
    - 正則化を行うか。行うならどのような方法か。
- **ロジスティック回帰**と**線形サポートベクターマシン**は一般的な線形クラスアルゴリズムである。

**LogisticRegression**と**LinearSVC**により**forge**を分類する決定境界を可視化する。

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize = (10, 3))

for model, ax in zip([LinearSVC(), LogisticRegression()], axes):
  clf = model.fit(X, y)
  mglearn.plots.plot_2d_separator(clf, X, fill = False, eps = 0.5, ax = ax, alpha = 0.7)
  mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax = ax)
  ax.set_title("{}".format(clf.__class__.__name__))
  ax.set_xlabel("特徴量 0")
  ax.set_ylabel("特徴量 1")
  
axes[0].legend()
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

- 2つのクラス分類器はいずれも正則化パラメータCを持つ。Cは大きいほど正則化が弱くなる。
- Cがは小さいとデータポイントの多数派に適合しようとするが、大きくすると個々のデータポイントを正確に分類しようとする。

```{python}
mglearn.plots.plot_linear_svc_regularization()
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

- 上記の例では、Cを大きくすると誤分類した少数の点に決定境界が大きく影響されていることがわかる。
- 低次元の場合は線形分類は制約が強いように思えるが、次元数が大きくなるとモデルは強力になり、むしろ過剰適合をいかに避けるかがポイントになる。

**cancer**に**LogisticRegression**を適用してみる。

```{python}
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
  cancer.data, cancer.target, stratify = cancer.target, random_state = 42
)
logreg = LogisticRegression().fit(X_train, y_train)
print("テストセットスコア: {:.3f}".format(logreg.score(X_train, y_train)))
print("訓練セットスコア: {:.3f}".format(logreg.score(X_test, y_test)))
```

- **訓練セットとテストセットのスコアが近い場合は適合不足を疑う。**

パラメータCを大きくしてモデルの複雑さを上げる。

```{python}
logreg100 = LogisticRegression(C=100).fit(X_train, y_train)
print("テストセットスコア: {:.3f}".format(logreg100.score(X_train, y_train)))
print("訓練セットスコア: {:.3f}".format(logreg100.score(X_test, y_test)))
```

精度が上がった。今度は逆にパラメータCを小さくしてみる。

```{python}
logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)
print("テストセットスコア: {:.3f}".format(logreg001.score(X_train, y_train)))
print("訓練セットスコア: {:.3f}".format(logreg001.score(X_test, y_test)))
```

精度が下がった。最後に、3つのパターンについて係数を可視化してみる。

```{python}
plt.plot(logreg.coef_.T, 'o', label = "C=1")
plt.plot(logreg100.coef_.T, '^', label = "C=100")
plt.plot(logreg001.coef_.T, 'v', label = "C=0.01")
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
plt.hlines(0, 0, cancer.data.shape[1])
plt.xlabel("特徴量")
plt.ylabel("係数の大きさ")
plt.legend()
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

- デフォルトでは**LogisticRegression**はL2正則化を行う。
- `penalty="l1"`の指定でL1正則化に切り替えることができる。より単純なモデルが欲しければこちらを試すと良い。

```{python}
for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):
  lr_l1 = LogisticRegression(C = C, penalty = "l1").fit(X_train, y_train)
  print("訓練セットに対する精度(C={:.3f}): {:.2f}".format(C, lr_l1.score(X_train, y_train)))
  print("テストセットに対する精度(C={:.3f}): {:.2f}".format(C, lr_l1.score(X_test, y_test)))
  plt.plot(lr_l1.coef_.T, marker, label = "C={:.3f}".format(C))

plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation = 90)
plt.hlines(0, 0, cancer.data.shape[1])
plt.xlabel("特徴量")
plt.ylabel("係数の大きさ")
plt.legend(loc = 3)
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

### 線形モデルによる多クラス分類

- 大抵の線形クラス分類は2クラス分類にしか対応しておらず、そのままでは多クラスに拡張することはできない。
    - ロジスティック回帰は例外
- 拡張するための方法として**1対その他(one-vs.-rest)**アプローチがある。
    - **1つのクラスとその他のクラス**という2クラス分類に対してモデルを学習させる。
    - データポイントに対しては全ての2クラス分類を実行する。
    - **一番高いスコアのクラス分類器**の分類結果を予測結果とする。
    - クラスごとに2クラス分類が存在するということなので、クラスごとに以下の式で表す確信度が存在し、確信度が最も大きいクラスがクラスラベルとなる。
    
$$ w[0] \times x[0] + w[1] \times x[1] + \dots + w[p] \times x[p] + b$$

- 多クラスロジスティック回帰と1対多アプローチは多少異なるが、1クラスあたり係数ベクトルと切片ができるという点は共通している。

3クラス分類に対して1対多アプローチを試す。データはガウス分布からサンプリングした2次元データセットとする。

```{python}
from sklearn.datasets import make_blobs

X, y = make_blobs(random_state = 42)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("特徴量0")
plt.ylabel("特徴量1")
plt.legend(["クラス0", "クラス1", "クラス2"])
```

```{python, echo = FALSE}
plt.show()
plt.close()
```

このデータセットで**LinearSVC**を学習させる。

```{python}
linear_svm = LinearSVC().fit(X, y)
print("係数ベクトルの形状", linear_svm.coef_.shape)
print("切片ベクトルの形状", linear_svm.intercept_.shape)
```

- 係数ベクトルの形状が3行2列ということは、各行に各クラスに対応する2次元の係数ベクトルが格納されているということである。
- 切片ベクトルはクラスの数に対応している。
- 上記2点をまとめると、3つのクラス分類器が得られているということである。

3つのクラス分類器が作る決定境界を可視化する。

```{python}
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
line = np.linspace(-15, 15)
for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, ['b', 'r', 'g']):
  plt.plot(line, -(line * coef[0] + intercept) / coef[1], c = color)
plt.xlabel("特徴量0")
plt.ylabel("特徴量1")
plt.legend(['クラス0', 'クラス1', 'クラス2', 'クラス0の決定境界', 'クラス1の決定境界', 'クラス2の決定境界'],
  loc = (1.01, 0.3))
```

```{python}
plt.show()
plt.close()
```

