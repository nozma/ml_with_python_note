<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Pythonで始める機械学習の学習</title>
  <meta name="description" content="Pythonで始める機械学習の学習">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Pythonで始める機械学習の学習" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Pythonで始める機械学習の学習" />
  
  
  

<meta name="author" content="R. Ito">


<meta name="date" content="2018-03-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="2-3-section-2-3.html">
<link rel="next" href="2-5-2-.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code > span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code > span.dt { color: #0057ae; } /* DataType */
code > span.dv { color: #b08000; } /* DecVal */
code > span.bn { color: #b08000; } /* BaseN */
code > span.fl { color: #b08000; } /* Float */
code > span.cn { color: #aa5500; } /* Constant */
code > span.ch { color: #924c9d; } /* Char */
code > span.sc { color: #3daee9; } /* SpecialChar */
code > span.st { color: #bf0303; } /* String */
code > span.vs { color: #bf0303; } /* VerbatimString */
code > span.ss { color: #ff5500; } /* SpecialString */
code > span.im { color: #ff5500; } /* Import */
code > span.co { color: #898887; } /* Comment */
code > span.do { color: #607880; } /* Documentation */
code > span.an { color: #ca60ca; } /* Annotation */
code > span.cv { color: #0095ff; } /* CommentVar */
code > span.ot { color: #006e28; } /* Other */
code > span.fu { color: #644a9b; } /* Function */
code > span.va { color: #0057ae; } /* Variable */
code > span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code > span.op { color: #1f1c1b; } /* Operator */
code > span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code > span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code > span.pp { color: #006e28; } /* Preprocessor */
code > span.at { color: #0057ae; } /* Attribute */
code > span.re { color: #0057ae; } /* RegionMarker */
code > span.in { color: #b08000; } /* Information */
code > span.wa { color: #bf0303; } /* Warning */
code > span.al { color: #bf0303; font-weight: bold; } /* Alert */
code > span.er { color: #bf0303; text-decoration: underline; } /* Error */
code > span. { color: #1f1c1b; } /* Normal */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>まえおき</a><ul>
<li class="chapter" data-level="" data-path="e696b9e9879d.html"><a href="e696b9e9879d.html"><i class="fa fa-check"></i>方針</a></li>
<li class="chapter" data-level="" data-path="e59fb7e7ad86e792b0e5a283e381a8e3818b.html"><a href="e59fb7e7ad86e792b0e5a283e381a8e3818b.html"><i class="fa fa-check"></i>執筆環境とか</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-section-1.html"><a href="1-section-1.html"><i class="fa fa-check"></i><b>1</b> はじめに</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-section-1-1.html"><a href="1-1-section-1-1.html"><i class="fa fa-check"></i><b>1.1</b> なぜ機械学習なのか</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-section-1-1.html"><a href="1-1-section-1-1.html#section-1.1.1"><i class="fa fa-check"></i><b>1.1.1</b> 機械学習で解決可能な問題</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-section-1-1.html"><a href="1-1-section-1-1.html#section-1.1.2"><i class="fa fa-check"></i><b>1.1.2</b> タスクを知り、データを知る</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-python.html"><a href="1-2-python.html"><i class="fa fa-check"></i><b>1.2</b> なぜPythonなのか？</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-scikit-learn.html"><a href="1-3-scikit-learn.html"><i class="fa fa-check"></i><b>1.3</b> scikit-learn</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-scikit-learn.html"><a href="1-3-scikit-learn.html#section-1.3.1"><i class="fa fa-check"></i><b>1.3.1</b> インストール</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-section-1-4.html"><a href="1-4-section-1-4.html"><i class="fa fa-check"></i><b>1.4</b> 必要なライブラリとツール</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-python-2-vs-python-3.html"><a href="1-5-python-2-vs-python-3.html"><i class="fa fa-check"></i><b>1.5</b> Python 2 vs. Python 3</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-.html"><a href="1-6-.html"><i class="fa fa-check"></i><b>1.6</b> 最初のアプリケーション: アイリスのクラス分類</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-6-.html"><a href="1-6-.html#section-1.6.1"><i class="fa fa-check"></i><b>1.6.1</b> データを読む</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-6-.html"><a href="1-6-.html#-"><i class="fa fa-check"></i><b>1.6.2</b> 成功度合いの測定: 訓練データとテストデータ</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-6-.html"><a href="1-6-.html#-"><i class="fa fa-check"></i><b>1.6.3</b> 最初にすべきこと: データを良く観察する</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-6-.html"><a href="1-6-.html#-k-"><i class="fa fa-check"></i><b>1.6.4</b> 最初のモデル: k-最近傍法</a></li>
<li class="chapter" data-level="1.6.5" data-path="1-6-.html"><a href="1-6-.html#section-1.6.5"><i class="fa fa-check"></i><b>1.6.5</b> 予測を行う</a></li>
<li class="chapter" data-level="1.6.6" data-path="1-6-.html"><a href="1-6-.html#section-1.6.6"><i class="fa fa-check"></i><b>1.6.6</b> モデルの評価</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-section-2.html"><a href="2-section-2.html"><i class="fa fa-check"></i><b>2</b> 教師あり学習</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-section-2-1.html"><a href="2-1-section-2-1.html"><i class="fa fa-check"></i><b>2.1</b> クラス分類と回帰</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-section-2-2.html"><a href="2-2-section-2-2.html"><i class="fa fa-check"></i><b>2.2</b> 汎化、過剰適合、適合不足</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-section-2-2.html"><a href="2-2-section-2-2.html#section-2.2.1"><i class="fa fa-check"></i><b>2.2.1</b> モデルの複雑さとデータセットの大きさ</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-section-2-3.html"><a href="2-3-section-2-3.html"><i class="fa fa-check"></i><b>2.3</b> 教師あり機械学習アルゴリズム</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-section-2-3.html"><a href="2-3-section-2-3.html#section-2.3.1"><i class="fa fa-check"></i><b>2.3.1</b> サンプルデータセット</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-1-k-.html"><a href="2-4-1-k-.html"><i class="fa fa-check"></i><b>2.4</b> アルゴリズム1 <span class="math inline">\(k\)</span>-最近傍法</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-1-k-.html"><a href="2-4-1-k-.html#k-"><i class="fa fa-check"></i><b>2.4.1</b> <span class="math inline">\(k\)</span>-最近傍法によるクラス分類</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-1-k-.html"><a href="2-4-1-k-.html#kneighborsclassifier"><i class="fa fa-check"></i><b>2.4.2</b> KNeighborsClassifierの解析</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-4-1-k-.html"><a href="2-4-1-k-.html#k-"><i class="fa fa-check"></i><b>2.4.3</b> <span class="math inline">\(k\)</span>-近傍回帰</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-4-1-k-.html"><a href="2-4-1-k-.html#kneighborsregressor"><i class="fa fa-check"></i><b>2.4.4</b> KNeighborsRegressorの解析</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-4-1-k-.html"><a href="2-4-1-k-.html#section-2.4.5"><i class="fa fa-check"></i><b>2.4.5</b> 利点と欠点とパラメータ</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-2-.html"><a href="2-5-2-.html"><i class="fa fa-check"></i><b>2.5</b> アルゴリズム2 線形モデル</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-5-2-.html"><a href="2-5-2-.html#section-2.5.1"><i class="fa fa-check"></i><b>2.5.1</b> 線形モデルによる回帰</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-5-2-.html"><a href="2-5-2-.html#section-2.5.2"><i class="fa fa-check"></i><b>2.5.2</b> 線形回帰(通常最小二乗法)</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-5-2-.html"><a href="2-5-2-.html#section-2.5.3"><i class="fa fa-check"></i><b>2.5.3</b> リッジ回帰</a></li>
<li class="chapter" data-level="2.5.4" data-path="2-5-2-.html"><a href="2-5-2-.html#lasso"><i class="fa fa-check"></i><b>2.5.4</b> Lasso</a></li>
<li class="chapter" data-level="2.5.5" data-path="2-5-2-.html"><a href="2-5-2-.html#section-2.5.5"><i class="fa fa-check"></i><b>2.5.5</b> クラス分類のための線形モデル</a></li>
<li class="chapter" data-level="2.5.6" data-path="2-5-2-.html"><a href="2-5-2-.html#section-2.5.6"><i class="fa fa-check"></i><b>2.5.6</b> 線形モデルによる多クラス分類</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown">
Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Pythonで始める機械学習の学習</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="1-k-" class="section level2">
<h2><span class="header-section-number">2.4</span> アルゴリズム1 <span class="math inline">\(k\)</span>-最近傍法</h2>
<ul>
<li>a.k.a. <span class="math inline">\(k\)</span>-NN</li>
<li>近いやつは大体おんなじ。</li>
</ul>
<div id="k-" class="section level3">
<h3><span class="header-section-number">2.4.1</span> <span class="math inline">\(k\)</span>-最近傍法によるクラス分類</h3>
<ul>
<li><span class="math inline">\(k\)</span>は参考にする近傍点の個数。</li>
<li>1-NNの例。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">mglearn.plots.plot_knn_classification(n_neighbors<span class="op">=</span><span class="dv">1</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-10-1.png" /><!-- --></p>
<ul>
<li>3-NNの例
<ul>
<li>近傍点が複数のときは多数決で決める。</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">mglearn.plots.plot_knn_classification(n_neighbors<span class="op">=</span><span class="dv">3</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-12-1.png" /><!-- --></p>
<ul>
<li><strong>scikit-learn</strong>でやる。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split
X, y <span class="op">=</span> mglearn.datasets.make_forge()
X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, random_state<span class="op">=</span><span class="dv">0</span>)
<span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier
clf <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">3</span>).fit(X_train, y_train)
<span class="bu">print</span>(clf.score(X_test, y_test))
 <span class="co">## 0.8571428571428571</span></code></pre></div>
</div>
<div id="kneighborsclassifier" class="section level3">
<h3><span class="header-section-number">2.4.2</span> KNeighborsClassifierの解析</h3>
<ul>
<li>特徴量が2つしかなければ、散布図が描ける。</li>
<li>散布図上のあらゆる箇所について<strong>もしその場所に点があったら</strong>と考えて判別ができる。</li>
<li>つまり、特徴量がつくる平面を分類クラスで塗り分けることができる。</li>
<li>境界線を<strong>決定境界</strong>と呼ぶ。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">3</span>))
<span class="cf">for</span> n, ax <span class="kw">in</span> <span class="bu">zip</span>([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>], axes):
    clf <span class="op">=</span> KNeighborsClassifier(n_neighbors <span class="op">=</span> n).fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill<span class="op">=</span><span class="va">True</span>, eps <span class="op">=</span> <span class="fl">0.5</span>, ax <span class="op">=</span> ax, alpha <span class="op">=</span> .<span class="dv">4</span>)
    mglearn.discrete_scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], y, ax <span class="op">=</span> ax)
    ax.set_title(<span class="st">&quot;</span><span class="sc">{}</span><span class="st"> neighbor(s)&quot;</span>.<span class="bu">format</span>(n))
    ax.set_xlabel(<span class="st">&quot;特徴量 0&quot;</span>)
    ax.set_ylabel(<span class="st">&quot;特徴量 1&quot;</span>)
axes[<span class="dv">0</span>].legend(loc<span class="op">=</span><span class="dv">3</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-15-1.png" /><!-- --></p>
<ul>
<li>近傍点が多いほど境界がなめらか = モデルは単純になる。
<ul>
<li>近傍点1 = 最も複雑なモデル</li>
<li>近傍点数 = データ数 -&gt; ただの多数決</li>
</ul></li>
<li>ということは近傍点数の数を増やしていくと、どこかで汎化能力のピークが…？</li>
<li><strong>cancer</strong>データセットで試してみる。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer
cancer <span class="op">=</span> load_breast_cancer()
X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(
  cancer.data, cancer.target, stratify <span class="op">=</span> cancer.target, random_state <span class="op">=</span> <span class="dv">66</span>
)
training_accuracy <span class="op">=</span> []
test_accuracy <span class="op">=</span> []
n_settings <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>)
<span class="cf">for</span> n <span class="kw">in</span> n_settings:
  clf <span class="op">=</span> KNeighborsClassifier(n_neighbors <span class="op">=</span> n).fit(X_train, y_train)
  training_accuracy.append(clf.score(X_train, y_train))
  test_accuracy.append(clf.score(X_test, y_test))
plt.plot(n_settings, training_accuracy, label <span class="op">=</span> <span class="st">&quot;訓練セット精度&quot;</span>)
plt.plot(n_settings, test_accuracy, label <span class="op">=</span> <span class="st">&quot;テストセット精度&quot;</span>)
plt.ylabel(<span class="st">&quot;精度&quot;</span>)
plt.xlabel(<span class="st">&quot;近傍点数&quot;</span>)
plt.legend()</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-17-1.png" /><!-- --></p>
</div>
<div id="k-" class="section level3">
<h3><span class="header-section-number">2.4.3</span> <span class="math inline">\(k\)</span>-近傍回帰</h3>
<ul>
<li>kNNは回帰もできる。</li>
<li>1-NNでは近傍点の値が新しい観測値に対応する値だと考える。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">mglearn.plots.plot_knn_regression(n_neighbors <span class="op">=</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-19-1.png" /><!-- --></p>
<ul>
<li>近傍点が複数の時は平均値を使う。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">mglearn.plots.plot_knn_regression(n_neighbors <span class="op">=</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-21-1.png" /><!-- --></p>
<ul>
<li><strong>scikit-learn</strong>では、<strong>KNeighborsRegressor</strong>クラスに実装されてる。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsRegressor
X, y <span class="op">=</span> mglearn.datasets.make_wave(n_samples <span class="op">=</span> <span class="dv">40</span>)
X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, random_state<span class="op">=</span><span class="dv">0</span>)
reg <span class="op">=</span> KNeighborsRegressor(n_neighbors <span class="op">=</span> <span class="dv">3</span>).fit(X_train, y_train)
<span class="bu">print</span>(reg.score(X_test, y_test))
 <span class="co">## 0.8344172446249604</span></code></pre></div>
</div>
<div id="kneighborsregressor" class="section level3">
<h3><span class="header-section-number">2.4.4</span> KNeighborsRegressorの解析</h3>
<ul>
<li>1次元のデータセットに対する予測値は、近傍点数<span class="math inline">\(k\)</span>に対してどのように変化するか？</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># プロット先を3つ作る</span>
fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">4</span>))
<span class="co"># -3〜3までの間にデータポイントを1000点作る</span>
line <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1000</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)
<span class="cf">for</span> n_neighbors, ax <span class="kw">in</span> <span class="bu">zip</span>([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>], axes):
  reg <span class="op">=</span> KNeighborsRegressor(n_neighbors <span class="op">=</span> n_neighbors)
  reg.fit(X_train, y_train)
  ax.plot(line, reg.predict(line))
  ax.plot(X_train, y_train, <span class="st">&#39;^&#39;</span>)
  ax.plot(X_test, y_test, <span class="st">&#39;v&#39;</span>)
  ax.set_title(
    <span class="st">&quot;</span><span class="sc">{}</span><span class="st"> 近傍点</span><span class="ch">\n</span><span class="st"> 訓練スコア: </span><span class="sc">{:.2f}</span><span class="st"> テストスコア</span><span class="sc">{:.2f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(
      n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test)))
  ax.set_xlabel(<span class="st">&quot;特徴量&quot;</span>)
  ax.set_ylabel(<span class="st">&quot;目的変数&quot;</span>)
  
axes[<span class="dv">0</span>].legend([<span class="st">&quot;モデルによる予測値&quot;</span>, <span class="st">&quot;訓練データ&quot;</span>, <span class="st">&quot;テストデータ&quot;</span>], loc<span class="op">=</span><span class="st">&quot;best&quot;</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-24-1.png" /><!-- --></p>
<ul>
<li><span class="math inline">\(k=1\)</span>の場合は予測値が全ての訓練データを通るので、モデルが不安定になる。</li>
<li>近傍点を増やしていくと予測は滑らかになるが、その反面訓練データへの適合度が下がる。</li>
</ul>
</div>
<div id="section-2.4.5" class="section level3">
<h3><span class="header-section-number">2.4.5</span> 利点と欠点とパラメータ</h3>
<ul>
<li>利点
<ul>
<li>モデルが理解しやすい。</li>
<li>あまり調整しなくても性能が出やすい。</li>
<li>モデル構築は高速</li>
</ul></li>
<li>欠点
<ul>
<li>訓練セットが大きくなると予測が遅くなる。
<ul>
<li>実際に使う前には前処理を行うことが重要。</li>
</ul></li>
<li>疎なデータセット(特徴量の多くが0である)に対しては十分な性能が出にくい。</li>
</ul></li>
</ul>
<p>上記の理由から、kNNは実際に使われることは少ない。</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-3-section-2-3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-5-2-.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/nozma/ml_with_python_note/edit/master/02_supervised_learning.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
