<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Pythonで始める機械学習の学習</title>
  <meta name="description" content="Pythonで始める機械学習の学習">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Pythonで始める機械学習の学習" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Pythonで始める機械学習の学習" />
  
  
  

<meta name="author" content="R. Ito">


<meta name="date" content="2018-04-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html">
<link rel="next" href="3-教師あり学習-2.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.0/grViz.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code > span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code > span.dt { color: #0057ae; } /* DataType */
code > span.dv { color: #b08000; } /* DecVal */
code > span.bn { color: #b08000; } /* BaseN */
code > span.fl { color: #b08000; } /* Float */
code > span.cn { color: #aa5500; } /* Constant */
code > span.ch { color: #924c9d; } /* Char */
code > span.sc { color: #3daee9; } /* SpecialChar */
code > span.st { color: #bf0303; } /* String */
code > span.vs { color: #bf0303; } /* VerbatimString */
code > span.ss { color: #ff5500; } /* SpecialString */
code > span.im { color: #ff5500; } /* Import */
code > span.co { color: #898887; } /* Comment */
code > span.do { color: #607880; } /* Documentation */
code > span.an { color: #ca60ca; } /* Annotation */
code > span.cv { color: #0095ff; } /* CommentVar */
code > span.ot { color: #006e28; } /* Other */
code > span.fu { color: #644a9b; } /* Function */
code > span.va { color: #0057ae; } /* Variable */
code > span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code > span.op { color: #1f1c1b; } /* Operator */
code > span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code > span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code > span.pp { color: #006e28; } /* Preprocessor */
code > span.at { color: #0057ae; } /* Attribute */
code > span.re { color: #0057ae; } /* RegionMarker */
code > span.in { color: #b08000; } /* Information */
code > span.wa { color: #bf0303; } /* Warning */
code > span.al { color: #bf0303; font-weight: bold; } /* Alert */
code > span.er { color: #bf0303; text-decoration: underline; } /* Error */
code > span. { color: #1f1c1b; } /* Normal */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>まえおき</a><ul>
<li class="chapter" data-level="" data-path="方針とか.html"><a href="方針とか.html"><i class="fa fa-check"></i>方針とか</a></li>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html"><i class="fa fa-check"></i>実行環境とか</a><ul>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html#サンプルコード実行用jupyter-notebook"><i class="fa fa-check"></i>サンプルコード実行用Jupyter notebook</a></li>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html#この文章を執筆しているrstudio"><i class="fa fa-check"></i>この文章を執筆しているRStudio</a></li>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html#r-sessioninfo"><i class="fa fa-check"></i>R sessionInfo</a></li>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html#python環境"><i class="fa fa-check"></i>Python環境</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-はじめに.html"><a href="1-はじめに.html"><i class="fa fa-check"></i><b>1</b> はじめに</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-なぜ機械学習なのか.html"><a href="1-1-なぜ機械学習なのか.html"><i class="fa fa-check"></i><b>1.1</b> なぜ機械学習なのか</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-なぜ機械学習なのか.html"><a href="1-1-なぜ機械学習なのか.html#機械学習で解決可能な問題"><i class="fa fa-check"></i><b>1.1.1</b> 機械学習で解決可能な問題</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-なぜ機械学習なのか.html"><a href="1-1-なぜ機械学習なのか.html#タスクを知りデータを知る"><i class="fa fa-check"></i><b>1.1.2</b> タスクを知り、データを知る</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-なぜpythonなのか.html"><a href="1-2-なぜpythonなのか.html"><i class="fa fa-check"></i><b>1.2</b> なぜPythonなのか？</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-scikit-learn.html"><a href="1-3-scikit-learn.html"><i class="fa fa-check"></i><b>1.3</b> scikit-learn</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-scikit-learn.html"><a href="1-3-scikit-learn.html#インストール"><i class="fa fa-check"></i><b>1.3.1</b> インストール</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-必要なライブラリとツール.html"><a href="1-4-必要なライブラリとツール.html"><i class="fa fa-check"></i><b>1.4</b> 必要なライブラリとツール</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-python-2-vs-python-3.html"><a href="1-5-python-2-vs-python-3.html"><i class="fa fa-check"></i><b>1.5</b> Python 2 vs. Python 3</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html"><i class="fa fa-check"></i><b>1.6</b> 最初のアプリケーション: アイリスのクラス分類</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#データを読む"><i class="fa fa-check"></i><b>1.6.1</b> データを読む</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#成功度合いの測定-訓練データとテストデータ"><i class="fa fa-check"></i><b>1.6.2</b> 成功度合いの測定: 訓練データとテストデータ</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#最初にすべきこと-データを良く観察する"><i class="fa fa-check"></i><b>1.6.3</b> 最初にすべきこと: データを良く観察する</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#最初のモデル-k-最近傍法"><i class="fa fa-check"></i><b>1.6.4</b> 最初のモデル: k-最近傍法</a></li>
<li class="chapter" data-level="1.6.5" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#予測を行う"><i class="fa fa-check"></i><b>1.6.5</b> 予測を行う</a></li>
<li class="chapter" data-level="1.6.6" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#モデルの評価"><i class="fa fa-check"></i><b>1.6.6</b> モデルの評価</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-教師あり学習-1.html"><a href="2-教師あり学習-1.html"><i class="fa fa-check"></i><b>2</b> 教師あり学習 (1)</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-クラス分類と回帰.html"><a href="2-1-クラス分類と回帰.html"><i class="fa fa-check"></i><b>2.1</b> クラス分類と回帰</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-汎化過剰適合適合不足.html"><a href="2-2-汎化過剰適合適合不足.html"><i class="fa fa-check"></i><b>2.2</b> 汎化、過剰適合、適合不足</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-汎化過剰適合適合不足.html"><a href="2-2-汎化過剰適合適合不足.html#モデルの複雑さとデータセットの大きさ"><i class="fa fa-check"></i><b>2.2.1</b> モデルの複雑さとデータセットの大きさ</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-教師あり機械学習アルゴリズム.html"><a href="2-3-教師あり機械学習アルゴリズム.html"><i class="fa fa-check"></i><b>2.3</b> 教師あり機械学習アルゴリズム</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-教師あり機械学習アルゴリズム.html"><a href="2-3-教師あり機械学習アルゴリズム.html#サンプルデータセット"><i class="fa fa-check"></i><b>2.3.1</b> サンプルデータセット</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html"><i class="fa fa-check"></i><b>2.4</b> アルゴリズム1 <span class="math inline">\(k\)</span>-最近傍法</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#k-最近傍法によるクラス分類"><i class="fa fa-check"></i><b>2.4.1</b> <span class="math inline">\(k\)</span>-最近傍法によるクラス分類</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#kneighborsclassifierの解析"><i class="fa fa-check"></i><b>2.4.2</b> KNeighborsClassifierの解析</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#k-近傍回帰"><i class="fa fa-check"></i><b>2.4.3</b> <span class="math inline">\(k\)</span>-近傍回帰</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#kneighborsregressorの解析"><i class="fa fa-check"></i><b>2.4.4</b> KNeighborsRegressorの解析</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#利点と欠点とパラメータ"><i class="fa fa-check"></i><b>2.4.5</b> 利点と欠点とパラメータ</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html"><i class="fa fa-check"></i><b>2.5</b> アルゴリズム2 線形モデル</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#線形モデルによる回帰"><i class="fa fa-check"></i><b>2.5.1</b> 線形モデルによる回帰</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#線形回帰通常最小二乗法"><i class="fa fa-check"></i><b>2.5.2</b> 線形回帰(通常最小二乗法)</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#リッジ回帰"><i class="fa fa-check"></i><b>2.5.3</b> リッジ回帰</a></li>
<li class="chapter" data-level="2.5.4" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#lasso"><i class="fa fa-check"></i><b>2.5.4</b> Lasso</a></li>
<li class="chapter" data-level="2.5.5" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#クラス分類のための線形モデル"><i class="fa fa-check"></i><b>2.5.5</b> クラス分類のための線形モデル</a></li>
<li class="chapter" data-level="2.5.6" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#線形モデルによる多クラス分類"><i class="fa fa-check"></i><b>2.5.6</b> 線形モデルによる多クラス分類</a></li>
<li class="chapter" data-level="2.5.7" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#利点欠点パラメータ"><i class="fa fa-check"></i><b>2.5.7</b> 利点、欠点、パラメータ</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html"><a href="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html"><i class="fa fa-check"></i><b>2.6</b> アルゴリズム3 ナイーブベイズクラス分類器</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html"><a href="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html#利点欠点パラメータ-1"><i class="fa fa-check"></i><b>2.6.1</b> 利点、欠点、パラメータ</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html"><i class="fa fa-check"></i><b>2.7</b> アルゴリズム4 決定木</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#決定木の構築"><i class="fa fa-check"></i><b>2.7.1</b> 決定木の構築</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#決定木の複雑さの制御"><i class="fa fa-check"></i><b>2.7.2</b> 決定木の複雑さの制御</a></li>
<li class="chapter" data-level="2.7.3" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#決定木の解析"><i class="fa fa-check"></i><b>2.7.3</b> 決定木の解析</a></li>
<li class="chapter" data-level="2.7.4" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#決定木の特徴量の重要性"><i class="fa fa-check"></i><b>2.7.4</b> 決定木の特徴量の重要性</a></li>
<li class="chapter" data-level="2.7.5" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#長所短所パラメータ"><i class="fa fa-check"></i><b>2.7.5</b> 長所、短所、パラメータ</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-教師あり学習-2.html"><a href="3-教師あり学習-2.html"><i class="fa fa-check"></i><b>3</b> 教師あり学習 (2)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-アルゴリズム5-決定木のアンサンブル法.html"><a href="3-1-アルゴリズム5-決定木のアンサンブル法.html"><i class="fa fa-check"></i><b>3.1</b> アルゴリズム5 決定木のアンサンブル法</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-アルゴリズム5-決定木のアンサンブル法.html"><a href="3-1-アルゴリズム5-決定木のアンサンブル法.html#ランダムフォレスト"><i class="fa fa-check"></i><b>3.1.1</b> ランダムフォレスト</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-アルゴリズム5-決定木のアンサンブル法.html"><a href="3-1-アルゴリズム5-決定木のアンサンブル法.html#勾配ブースティング回帰木勾配ブースティングマシン"><i class="fa fa-check"></i><b>3.1.2</b> 勾配ブースティング回帰木(勾配ブースティングマシン)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><i class="fa fa-check"></i><b>3.2</b> アルゴリズム6 カーネル法を用いたサポートベクタマシン</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#線形モデルと非線形特徴量"><i class="fa fa-check"></i><b>3.2.1</b> 線形モデルと非線形特徴量</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#カーネルトリック"><i class="fa fa-check"></i><b>3.2.2</b> カーネルトリック</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#svmを理解する"><i class="fa fa-check"></i><b>3.2.3</b> SVMを理解する</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#svmパラメータの調整"><i class="fa fa-check"></i><b>3.2.4</b> SVMパラメータの調整</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#svmのためのデータの前処理"><i class="fa fa-check"></i><b>3.2.5</b> SVMのためのデータの前処理</a></li>
<li class="chapter" data-level="3.2.6" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#利点欠点パラメータ-2"><i class="fa fa-check"></i><b>3.2.6</b> 利点、欠点、パラメータ</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-ニューラルネットワークディープラーニング.html"><a href="3-3-ニューラルネットワークディープラーニング.html"><i class="fa fa-check"></i><b>3.3</b> ニューラルネットワーク(ディープラーニング)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-ニューラルネットワークディープラーニング.html"><a href="3-3-ニューラルネットワークディープラーニング.html#ニューラルネットワークモデル"><i class="fa fa-check"></i><b>3.3.1</b> ニューラルネットワークモデル</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-3-ニューラルネットワークディープラーニング.html"><a href="3-3-ニューラルネットワークディープラーニング.html#ニューラルネットワークのチューニング"><i class="fa fa-check"></i><b>3.3.2</b> ニューラルネットワークのチューニング</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-3-ニューラルネットワークディープラーニング.html"><a href="3-3-ニューラルネットワークディープラーニング.html#長所短所パラメータ-3"><i class="fa fa-check"></i><b>3.3.3</b> 長所、短所、パラメータ</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown">
Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Pythonで始める機械学習の学習</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="アルゴリズム4-決定木" class="section level2">
<h2><span class="header-section-number">2.7</span> アルゴリズム4 決定木</h2>
<ul>
<li>このセクションのためにはgraphvizをインストールしておく必要がある。
<ul>
<li><code>pip install graphviz</code>以外に、別途OSに応じた方法でgraphvizをインストール。</li>
<li>ubuntuならば<code>sudo apt-get install graphviz</code>。</li>
</ul></li>
<li>回帰にも分類にも使える。</li>
<li>Yes/Noで答えられる質問で出来た<strong>木</strong>を構成する。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">mglearn.plots.plot_animal_tree()</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-63-1.png" /><!-- --></p>
<div id="決定木の構築" class="section level3">
<h3><span class="header-section-number">2.7.1</span> 決定木の構築</h3>
<ul>
<li>2つの特徴量、2つのクラスを持つデータセットtwo_moonを使用する。
<ul>
<li>2つの特徴量のなす平面上で2つのクラスが半月を組合せたように分布している。</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons
<span class="im">from</span> mglearn.tools <span class="im">import</span> discrete_scatter
X, y <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">100</span>, noise<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">3</span>)
plt.figure()
ax <span class="op">=</span> plt.gca()
discrete_scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], y, ax<span class="op">=</span>ax)
ax.set_xticks(())
ax.set_yticks(())</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-65-1.png" /><!-- --></p>
<ul>
<li>木の構築は、データセットの分割の繰り返しである。分割された部分を<strong>葉</strong>と呼ぶ。</li>
<li>分割によりテストが1段階増える(e.g. X[1]は0.06以上であるか？)</li>
<li>各ステップで分割は情報量が最も多くなるように（最もクラスを分割するように）行われる。</li>
<li>分割はテストによってデータセットが完全に分類できるようになるまで進む。</li>
<li>1つの葉に1種類のクラスや値しか含まない状態になった木を<strong>純粋</strong>(pure)と呼ぶ。</li>
</ul>
<p>以下にtwo_moonから純粋な決定木を作成する過程を示す。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="cf">for</span> i, max_depth <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">9</span>]):
  fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">4</span>), subplot_kw<span class="op">=</span>{<span class="st">&#39;xticks&#39;</span>: (), <span class="st">&#39;yticks&#39;</span>: ()})
  tree <span class="op">=</span> mglearn.plot_interactive_tree.plot_tree(X, y, max_depth <span class="op">=</span> max_depth, ax <span class="op">=</span> ax[<span class="dv">0</span>])
  ax[<span class="dv">1</span>].imshow(mglearn.plot_interactive_tree.tree_image(tree))
  plt.show()
  plt.close()</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-66-1.png" /><!-- --><img src="02_supervised_learning_files/figure-html/unnamed-chunk-66-2.png" /><!-- --><img src="02_supervised_learning_files/figure-html/unnamed-chunk-66-3.png" /><!-- --></p>
<p>決定木はターゲットがクラスではなく連続値になっても同じように機能するので、回帰にも使える。</p>
</div>
<div id="決定木の複雑さの制御" class="section level3">
<h3><span class="header-section-number">2.7.2</span> 決定木の複雑さの制御</h3>
<ul>
<li>純粋になるまで分割を続けるとルールが複雑になりすぎ、容易に過剰適合してしまう。</li>
<li>過剰適合を防ぐ戦略は2つある。
<ul>
<li>事前枝刈り: 構築過程で木の生成を止める。単に枝刈りとも。
<ul>
<li>木の深さを制限する方法、葉の最大値を制限する方法、葉に含まれるデータ点の最小数を制限する方法がある。</li>
<li>scikit-learnには事前枝刈りしか実装されていない。</li>
</ul></li>
<li>事後枝刈り: 木を構築してから情報量の少ない枝を削除する。</li>
</ul></li>
<li>scikit-learnの決定木の実装
<ul>
<li>回帰: DecisionTreeRegressorクラス</li>
<li>分類: DecisionTreeClassifierクラス</li>
</ul></li>
</ul>
<p>以下ではcancerデータに対して決定木を作成し、枝刈りの効果を確認する。まずはデフォルトの設定で訓練セットに対して木を構築する。デフォルトでは葉が純粋になるまで分類する。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier
cancer <span class="op">=</span> load_breast_cancer()
X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(
  cancer.data, cancer.target, stratify<span class="op">=</span>cancer.target, random_state <span class="op">=</span> <span class="dv">42</span>
)
tree <span class="op">=</span> DecisionTreeClassifier(random_state <span class="op">=</span> <span class="dv">0</span>) <span class="co"># 内部でタイブレークの判定に使う乱数を固定している</span>
tree.fit(X_train, y_train)
<span class="bu">print</span>(<span class="st">&quot;訓練セットに対する精度:</span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(tree.score(X_train, y_train)))
 <span class="co">## 訓練セットに対する精度:1.000</span>
<span class="bu">print</span>(<span class="st">&quot;テストセットに対する精度:</span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(tree.score(X_test, y_test)))
 <span class="co">## テストセットに対する精度:0.937</span></code></pre></div>
<ul>
<li>葉が純粋になるまで分割しているので、訓練セットに対する精度は当然1になる。</li>
<li>テストセットに対する制度は線形モデルの例で見た時より若干低い。</li>
</ul>
<p>次に、枝刈りの例として木の深さを4に固定してみる。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tree <span class="op">=</span> DecisionTreeClassifier(max_depth <span class="op">=</span> <span class="dv">4</span>, random_state <span class="op">=</span> <span class="dv">0</span>)
tree.fit(X_train, y_train)
<span class="bu">print</span>(<span class="st">&quot;訓練セットに対する精度:</span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(tree.score(X_train, y_train)))
 <span class="co">## 訓練セットに対する精度:0.988</span>
<span class="bu">print</span>(<span class="st">&quot;テストセットに対する精度:</span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(tree.score(X_test, y_test)))
 <span class="co">## テストセットに対する精度:0.951</span></code></pre></div>
<p>訓練セットに対する精度と引き換えに、汎化性能が向上していることが分かる。</p>
</div>
<div id="決定木の解析" class="section level3">
<h3><span class="header-section-number">2.7.3</span> 決定木の解析</h3>
<ul>
<li>木の可視化のために、まずは<code>tree</code>モジュールの<code>export_graphviz</code>関数でグラフを書き出す。</li>
<li>出力はグラフに対応するファイル形式である.dot形式のファイル。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.tree <span class="im">import</span> export_graphviz
export_graphviz(
  tree, out_file <span class="op">=</span> <span class="st">&quot;output/tree.dot&quot;</span>, class_names <span class="op">=</span> [<span class="st">&quot;malignant&quot;</span>, <span class="st">&quot;benign&quot;</span>],
  feature_names <span class="op">=</span> cancer.feature_names, impurity <span class="op">=</span> <span class="va">False</span>, filled <span class="op">=</span> <span class="va">True</span>
  )</code></pre></div>
<ul>
<li>.dotファイルの可視化はgraphvizモジュールで行う</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> graphviz
<span class="im">from</span> PIL <span class="im">import</span> Image
<span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;output/tree.dot&quot;</span>) <span class="im">as</span> f:
  dot_graph <span class="op">=</span> f.read()
g <span class="op">=</span> graphviz.Source(dot_graph)
g.<span class="bu">format</span> <span class="op">=</span> <span class="st">&quot;png&quot;</span>
g.render(<span class="st">&quot;output/tree.gv&quot;</span>)
img <span class="op">=</span> np.array(Image.<span class="bu">open</span>(<span class="st">&quot;output/tree.gv.png&quot;</span>))
plt.imshow(img)
plt.axis(<span class="st">&#39;off&#39;</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-71-1.png" /><!-- --></p>
<p>可視化した木は多くの情報を含むが、特に<strong>大部分のデータが通るパスはどこか？</strong>に注目すると良い。</p>
</div>
<div id="決定木の特徴量の重要性" class="section level3">
<h3><span class="header-section-number">2.7.4</span> 決定木の特徴量の重要性</h3>
<p>決定木全体を確認し、把握するのは大変な作業なので、以下のような情報が使用される場合がある。</p>
<ul>
<li><strong>特徴量の重要度</strong> (feature importance) 個々の特徴量はそれぞれの判断に対してどの程度重要なのか？
<ul>
<li>1に近いほど重要。1であればその特徴量だけで完全に判別ができるということ。</li>
<li>0に近いほど重要ではない</li>
</ul></li>
</ul>
<p>特徴量の重要度はフィット済みオブジェクトの<code>.feature_importance_</code>に格納されている。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(tree.feature_importances_)
 <span class="co">## [0.         0.         0.         0.         0.         0.</span>
 <span class="co">##  0.         0.         0.         0.         0.01019737 0.04839825</span>
 <span class="co">##  0.         0.         0.0024156  0.         0.         0.</span>
 <span class="co">##  0.         0.         0.72682851 0.0458159  0.         0.</span>
 <span class="co">##  0.0141577  0.         0.018188   0.1221132  0.01188548 0.        ]</span></code></pre></div>
<p>このままではわかりにくいのでプロットしてみる。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">n_features <span class="op">=</span> cancer.data.shape[<span class="dv">1</span>]
plt.barh(<span class="bu">range</span>(n_features), tree.feature_importances_, align <span class="op">=</span> <span class="st">&#39;center&#39;</span>)
plt.yticks(np.arange(n_features), cancer.feature_names)
plt.xlabel(<span class="st">&quot;特徴量の重要度&quot;</span>)
plt.ylabel(<span class="st">&quot;特徴量&quot;</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-74-1.png" /><!-- --></p>
<ul>
<li><strong>特徴量の重要度が高い特徴量</strong>は重要だが、逆は必ずしも成り立たないという点に注意が必要である。
<ul>
<li>特徴量間に強い相関があり、いずれかの特徴量で十分説明出来てしまう場合は、残りの特徴量がたまたま採用されないということがありうる。</li>
</ul></li>
<li>特徴量の重要度は係数と異なって常に正であり、その特徴量が大きいとクラスがどれになるのかは直接は分からない。</li>
<li>上記の例ではworst radiusは少なくとも重要だが、他に重要な特徴量がある可能性は除外できないし、worst radiusの値と良性・悪性の関係がどのようになっているのかも自明ではない。</li>
</ul>
<p>そもそも、特徴量とクラスの関係は必ずしも単純とは限らない。例えば次のような2つの特徴量からなる2クラス分類問題を考えてみる。この例は、クラスを分けるルールは単純で明確だが、クラス1はクラス0の中に分布しているので、一定の大小関係だけでは分類できない。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tree <span class="op">=</span> mglearn.plots.plot_tree_not_monotone()
 <span class="co">## Feature importances: [0. 1.]</span></code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-76-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tree.<span class="bu">format</span> <span class="op">=</span> <span class="st">&quot;png&quot;</span>
tree.render(<span class="st">&quot;output/not_monotone.gv&quot;</span>)
img <span class="op">=</span> np.array(Image.<span class="bu">open</span>(<span class="st">&quot;output/not_monotone.gv.png&quot;</span>))
plt.imshow(img)
plt.axis(<span class="st">&#39;off&#39;</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-78-1.png" /><!-- --></p>
<ul>
<li>決定木による分類の議論は決定木による回帰にも当てはまる。</li>
<li>決定木による回帰では、<strong>外挿</strong> (extrapolate)ができない点に注意する。</li>
</ul>
<p>決定木は外挿ができないという点について、RAM価格の推移データセットを使って例を示そう。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> os
ram_prices <span class="op">=</span> pd.read_csv(os.path.join(mglearn.datasets.DATA_PATH, <span class="st">&quot;ram_price.csv&quot;</span>))
plt.semilogy(ram_prices.date, ram_prices.price)
plt.xlabel(<span class="st">&quot;年&quot;</span>)
plt.ylabel(<span class="st">&quot;1Mバイトあたりの価格($)&quot;</span>)</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-80-1.png" /><!-- --></p>
<p>関係を直線的にするために、価格を対数変換しているという点に注意してもらいたい。この種の変換は線形回帰を行う際に重要となる。</p>
<p>データセットに対し、線形回帰と回帰木を適用する。ここでは、2000年より前のデータを訓練セットとし、2000年以降のデータをテストセットとする。つまり、過去のデータから将来を予測する。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor
data_train <span class="op">=</span> ram_prices[ram_prices.date <span class="op">&lt;</span> <span class="dv">2000</span>]
data_test <span class="op">=</span> ram_prices[ram_prices.date <span class="op">&gt;=</span> <span class="dv">2000</span>]
X_train <span class="op">=</span> data_train.date[:, np.newaxis]
y_train <span class="op">=</span> np.log(data_train.price) <span class="co">#対数変換</span>
<span class="co"># モデルに訓練データをフィットさせる</span>
tree <span class="op">=</span> DecisionTreeRegressor().fit(X_train, y_train)
linear_reg <span class="op">=</span> LinearRegression().fit(X_train, y_train)
<span class="co"># 2000年以降も含めた全てのデータポイントに対して予測を行う</span>
X_all <span class="op">=</span> ram_prices.date[:, np.newaxis]
pred_tree <span class="op">=</span> tree.predict(X_all)
pred_lr <span class="op">=</span> linear_reg.predict(X_all)
price_tree <span class="op">=</span> np.exp(pred_tree) <span class="co">#対数変換を解除</span>
price_lr <span class="op">=</span> np.exp(pred_lr)
<span class="co">## プロット</span>
plt.semilogy(data_train.date, data_train.price, label <span class="op">=</span> <span class="st">&quot;訓練データ&quot;</span>)
plt.semilogy(data_test.date, data_test.price, label <span class="op">=</span> <span class="st">&quot;テストデータ&quot;</span>)
plt.semilogy(ram_prices.date, price_tree, label <span class="op">=</span> <span class="st">&quot;回帰木&quot;</span>)
plt.semilogy(ram_prices.date, price_lr, label <span class="op">=</span> <span class="st">&quot;線形回帰&quot;</span>)
plt.legend()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.tight_layout()
plt.show()</code></pre></div>
<p><img src="02_supervised_learning_files/figure-html/unnamed-chunk-82-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.close()</code></pre></div>
<p>線形回帰は2000年以降の値も予測出来ているのに対して、回帰木は単に2000年の値を返すだけになっている。</p>
</div>
<div id="長所短所パラメータ" class="section level3">
<h3><span class="header-section-number">2.7.5</span> 長所、短所、パラメータ</h3>
<ul>
<li>決定木のパラメータは事前枝刈りに関するパラメータである。</li>
<li>大抵の場合はmax_depth、max_leaf_nodes、min_samples_leafのいずれか1つの指定で十分である。</li>
<li>決定木は容易に可視化可能であり、理解しやすい。</li>
<li>決定木の分割は特徴量毎に行われるため、特徴量を正規化したり標準化したりする必要はない。</li>
<li>特徴量の最大の欠点は事前枝刈りを行ったとしても過剰適合しやすく、汎化性能が低くなりやすいという点である。</li>
</ul>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-教師あり学習-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/nozma/ml_with_python_note/edit/master/02_supervised_learning.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
