<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Pythonで始める機械学習の学習</title>
  <meta name="description" content="Pythonで始める機械学習の学習">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Pythonで始める機械学習の学習" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Pythonで始める機械学習の学習" />
  
  
  

<meta name="author" content="R. Ito">


<meta name="date" content="2018-04-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="3-1-アルゴリズム5-決定木のアンサンブル法.html">
<link rel="next" href="3-3-ニューラルネットワークディープラーニング.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.0/grViz.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code > span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code > span.dt { color: #0057ae; } /* DataType */
code > span.dv { color: #b08000; } /* DecVal */
code > span.bn { color: #b08000; } /* BaseN */
code > span.fl { color: #b08000; } /* Float */
code > span.cn { color: #aa5500; } /* Constant */
code > span.ch { color: #924c9d; } /* Char */
code > span.sc { color: #3daee9; } /* SpecialChar */
code > span.st { color: #bf0303; } /* String */
code > span.vs { color: #bf0303; } /* VerbatimString */
code > span.ss { color: #ff5500; } /* SpecialString */
code > span.im { color: #ff5500; } /* Import */
code > span.co { color: #898887; } /* Comment */
code > span.do { color: #607880; } /* Documentation */
code > span.an { color: #ca60ca; } /* Annotation */
code > span.cv { color: #0095ff; } /* CommentVar */
code > span.ot { color: #006e28; } /* Other */
code > span.fu { color: #644a9b; } /* Function */
code > span.va { color: #0057ae; } /* Variable */
code > span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code > span.op { color: #1f1c1b; } /* Operator */
code > span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code > span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code > span.pp { color: #006e28; } /* Preprocessor */
code > span.at { color: #0057ae; } /* Attribute */
code > span.re { color: #0057ae; } /* RegionMarker */
code > span.in { color: #b08000; } /* Information */
code > span.wa { color: #bf0303; } /* Warning */
code > span.al { color: #bf0303; font-weight: bold; } /* Alert */
code > span.er { color: #bf0303; text-decoration: underline; } /* Error */
code > span. { color: #1f1c1b; } /* Normal */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>まえおき</a><ul>
<li class="chapter" data-level="" data-path="方針とか.html"><a href="方針とか.html"><i class="fa fa-check"></i>方針とか</a></li>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html"><i class="fa fa-check"></i>実行環境とか</a><ul>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html#サンプルコード実行用jupyter-notebook"><i class="fa fa-check"></i>サンプルコード実行用Jupyter notebook</a></li>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html#この文章を執筆しているrstudio"><i class="fa fa-check"></i>この文章を執筆しているRStudio</a></li>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html#r-sessioninfo"><i class="fa fa-check"></i>R sessionInfo</a></li>
<li class="chapter" data-level="" data-path="実行環境とか.html"><a href="実行環境とか.html#python環境"><i class="fa fa-check"></i>Python環境</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-はじめに.html"><a href="1-はじめに.html"><i class="fa fa-check"></i><b>1</b> はじめに</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-なぜ機械学習なのか.html"><a href="1-1-なぜ機械学習なのか.html"><i class="fa fa-check"></i><b>1.1</b> なぜ機械学習なのか</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-なぜ機械学習なのか.html"><a href="1-1-なぜ機械学習なのか.html#機械学習で解決可能な問題"><i class="fa fa-check"></i><b>1.1.1</b> 機械学習で解決可能な問題</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-なぜ機械学習なのか.html"><a href="1-1-なぜ機械学習なのか.html#タスクを知りデータを知る"><i class="fa fa-check"></i><b>1.1.2</b> タスクを知り、データを知る</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-なぜpythonなのか.html"><a href="1-2-なぜpythonなのか.html"><i class="fa fa-check"></i><b>1.2</b> なぜPythonなのか？</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-scikit-learn.html"><a href="1-3-scikit-learn.html"><i class="fa fa-check"></i><b>1.3</b> scikit-learn</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-scikit-learn.html"><a href="1-3-scikit-learn.html#インストール"><i class="fa fa-check"></i><b>1.3.1</b> インストール</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-必要なライブラリとツール.html"><a href="1-4-必要なライブラリとツール.html"><i class="fa fa-check"></i><b>1.4</b> 必要なライブラリとツール</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-python-2-vs-python-3.html"><a href="1-5-python-2-vs-python-3.html"><i class="fa fa-check"></i><b>1.5</b> Python 2 vs. Python 3</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html"><i class="fa fa-check"></i><b>1.6</b> 最初のアプリケーション: アイリスのクラス分類</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#データを読む"><i class="fa fa-check"></i><b>1.6.1</b> データを読む</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#成功度合いの測定-訓練データとテストデータ"><i class="fa fa-check"></i><b>1.6.2</b> 成功度合いの測定: 訓練データとテストデータ</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#最初にすべきこと-データを良く観察する"><i class="fa fa-check"></i><b>1.6.3</b> 最初にすべきこと: データを良く観察する</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#最初のモデル-k-最近傍法"><i class="fa fa-check"></i><b>1.6.4</b> 最初のモデル: k-最近傍法</a></li>
<li class="chapter" data-level="1.6.5" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#予測を行う"><i class="fa fa-check"></i><b>1.6.5</b> 予測を行う</a></li>
<li class="chapter" data-level="1.6.6" data-path="1-6-最初のアプリケーション-アイリスのクラス分類.html"><a href="1-6-最初のアプリケーション-アイリスのクラス分類.html#モデルの評価"><i class="fa fa-check"></i><b>1.6.6</b> モデルの評価</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-教師あり学習-1.html"><a href="2-教師あり学習-1.html"><i class="fa fa-check"></i><b>2</b> 教師あり学習 (1)</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-クラス分類と回帰.html"><a href="2-1-クラス分類と回帰.html"><i class="fa fa-check"></i><b>2.1</b> クラス分類と回帰</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-汎化過剰適合適合不足.html"><a href="2-2-汎化過剰適合適合不足.html"><i class="fa fa-check"></i><b>2.2</b> 汎化、過剰適合、適合不足</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-汎化過剰適合適合不足.html"><a href="2-2-汎化過剰適合適合不足.html#モデルの複雑さとデータセットの大きさ"><i class="fa fa-check"></i><b>2.2.1</b> モデルの複雑さとデータセットの大きさ</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-教師あり機械学習アルゴリズム.html"><a href="2-3-教師あり機械学習アルゴリズム.html"><i class="fa fa-check"></i><b>2.3</b> 教師あり機械学習アルゴリズム</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-教師あり機械学習アルゴリズム.html"><a href="2-3-教師あり機械学習アルゴリズム.html#サンプルデータセット"><i class="fa fa-check"></i><b>2.3.1</b> サンプルデータセット</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html"><i class="fa fa-check"></i><b>2.4</b> アルゴリズム1 <span class="math inline">\(k\)</span>-最近傍法</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#k-最近傍法によるクラス分類"><i class="fa fa-check"></i><b>2.4.1</b> <span class="math inline">\(k\)</span>-最近傍法によるクラス分類</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#kneighborsclassifierの解析"><i class="fa fa-check"></i><b>2.4.2</b> KNeighborsClassifierの解析</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#k-近傍回帰"><i class="fa fa-check"></i><b>2.4.3</b> <span class="math inline">\(k\)</span>-近傍回帰</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#kneighborsregressorの解析"><i class="fa fa-check"></i><b>2.4.4</b> KNeighborsRegressorの解析</a></li>
<li class="chapter" data-level="2.4.5" data-path="2-4-アルゴリズム1-k-最近傍法.html"><a href="2-4-アルゴリズム1-k-最近傍法.html#利点と欠点とパラメータ"><i class="fa fa-check"></i><b>2.4.5</b> 利点と欠点とパラメータ</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html"><i class="fa fa-check"></i><b>2.5</b> アルゴリズム2 線形モデル</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#線形モデルによる回帰"><i class="fa fa-check"></i><b>2.5.1</b> 線形モデルによる回帰</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#線形回帰通常最小二乗法"><i class="fa fa-check"></i><b>2.5.2</b> 線形回帰(通常最小二乗法)</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#リッジ回帰"><i class="fa fa-check"></i><b>2.5.3</b> リッジ回帰</a></li>
<li class="chapter" data-level="2.5.4" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#lasso"><i class="fa fa-check"></i><b>2.5.4</b> Lasso</a></li>
<li class="chapter" data-level="2.5.5" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#クラス分類のための線形モデル"><i class="fa fa-check"></i><b>2.5.5</b> クラス分類のための線形モデル</a></li>
<li class="chapter" data-level="2.5.6" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#線形モデルによる多クラス分類"><i class="fa fa-check"></i><b>2.5.6</b> 線形モデルによる多クラス分類</a></li>
<li class="chapter" data-level="2.5.7" data-path="2-5-アルゴリズム2-線形モデル.html"><a href="2-5-アルゴリズム2-線形モデル.html#利点欠点パラメータ"><i class="fa fa-check"></i><b>2.5.7</b> 利点、欠点、パラメータ</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html"><a href="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html"><i class="fa fa-check"></i><b>2.6</b> アルゴリズム3 ナイーブベイズクラス分類器</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html"><a href="2-6-アルゴリズム3-ナイーブベイズクラス分類器.html#利点欠点パラメータ-1"><i class="fa fa-check"></i><b>2.6.1</b> 利点、欠点、パラメータ</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html"><i class="fa fa-check"></i><b>2.7</b> アルゴリズム4 決定木</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#決定木の構築"><i class="fa fa-check"></i><b>2.7.1</b> 決定木の構築</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#決定木の複雑さの制御"><i class="fa fa-check"></i><b>2.7.2</b> 決定木の複雑さの制御</a></li>
<li class="chapter" data-level="2.7.3" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#決定木の解析"><i class="fa fa-check"></i><b>2.7.3</b> 決定木の解析</a></li>
<li class="chapter" data-level="2.7.4" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#決定木の特徴量の重要性"><i class="fa fa-check"></i><b>2.7.4</b> 決定木の特徴量の重要性</a></li>
<li class="chapter" data-level="2.7.5" data-path="2-7-アルゴリズム4-決定木.html"><a href="2-7-アルゴリズム4-決定木.html#長所短所パラメータ"><i class="fa fa-check"></i><b>2.7.5</b> 長所、短所、パラメータ</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-教師あり学習-2.html"><a href="3-教師あり学習-2.html"><i class="fa fa-check"></i><b>3</b> 教師あり学習 (2)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-アルゴリズム5-決定木のアンサンブル法.html"><a href="3-1-アルゴリズム5-決定木のアンサンブル法.html"><i class="fa fa-check"></i><b>3.1</b> アルゴリズム5 決定木のアンサンブル法</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-アルゴリズム5-決定木のアンサンブル法.html"><a href="3-1-アルゴリズム5-決定木のアンサンブル法.html#ランダムフォレスト"><i class="fa fa-check"></i><b>3.1.1</b> ランダムフォレスト</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-アルゴリズム5-決定木のアンサンブル法.html"><a href="3-1-アルゴリズム5-決定木のアンサンブル法.html#勾配ブースティング回帰木勾配ブースティングマシン"><i class="fa fa-check"></i><b>3.1.2</b> 勾配ブースティング回帰木(勾配ブースティングマシン)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><i class="fa fa-check"></i><b>3.2</b> アルゴリズム6 カーネル法を用いたサポートベクタマシン</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#線形モデルと非線形特徴量"><i class="fa fa-check"></i><b>3.2.1</b> 線形モデルと非線形特徴量</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#カーネルトリック"><i class="fa fa-check"></i><b>3.2.2</b> カーネルトリック</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#svmを理解する"><i class="fa fa-check"></i><b>3.2.3</b> SVMを理解する</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#svmパラメータの調整"><i class="fa fa-check"></i><b>3.2.4</b> SVMパラメータの調整</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#svmのためのデータの前処理"><i class="fa fa-check"></i><b>3.2.5</b> SVMのためのデータの前処理</a></li>
<li class="chapter" data-level="3.2.6" data-path="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html"><a href="3-2-アルゴリズム6-カーネル法を用いたサポートベクタマシン.html#利点欠点パラメータ-2"><i class="fa fa-check"></i><b>3.2.6</b> 利点、欠点、パラメータ</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-ニューラルネットワークディープラーニング.html"><a href="3-3-ニューラルネットワークディープラーニング.html"><i class="fa fa-check"></i><b>3.3</b> ニューラルネットワーク(ディープラーニング)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-ニューラルネットワークディープラーニング.html"><a href="3-3-ニューラルネットワークディープラーニング.html#ニューラルネットワークモデル"><i class="fa fa-check"></i><b>3.3.1</b> ニューラルネットワークモデル</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-3-ニューラルネットワークディープラーニング.html"><a href="3-3-ニューラルネットワークディープラーニング.html#ニューラルネットワークのチューニング"><i class="fa fa-check"></i><b>3.3.2</b> ニューラルネットワークのチューニング</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-3-ニューラルネットワークディープラーニング.html"><a href="3-3-ニューラルネットワークディープラーニング.html#長所短所パラメータ-3"><i class="fa fa-check"></i><b>3.3.3</b> 長所、短所、パラメータ</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown">
Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Pythonで始める機械学習の学習</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="アルゴリズム6-カーネル法を用いたサポートベクタマシン" class="section level2">
<h2><span class="header-section-number">3.2</span> アルゴリズム6 カーネル法を用いたサポートベクタマシン</h2>
<ul>
<li>より複雑なモデルを可能とするため線形モデルを拡張したもの。</li>
<li>クラス分類にも回帰にも使える(例はクラス分類だけ)。</li>
<li>背後にある数学はめっちゃむずい。</li>
</ul>
<div id="線形モデルと非線形特徴量" class="section level3">
<h3><span class="header-section-number">3.2.1</span> 線形モデルと非線形特徴量</h3>
<ul>
<li>線形モデルは特徴量を追加することで柔軟性が増す。
<ul>
<li>例: 特徴量の交互作用や多項式項を追加する。</li>
</ul></li>
</ul>
<p>過去に扱った、線形分離できない(単純な大小関係だけで判別できない)2クラス分類問題を例に扱う。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs
X, y <span class="op">=</span> make_blobs(centers<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">8</span>)
y <span class="op">=</span> y <span class="op">%</span> <span class="dv">2</span>
mglearn.discrete_scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], y)
plt.xlabel(<span class="st">&quot;特徴量0&quot;</span>)
plt.ylabel(<span class="st">&quot;特徴量1&quot;</span>)</code></pre></div>
<p><img src="02_supervised_learning_2_files/figure-html/unnamed-chunk-14-1.png" /><!-- --></p>
<p>このデータは明らかに直線では分離できない。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC
linear_svc <span class="op">=</span> LinearSVC().fit(X, y)
mglearn.plots.plot_2d_separator(linear_svc, X)
mglearn.discrete_scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>] ,y)
plt.xlabel(<span class="st">&quot;特徴量0&quot;</span>)
plt.ylabel(<span class="st">&quot;特徴量1&quot;</span>)</code></pre></div>
<p><img src="02_supervised_learning_2_files/figure-html/unnamed-chunk-16-1.png" /><!-- --></p>
<p>特徴量1の二乗を新たな特徴量として加え、データを3次元にしてみる。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">X_new <span class="op">=</span> np.hstack([X, X[:, <span class="dv">1</span>:] <span class="op">**</span> <span class="dv">2</span>])
<span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D, axes3d
figure <span class="op">=</span> plt.figure()
ax <span class="op">=</span> Axes3D(figure, elev<span class="op">=-</span><span class="dv">152</span>, azim<span class="op">=-</span><span class="dv">26</span>)
mask <span class="op">=</span> y <span class="op">==</span> <span class="dv">0</span>
ax.scatter(X_new[mask, <span class="dv">0</span>], X_new[mask, <span class="dv">1</span>], X_new[mask, <span class="dv">2</span>], c<span class="op">=</span><span class="st">&#39;b&#39;</span>)
ax.scatter(X_new[<span class="op">~</span>mask, <span class="dv">0</span>], X_new[<span class="op">~</span>mask, <span class="dv">1</span>], X_new[<span class="op">~</span>mask, <span class="dv">2</span>], c<span class="op">=</span><span class="st">&#39;r&#39;</span>, marker<span class="op">=</span><span class="st">&#39;^&#39;</span>)
ax.set_xlabel(<span class="st">&quot;特徴量0&quot;</span>)
ax.set_ylabel(<span class="st">&quot;特徴量1&quot;</span>)
ax.set_zlabel(<span class="st">&quot;特徴量1の2乗&quot;)</span></code></pre></div>
<p><img src="02_supervised_learning_2_files/figure-html/unnamed-chunk-18-1.png" /><!-- --></p>
<p>新しく追加した特徴量方向にはクラス0とクラス1が分離可能になっている。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">linear_svm_3d <span class="op">=</span> LinearSVC().fit(X_new, y)
coef, intercept <span class="op">=</span> linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_
figure <span class="op">=</span> plt.figure()
ax <span class="op">=</span> Axes3D(figure, elev<span class="op">=-</span><span class="dv">152</span>, azim<span class="op">=-</span><span class="dv">26</span>)
xx <span class="op">=</span> np.linspace(X_new[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">2</span>, X_new[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">2</span>, <span class="dv">50</span>)
yy <span class="op">=</span> np.linspace(X_new[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">2</span>, X_new[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">2</span>, <span class="dv">50</span>)
XX, YY <span class="op">=</span> np.meshgrid(xx, yy)
ZZ <span class="op">=</span> (coef[<span class="dv">0</span>] <span class="op">*</span> XX <span class="op">+</span> coef[<span class="dv">1</span>] <span class="op">*</span> YY <span class="op">+</span> intercept) <span class="op">/</span> <span class="op">-</span>coef[<span class="dv">2</span>]
ax.plot_surface(XX, YY, ZZ, rstride<span class="op">=</span><span class="dv">8</span>, cstride<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span>.<span class="dv">3</span>)
ax.scatter(X_new[mask, <span class="dv">0</span>], X_new[mask, <span class="dv">1</span>], X_new[mask, <span class="dv">2</span>], c<span class="op">=</span><span class="st">&#39;b&#39;</span>)
ax.scatter(X_new[<span class="op">~</span>mask, <span class="dv">0</span>], X_new[<span class="op">~</span>mask, <span class="dv">1</span>], X_new[<span class="op">~</span>mask, <span class="dv">2</span>], c<span class="op">=</span><span class="st">&#39;r&#39;</span>, marker<span class="op">=</span><span class="st">&#39;^&#39;</span>)
ax.set_xlabel(<span class="st">&quot;特徴量0&quot;</span>)
ax.set_ylabel(<span class="st">&quot;特徴量1&quot;</span>)
ax.set_zlabel(<span class="st">&quot;特徴量1の2乗&quot;)</span></code></pre></div>
<p><img src="02_supervised_learning_2_files/figure-html/unnamed-chunk-20-1.png" /><!-- --></p>
<p>これを元の空間で見ると、決定境界が直線から曲線になっていることが分かる。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">ZZ <span class="op">=</span> YY <span class="op">**</span> <span class="dv">2</span>
dec <span class="op">=</span> linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])
plt.contourf(XX, YY, dec.reshape(XX.shape), levels<span class="op">=</span>[dec.<span class="bu">min</span>(), <span class="dv">0</span>, dec.<span class="bu">max</span>()],
             cmap<span class="op">=</span>mglearn.cm2, alpha<span class="op">=</span>.<span class="dv">5</span>)
mglearn.discrete_scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], y)
plt.xlabel(<span class="st">&quot;特徴量0&quot;</span>)
plt.ylabel(<span class="st">&quot;特徴量1&quot;</span>)</code></pre></div>
<p><img src="02_supervised_learning_2_files/figure-html/unnamed-chunk-22-1.png" /><!-- --></p>
</div>
<div id="カーネルトリック" class="section level3">
<h3><span class="header-section-number">3.2.2</span> カーネルトリック</h3>
<ul>
<li>上の例では特徴量1の2乗を新しい特徴量として加えることで線形分離可能になったが、通常、<strong>どんな特徴量を追加すれば線形分離可能になるか</strong>は自明ではない。だからといって無闇に特徴量を増やせば計算量が増大してしまう。</li>
<li><strong>カーネルトリック</strong>(kernel trick)はこの問題に対応する手法。</li>
<li>カーネルトリックを使うと実際に特徴量を拡張することなしに、拡張後の空間での特徴量を直接計算できる。この特徴量を直接計算するための関数をカーネルと呼ぶ。例えばガウシアンカーネルと呼ばれるカーネルを使うと、無限次元の空間上で特徴量を計算したのと同じ効果が得られ、しかも計算量はデータ数のみに依存するため、大きくなりすぎる心配がない。
<ul>
<li>テイラー展開すると無限次元への写像になっていることが分かるらしい。
<ul>
<li>cf. <a href="https://qiita.com/keisuke-nakata/items/9d46152ac52dcd2cc92a">カーネルとは直感的に説明するとなんなのか？ - Qiita</a></li>
</ul></li>
<li>ガウシアンカーネルは放射基底関数(radial basis function: RBF)カーネルとも呼ぶ。</li>
</ul></li>
<li>cf. <a href="http://enakai00.hatenablog.com/entry/2017/10/13/145337">機械学習におけるカーネル法について - めもめも</a></li>
</ul>
</div>
<div id="svmを理解する" class="section level3">
<h3><span class="header-section-number">3.2.3</span> SVMを理解する</h3>
<ul>
<li>SVMは、決定境界の表現にとって個々のデータポイントがどの程度重要かを計算する。</li>
<li>基本的には2クラスの境界付近の少数のデータポイントのみが重要となり、これらのデータポイントは<strong>サポートベクタ</strong>と呼ばれる。</li>
<li>予測の際は新しいデータポイントとサポートベクタの距離が測定され、これとサポートベクタの重要性を考慮してクラスが決定される。</li>
<li>ガウシアンカーネルを使用するSVMでは次のガウシアンカーネルを用いて距離を計算する。</li>
</ul>
<p><span class="math display">\[k_{\mathrm{rbf}}(x_1, x_2) = \exp (- \gamma || x_1-x_2 || ^2) \]</span></p>
<ul>
<li><span class="math inline">\(x_1\)</span>、<span class="math inline">\(x_2\)</span>はデータポイントを、<span class="math inline">\(||x_1-x_2||\)</span>はユークリッド距離を表し、<span class="math inline">\(\gamma\)</span>は制御パラメータである。
<ul>
<li>注: <span class="math inline">\(\gamma = 1/\sigma^2\)</span>として<span class="math inline">\(\sigma\)</span>がパラメータとして説明される場合も多い。</li>
</ul></li>
</ul>
<p>SVMによる2クラス分類の結果を示す。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC
X, y <span class="op">=</span> mglearn.tools.make_handcrafted_dataset()
svm <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">&#39;rbf&#39;</span>, C<span class="op">=</span><span class="dv">10</span>, gamma<span class="op">=</span><span class="fl">0.1</span>).fit(X, y)
mglearn.plots.plot_2d_separator(svm, X, eps<span class="op">=</span>.<span class="dv">5</span>)
mglearn.discrete_scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], y)
sv <span class="op">=</span> svm.support_vectors_ <span class="co"># サポートベクタをプロット</span>
sv_labels <span class="op">=</span> svm.dual_coef_.ravel() <span class="op">&gt;</span> <span class="dv">0</span> <span class="co"># クラスラベルのニ値化</span>
mglearn.discrete_scatter(sv[:, <span class="dv">0</span>], sv[:, <span class="dv">1</span>], sv_labels, s<span class="op">=</span><span class="dv">15</span>, markeredgewidth<span class="op">=</span><span class="dv">3</span>)
plt.xlabel(<span class="st">&quot;特徴量0&quot;</span>)
plt.ylabel(<span class="st">&quot;特徴量1&quot;</span>)</code></pre></div>
<p><img src="02_supervised_learning_2_files/figure-html/unnamed-chunk-24-1.png" /><!-- --></p>
<p>カーネル法を用いたSVMの決定境界は非線形となる。</p>
</div>
<div id="svmパラメータの調整" class="section level3">
<h3><span class="header-section-number">3.2.4</span> SVMパラメータの調整</h3>
<ul>
<li><code>gamma</code>: ガウシアンカーネルの幅を調整する。</li>
<li><code>C</code>: 正則化パラメータ。</li>
</ul>
<p>パラメータと分類の関係を可視化する。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))
<span class="cf">for</span> ax, C <span class="kw">in</span> <span class="bu">zip</span>(axes, [<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>]):
  <span class="cf">for</span> a, gamma <span class="kw">in</span> <span class="bu">zip</span>(ax, <span class="bu">range</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)):
    mglearn.plots.plot_svm(log_C<span class="op">=</span>C, log_gamma<span class="op">=</span>gamma, ax<span class="op">=</span>a)
axes[<span class="dv">0</span>, <span class="dv">0</span>].legend([<span class="st">&quot;class 0&quot;</span>, <span class="st">&quot;class 1&quot;</span>, <span class="st">&quot;sv class 0&quot;</span>, <span class="st">&quot;sv class 1&quot;</span>],
                  ncol<span class="op">=</span><span class="dv">4</span>, loc<span class="op">=</span>(.<span class="dv">9</span>, <span class="fl">1.2</span>))</code></pre></div>
<p><img src="02_supervised_learning_2_files/figure-html/unnamed-chunk-26-1.png" /><!-- --></p>
<ul>
<li><code>gamma</code>は大きいほど個々のデータポイントを重視するようになり、モデルが複雑になる。</li>
<li><code>C</code>は小さいほどモデルを制限する点は線形モデルと同様。</li>
</ul>
<p><strong>cancer</strong>にRBFカーネルを用いたSVMを適用してみよう。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(
  cancer.data, cancer.target, random_state<span class="op">=</span><span class="dv">0</span>
)
svc <span class="op">=</span> SVC().fit(X_train, y_train)
<span class="bu">print</span>(<span class="st">&quot;訓練セットの精度: </span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(svc.score(X_train, y_train)))
 <span class="co">## 訓練セットの精度: 1.000</span>
<span class="bu">print</span>(<span class="st">&quot;テストセットの精度: </span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(svc.score(X_test, y_test)))
 <span class="co">## テストセットの精度: 0.629</span></code></pre></div>
<ul>
<li>訓練セットの精度が1であり、テストセットの精度が低く、過学習している。</li>
<li>SVMはパラメータとデータのスケールに敏感に反応する。
<ul>
<li>特にデータのスケールは全ての特徴量で揃っている必要がある。</li>
</ul></li>
</ul>
<p>データのスケールを確認するために、個々の特徴量に対して対数スケールで箱ひげ図を作成する。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.boxplot(X_train, sym<span class="op">=</span><span class="st">&#39;+&#39;</span>)
plt.ylim(<span class="dv">10</span><span class="op">**-</span><span class="dv">1</span>, <span class="dv">10</span><span class="op">**</span><span class="dv">4</span>)
plt.xlabel(<span class="st">&quot;特徴量のインデックス&quot;</span>)
plt.ylabel(<span class="st">&quot;特徴量の大きさ&quot;</span>)
plt.yscale(<span class="st">&quot;log&quot;</span>)</code></pre></div>
<p><img src="02_supervised_learning_2_files/figure-html/unnamed-chunk-29-1.png" /><!-- --></p>
</div>
<div id="svmのためのデータの前処理" class="section level3">
<h3><span class="header-section-number">3.2.5</span> SVMのためのデータの前処理</h3>
<ul>
<li><strong>cancer</strong>は前掲のようにデータのスケールが特徴量により著しく異なる。</li>
<li>前処理のちゃんとしたやり方は後で説明するので、ここではとりあえず手作業でスケールを揃える。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># 最小値の計算</span>
min_on_training <span class="op">=</span> X_train.<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">0</span>)
<span class="co"># レンジ = 最大値 - 最小値を計算</span>
range_on_training <span class="op">=</span> (X_train <span class="op">-</span> min_on_training).<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">0</span>)
<span class="co"># 最小値を引いてからレンジで割ることで、min=0、max=1に変換される</span>
X_train_scaled <span class="op">=</span> (X_train <span class="op">-</span> min_on_training) <span class="op">/</span> range_on_training
<span class="bu">print</span>(<span class="st">&quot;スケール済み特徴量の最小値: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(X_train_scaled.<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">0</span>)))
 <span class="co">## スケール済み特徴量の最小値: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.</span>
 <span class="co">##  0. 0. 0. 0. 0. 0.]</span>
<span class="bu">print</span>(<span class="st">&quot;スケール済み特徴量の最大値: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(X_train_scaled.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">0</span>)))
 <span class="co">## スケール済み特徴量の最大値: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.</span>
 <span class="co">##  1. 1. 1. 1. 1. 1.]</span></code></pre></div>
<p>テストセットについても同様の変換を行うが、最小値とレンジは訓練セットのものを使う。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">X_test_scaled <span class="op">=</span> (X_test <span class="op">-</span> min_on_training) <span class="op">/</span> range_on_training</code></pre></div>
<p>したがって、テストセットのレンジは0〜1ではない。この点については第4章で解説される（らしい）。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(<span class="st">&quot;テストセットの最小値: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(X_test_scaled.<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">0</span>)))
 <span class="co">## テストセットの最小値: [ 0.03540158  0.04190871  0.02895446  0.01497349  0.14260888  0.04999658</span>
 <span class="co">##   0.          0.          0.07222222  0.00589722  0.00105015 -0.00057494</span>
 <span class="co">##   0.00067851 -0.0007963   0.05148726  0.01434497  0.          0.</span>
 <span class="co">##   0.04195752  0.01113138  0.03678406  0.01252665  0.03366702  0.01400904</span>
 <span class="co">##   0.08531995  0.01833687  0.          0.          0.00749064  0.02367834]</span>
<span class="bu">print</span>(<span class="st">&quot;テストセットの最大値: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(X_test_scaled.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">0</span>)))
 <span class="co">## テストセットの最大値: [0.76809125 1.22697095 0.75813696 0.64750795 1.20310633 1.11643038</span>
 <span class="co">##  0.99906279 0.90606362 0.93232323 0.94903117 0.45573058 0.72623944</span>
 <span class="co">##  0.48593507 0.31641282 1.36082713 1.2784499  0.36313131 0.77476795</span>
 <span class="co">##  1.32643996 0.72672498 0.82106012 0.87553305 0.77887345 0.67803775</span>
 <span class="co">##  0.78603975 0.87843331 0.93450479 1.0024113  0.76384782 0.58743277]</span></code></pre></div>
<p>スケール済みの特徴量を使ってSVCを行う。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">svc <span class="op">=</span> SVC().fit(X_train_scaled, y_train)
<span class="bu">print</span>(<span class="st">&quot;訓練セットに対する精度: </span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(svc.score(X_train_scaled, y_train)))
 <span class="co">## 訓練セットに対する精度: 0.948</span>
<span class="bu">print</span>(<span class="st">&quot;テストセットに対する精度: </span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(svc.score(X_test_scaled, y_test)))
 <span class="co">## テストセットに対する精度: 0.951</span></code></pre></div>
<p>スケール前に比べるとかなりの改善が見られる。さらにパラメータを少し調整すると、さらに良くなる。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">svc <span class="op">=</span> SVC(C<span class="op">=</span><span class="dv">1000</span>).fit(X_train_scaled, y_train)
<span class="bu">print</span>(<span class="st">&quot;訓練セットに対する精度: </span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(svc.score(X_train_scaled, y_train)))
 <span class="co">## 訓練セットに対する精度: 0.988</span>
<span class="bu">print</span>(<span class="st">&quot;テストセットに対する精度: </span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(svc.score(X_test_scaled, y_test)))
 <span class="co">## テストセットに対する精度: 0.972</span></code></pre></div>
</div>
<div id="利点欠点パラメータ-2" class="section level3">
<h3><span class="header-section-number">3.2.6</span> 利点、欠点、パラメータ</h3>
<ul>
<li>いろいろなデータに対して上手く機能する強力なモデルである。
<ul>
<li>データに僅かな特徴量しかなくても決定境界が引ける。</li>
<li>低次元のデータでも高次元のデータでも機能する。</li>
<li>サンプルの個数が多いと上手くいかないことがある。
<ul>
<li>10,000サンプルくらいまではいけるが、100,000サンプルくらいになるとメモリや速度面から厳しくなってくる。</li>
</ul></li>
</ul></li>
<li>データの前処理とパラメータのチューニングは必須といえる。</li>
<li>モデルについて決定木のように誰にでも分かる形で説明するのは難しい。</li>
<li>パラメータは正則化パラメータ<code>C</code>とカーネル固有のパラメータがある。
<ul>
<li>RBFカーネル以外のカーネルを使うこともできる。</li>
<li>RBFカーネルにおける<code>C</code>と<code>gamma</code>は強く相関するため同時いチューニングする必要がある。</li>
</ul></li>
</ul>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-1-アルゴリズム5-決定木のアンサンブル法.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-3-ニューラルネットワークディープラーニング.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/nozma/ml_with_python_note/edit/master/02_supervised_learning_2.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
