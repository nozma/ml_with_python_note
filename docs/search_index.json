[
["index.html", "まえおき", " まえおき やるぞい(๑•̀ㅂ•́)و✧ （テキストのコードは見た目重視で冗長なので、どんどん省略していきます） "],
["section-1.html", "1 はじめに 1.1 なぜ機械学習なのか 1.2 なぜPythonなのか？ 1.3 scikit-learn 1.4 必要なライブラリとツール 1.5 Python 2 vs. Python 3 1.6 最初のアプリケーション: アイリスのクラス分類", " 1 はじめに 1.1 なぜ機械学習なのか if〜then〜のエキスパートシステムには限界がある。 ロジックがドメイン固有のものになり、タスクが変化したら作り直さないといけない。 人間のエキスパートの思考に対する深い理解が必要。 顔認識とかはちょっと無理。 1.1.1 機械学習で解決可能な問題 世の中には教師のある学習と教師のない学習がある。 教師あり学習 入力と出力のペアを与えてアルゴリズムを学習させる。 未知の入力に対する出力を予測する。 データセットを集めることが大変な場合もある。 教師なし学習 出力のないデータセットでアルゴリズムを学習させる。 トピック解析やクラスタリング、異常検知など。 サンプルと特徴量。 サンプル 個々のエンティティまたは量。 特徴量 エンティティの持つ特性を表現する列。 情報量のないデータからは学習できないことを覚えておくこと。 1.1.2 タスクを知り、データを知る よくデータを理解すること そのデータでそもそも問題を解決できるのか？ どんな機械学習の問題に置き換えるべきか？ データの数は十分か？ アルゴリズムは問題の一部にすぎない。全体を心に留めておくこと。 1.2 なぜPythonなのか？ つよい 汎用言語の強さとドメイン特価スクリプト言語の強さがある。 Jupyter Notebookが使える データ解析はインタラクティブな過程。 1.3 scikit-learn Pythonで機械学習といったらこれ オープンソース ユーザーガイドも読んでおこうな http://scikit-learn.org/stable/user_guide.html 1.3.1 インストール $ pip install numpy scipy matplotlib ipython scikit-learn pandas pillow 最近はanaconda使わずにvenvやDocker使うのが流れっぽい気がしますがPython界隈の環境構築はすぐに主流が変わるので正直よく分かりません。 1.4 必要なライブラリとツール Jupyter Notebook ブラウザ上でMarkdownとコード混ぜて書けるやつ。 NumPy 配列計算ﾒｯﾁｬﾊﾔｲやつ SciPy 科学技術計算できるやつ matplotlib プロットするやつ pandas Rのデータフレーム的な感じでデータ扱えるやつ mglearn 「こちらに予め調理したものがあります」的に教科書の例試せるやつ 1.5 Python 2 vs. Python 3 3でやれ。 1.6 最初のアプリケーション: アイリスのクラス分類 みんな大好きiris。 以下の記事が詳しい。 irisの正体 (R Advent Calendar 2012 6日目) - どんな鳥も 1.6.1 データを読む scikit-learnのdatasetsモジュール いろんなサンプルデータセットが入ってる。 sklearn.datasets.load_iris()でirisが返ってくるので適当に受けよう。 from sklearn.datasets import load_iris iris_dataset = load_iris() load_iris()はBunchクラスのオブジェクトを返す。これはディクショナリみたいに扱える。値にはiris_dataset['data']以外にiris_dataset.dataみたいにアクセスしてもいい。 print(iris_dataset.keys()) ## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;]) キーDESCRの中にデータセットの説明が入っている。 print(iris_dataset.DESCR) ## Iris Plants Database ## ==================== ## ## Notes ## ----- ## Data Set Characteristics: ## :Number of Instances: 150 (50 in each of three classes) ## :Number of Attributes: 4 numeric, predictive attributes and the class ## :Attribute Information: ## - sepal length in cm ## - sepal width in cm ## - petal length in cm ## - petal width in cm ## - class: ## - Iris-Setosa ## - Iris-Versicolour ## - Iris-Virginica ## :Summary Statistics: ## ## ============== ==== ==== ======= ===== ==================== ## Min Max Mean SD Class Correlation ## ============== ==== ==== ======= ===== ==================== ## sepal length: 4.3 7.9 5.84 0.83 0.7826 ## sepal width: 2.0 4.4 3.05 0.43 -0.4194 ## petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) ## petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ## ============== ==== ==== ======= ===== ==================== ## ## :Missing Attribute Values: None ## :Class Distribution: 33.3% for each of 3 classes. ## :Creator: R.A. Fisher ## :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) ## :Date: July, 1988 ## ## This is a copy of UCI ML iris datasets. ## http://archive.ics.uci.edu/ml/datasets/Iris ## ## The famous Iris database, first used by Sir R.A Fisher ## ## This is perhaps the best known database to be found in the ## pattern recognition literature. Fisher&#39;s paper is a classic in the field and ## is referenced frequently to this day. (See Duda &amp; Hart, for example.) The ## data set contains 3 classes of 50 instances each, where each class refers to a ## type of iris plant. One class is linearly separable from the other 2; the ## latter are NOT linearly separable from each other. ## ## References ## ---------- ## - Fisher,R.A. &quot;The use of multiple measurements in taxonomic problems&quot; ## Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to ## Mathematical Statistics&quot; (John Wiley, NY, 1950). ## - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis. ## (Q327.D83) John Wiley &amp; Sons. ISBN 0-471-22361-1. See page 218. ## - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System ## Structure and Classification Rule for Recognition in Partially Exposed ## Environments&quot;. IEEE Transactions on Pattern Analysis and Machine ## Intelligence, Vol. PAMI-2, No. 1, 67-71. ## - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;. IEEE Transactions ## on Information Theory, May 1972, 431-433. ## - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al&quot;s AUTOCLASS II ## conceptual clustering system finds 3 classes in the data. ## - Many, many more ... targetは符号化されていて、対応する名前はtarget_namesに入っている。 print(iris_dataset.target) ## [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## 2 2] print(iris_dataset.target_names) ## [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] 要するにtargetでtarget_namesを参照すると実際の目的変数の構造が見える。 print(iris_dataset.target_names[iris_dataset.target]) ## [&#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39;] 特徴量はデータ件数×特徴量数のNumPy配列としてdataに入っている。 print(iris_dataset.data) ## [[5.1 3.5 1.4 0.2] ## [4.9 3. 1.4 0.2] ## [4.7 3.2 1.3 0.2] ## [4.6 3.1 1.5 0.2] ## [5. 3.6 1.4 0.2] ## [5.4 3.9 1.7 0.4] ## [4.6 3.4 1.4 0.3] ## [5. 3.4 1.5 0.2] ## [4.4 2.9 1.4 0.2] ## [4.9 3.1 1.5 0.1] ## [5.4 3.7 1.5 0.2] ## [4.8 3.4 1.6 0.2] ## [4.8 3. 1.4 0.1] ## [4.3 3. 1.1 0.1] ## [5.8 4. 1.2 0.2] ## [5.7 4.4 1.5 0.4] ## [5.4 3.9 1.3 0.4] ## [5.1 3.5 1.4 0.3] ## [5.7 3.8 1.7 0.3] ## [5.1 3.8 1.5 0.3] ## [5.4 3.4 1.7 0.2] ## [5.1 3.7 1.5 0.4] ## [4.6 3.6 1. 0.2] ## [5.1 3.3 1.7 0.5] ## [4.8 3.4 1.9 0.2] ## [5. 3. 1.6 0.2] ## [5. 3.4 1.6 0.4] ## [5.2 3.5 1.5 0.2] ## [5.2 3.4 1.4 0.2] ## [4.7 3.2 1.6 0.2] ## [4.8 3.1 1.6 0.2] ## [5.4 3.4 1.5 0.4] ## [5.2 4.1 1.5 0.1] ## [5.5 4.2 1.4 0.2] ## [4.9 3.1 1.5 0.1] ## [5. 3.2 1.2 0.2] ## [5.5 3.5 1.3 0.2] ## [4.9 3.1 1.5 0.1] ## [4.4 3. 1.3 0.2] ## [5.1 3.4 1.5 0.2] ## [5. 3.5 1.3 0.3] ## [4.5 2.3 1.3 0.3] ## [4.4 3.2 1.3 0.2] ## [5. 3.5 1.6 0.6] ## [5.1 3.8 1.9 0.4] ## [4.8 3. 1.4 0.3] ## [5.1 3.8 1.6 0.2] ## [4.6 3.2 1.4 0.2] ## [5.3 3.7 1.5 0.2] ## [5. 3.3 1.4 0.2] ## [7. 3.2 4.7 1.4] ## [6.4 3.2 4.5 1.5] ## [6.9 3.1 4.9 1.5] ## [5.5 2.3 4. 1.3] ## [6.5 2.8 4.6 1.5] ## [5.7 2.8 4.5 1.3] ## [6.3 3.3 4.7 1.6] ## [4.9 2.4 3.3 1. ] ## [6.6 2.9 4.6 1.3] ## [5.2 2.7 3.9 1.4] ## [5. 2. 3.5 1. ] ## [5.9 3. 4.2 1.5] ## [6. 2.2 4. 1. ] ## [6.1 2.9 4.7 1.4] ## [5.6 2.9 3.6 1.3] ## [6.7 3.1 4.4 1.4] ## [5.6 3. 4.5 1.5] ## [5.8 2.7 4.1 1. ] ## [6.2 2.2 4.5 1.5] ## [5.6 2.5 3.9 1.1] ## [5.9 3.2 4.8 1.8] ## [6.1 2.8 4. 1.3] ## [6.3 2.5 4.9 1.5] ## [6.1 2.8 4.7 1.2] ## [6.4 2.9 4.3 1.3] ## [6.6 3. 4.4 1.4] ## [6.8 2.8 4.8 1.4] ## [6.7 3. 5. 1.7] ## [6. 2.9 4.5 1.5] ## [5.7 2.6 3.5 1. ] ## [5.5 2.4 3.8 1.1] ## [5.5 2.4 3.7 1. ] ## [5.8 2.7 3.9 1.2] ## [6. 2.7 5.1 1.6] ## [5.4 3. 4.5 1.5] ## [6. 3.4 4.5 1.6] ## [6.7 3.1 4.7 1.5] ## [6.3 2.3 4.4 1.3] ## [5.6 3. 4.1 1.3] ## [5.5 2.5 4. 1.3] ## [5.5 2.6 4.4 1.2] ## [6.1 3. 4.6 1.4] ## [5.8 2.6 4. 1.2] ## [5. 2.3 3.3 1. ] ## [5.6 2.7 4.2 1.3] ## [5.7 3. 4.2 1.2] ## [5.7 2.9 4.2 1.3] ## [6.2 2.9 4.3 1.3] ## [5.1 2.5 3. 1.1] ## [5.7 2.8 4.1 1.3] ## [6.3 3.3 6. 2.5] ## [5.8 2.7 5.1 1.9] ## [7.1 3. 5.9 2.1] ## [6.3 2.9 5.6 1.8] ## [6.5 3. 5.8 2.2] ## [7.6 3. 6.6 2.1] ## [4.9 2.5 4.5 1.7] ## [7.3 2.9 6.3 1.8] ## [6.7 2.5 5.8 1.8] ## [7.2 3.6 6.1 2.5] ## [6.5 3.2 5.1 2. ] ## [6.4 2.7 5.3 1.9] ## [6.8 3. 5.5 2.1] ## [5.7 2.5 5. 2. ] ## [5.8 2.8 5.1 2.4] ## [6.4 3.2 5.3 2.3] ## [6.5 3. 5.5 1.8] ## [7.7 3.8 6.7 2.2] ## [7.7 2.6 6.9 2.3] ## [6. 2.2 5. 1.5] ## [6.9 3.2 5.7 2.3] ## [5.6 2.8 4.9 2. ] ## [7.7 2.8 6.7 2. ] ## [6.3 2.7 4.9 1.8] ## [6.7 3.3 5.7 2.1] ## [7.2 3.2 6. 1.8] ## [6.2 2.8 4.8 1.8] ## [6.1 3. 4.9 1.8] ## [6.4 2.8 5.6 2.1] ## [7.2 3. 5.8 1.6] ## [7.4 2.8 6.1 1.9] ## [7.9 3.8 6.4 2. ] ## [6.4 2.8 5.6 2.2] ## [6.3 2.8 5.1 1.5] ## [6.1 2.6 5.6 1.4] ## [7.7 3. 6.1 2.3] ## [6.3 3.4 5.6 2.4] ## [6.4 3.1 5.5 1.8] ## [6. 3. 4.8 1.8] ## [6.9 3.1 5.4 2.1] ## [6.7 3.1 5.6 2.4] ## [6.9 3.1 5.1 2.3] ## [5.8 2.7 5.1 1.9] ## [6.8 3.2 5.9 2.3] ## [6.7 3.3 5.7 2.5] ## [6.7 3. 5.2 2.3] ## [6.3 2.5 5. 1.9] ## [6.5 3. 5.2 2. ] ## [6.2 3.4 5.4 2.3] ## [5.9 3. 5.1 1.8]] dataの各列は1種類の特徴量に対応するが、具体的になんという特徴量なのかはfeature_namesに入っている。 print(iris_dataset.feature_names) ## [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] NumPy配列の扱い方を簡単に。配列の形状(次元)はshapeで取得できる。 print(iris_dataset.data.shape) ## (150, 4) 普通の配列みたいにスライスできる。 print(iris_dataset.data[:5]) ## [[5.1 3.5 1.4 0.2] ## [4.9 3. 1.4 0.2] ## [4.7 3.2 1.3 0.2] ## [4.6 3.1 1.5 0.2] ## [5. 3.6 1.4 0.2]] 1.6.2 成功度合いの測定: 訓練データとテストデータ 未知の測定値からアヤメの品種を予測するみたいなことやりたい。 未知の入力に対する予測能力 is 汎化能力。 こいつを最大化するのが目的。 訓練に使ったデータはモデルの評価に使えない。 そのデータに対してモデルを最適化しているので、良い性能が出るのは当然。 一部だけ訓練に使って残りをテスト用に取っておけばよいのでは？ → ホールドアウト法 訓練に使うデータセット: 訓練データ or 訓練セット。 テストに使うデータセット: テストデータ or テストセット。 scikit-learnでホールドアウト法をやるならmodel_selection.train_test_split。 scikit-learn的な慣習 (入力)データはXで表す データラベル(=入力に対応する出力)はyで表す 入力は行列で出力はベクトルなので、\\(f(\\textbf{X}) = \\textbf{y}\\)ということらしい。 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( iris_dataset.data, iris_dataset.target, random_state = 0 ) 引数random_stateは乱数種を固定する。再現性確保のため。 デフォルトで訓練:テスト=3:1に分解する。 print(X_train.shape) ## (112, 4) print(y_train.shape) ## (112,) print(X_test.shape) ## (38, 4) print(y_test.shape) ## (38,) 1.6.3 最初にすべきこと: データを良く観察する まずは散布図を作れ 多変量ならペアプロット(散布図行列)を作れ pandasにペアプロット作成関数があるので、pandas.DataFrameに変換して作業するとよい。 ## DataFrameへの変換 import pandas as pd iris_dataframe = pd.DataFrame(X_train, columns = iris_dataset.feature_names) 注: テキストで指定しているオプションの大半は外観調整のためのものなので、なくてもいい。 import matplotlib.pyplot as plt import mglearn ## プロット # pandas.scatter_matrixはdeprecated pd.plotting.scatter_matrix( iris_dataframe, # データの指定 c = y_train, # データポイントの色を出力=品種に対応付ける figsize = (15, 15),# 画像出力サイズの指定(なくてもいい) marker = &#39;o&#39;, # ポイントマーカーの指定(なくてもいい) hist_kwds = {&#39;bins&#39;: 20}, # ヒストグラムを作る関数に渡す引数の指定(とりあえずなくてもいい) s = 60, # データポイントのサイズ？(なくてもいい) alpha = .8, # 透過度調整(なくてもいい) cmap = mglearn.cm3 # 配色設定(カラーマップ、なくてもいい) ) ## 表示 plt.show() 1.6.4 最初のモデル: k-最近傍法 距離的に近いやつは仲間でいいんじゃね？に基づくアルゴリズム。 scikit-learnのアルゴリズムを使うためには、アルゴリズムに対応するオブジェクトのインスタンスを生成する必要がある。 from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors = 1) # インスタンス生成時にパラメータを指定できるものもある モデルに訓練セットを適合させるためには、fitメソッドを呼び出して訓練セットを渡すだけで良い。 knn.fit(X_train, y_train) 1.6.5 予測を行う 予測はpredictメソッドで行う。適当にデータを生成して予測してみよう。予測結果は符号化された値になるが、iris_dataset.target_namesを使うと実際のラベル名が分かる。 import numpy as np X_new = np.array([[5, 2.9, 1, 0.2]]) print(knn.predict(X_new)) ## [0] print(iris_dataset.target_names[knn.predict(X_new)]) ## [&#39;setosa&#39;] 適当な数字から予測したから正解なのかどうか分からない！ そのためのテストデータ。 1.6.6 モデルの評価 精度 (accuracy): テストデータのラベルを正しく判別できた割合。 テストデータを使って予測を行い、正解と同じラベルがどれだけあるか、をカウントする。 # 予測 y_pred = knn.predict(X_test) # 比較 print(y_pred == y_test) ## [ True True True True True True True True True True True True ## True True True True True True True True True True True True ## True True True True True True True True True True True True ## True False] pythonは数値計算の際Trueは1、Falseは0として扱うので、y_pred == y_testの平均値がそのまま精度になる。 print(np.mean(y_pred == y_test)) ## 0.9736842105263158 予測から精度計算まで一発でやってくれるメソッドとしてscoreもある。 print(knn.score(X_test, y_test)) ## 0.9736842105263158 "],
["section-2.html", "2 教師あり学習 2.1 クラス分類と回帰 2.2 汎化、過剰適合、適合不足 2.3 教師あり機械学習アルゴリズム", " 2 教師あり学習 備えます。 import numpy as np import scipy as sp import pandas as pd import matplotlib.pyplot as plt import matplotlib matplotlib.rc(&#39;font&#39;, family=&#39;IPAexGothic&#39;) # 日本語プロット設定 import mglearn 2.1 クラス分類と回帰 教師あり学習はさらに2つに分けられる。 クラス分類: クラスラベルを予測する問題。 2クラス分類 (binary classification): Yes/Noみたいな2択。 片方を陽性 (positive)、もう片方を陰性 (negative)とする場合がしばしばある。 他クラス分類 (multiclass classification): もっと選択肢多いやつ。 回帰: 連続値を予測する問題。 2つを区別するのは出力が連続かどうか。入力はどちらの問題でも連続の場合も離散的な場合もある。 2.2 汎化、過剰適合、適合不足 汎化能力: 未知のデータ(訓練に使ってないデータ)に対する正しい値を予測する能力。 過剰適合: 訓練データはめっちゃ正確に予測できるけど新しいデータはてんでダメという状態。 適合不足: 訓練データすらちゃんと予測できてないという状態。 一般的にはモデルを複雑にするほど訓練データに適合していく。適合不足でなく、過剰適合にならない適度なモデルの複雑さの時に汎化能力が最大になる。そこを目指そう。 2.2.1 モデルの複雑さとデータセットの大きさ モデルが複雑でも、データセットが大きければ過剰適合を避けられる。 適度な複雑さのモデルと十分に大きなデータセットを使うことが成功のポイント。 2.3 教師あり機械学習アルゴリズム 2.3.1 サンプルデータセット 人工的な単純なデータセットと、実世界の割と複雑なデータセットを使う。 2.3.1.1 人工的な単純なデータセット 単純なデータセットはmglearnで生成する。 forge: mglearn.datasets.make_forge()で生成する2クラス分類向けデータ。 2つの特徴量と1つの2値目的変数をもつ。 X, y = mglearn.datasets.make_forge() mglearn.discrete_scatter(X[:, 0], X[:, 1], y) plt.legend([&quot;Class 0&quot;, &quot;Class 1&quot;], loc = 4) # 凡例 plt.xlabel(&quot;第1特徴量&quot;) plt.ylabel(&quot;第2特徴量&quot;) plt.show() plt.close() wave: mglearn.datasets.make_waveで生成する回帰向けデータ。 1つの特徴量と1つの目的変数を持つ。 X, y = mglearn.datasets.make_wave(n_samples = 40) plt.plot(X, y, &#39;o&#39;) plt.xlabel(&quot;特徴量&quot;) plt.xlabel(&quot;目的変数&quot;) plt.show() plt.close() 2.3.1.2 実データ 実データはscikit-learnに入ってるものを使う。第1章でも説明したBunchクラスになっている。 cancer: ウィスコンシン乳癌データセット 目的変数は良性(benign)と悪性(malignant)の2値。 特徴量は30。 データポイントは569点。 from sklearn.datasets import load_breast_cancer cancer = load_breast_cancer() print(cancer.keys()) ## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;]) print(cancer.data.shape) ## (569, 30) print(cancer.target_names) ## [&#39;malignant&#39; &#39;benign&#39;] print(np.bincount(cancer.target)) ## [212 357] boston_housing: 1970年代のボストン近郊の住宅価格。 住宅価格の中央値が目的変数。 特徴量は13。 データポイントは506点。 from sklearn.datasets import load_boston boston = load_boston() print(boston.data.shape) ## (506, 13) print(boston.feature_names) ## [&#39;CRIM&#39; &#39;ZN&#39; &#39;INDUS&#39; &#39;CHAS&#39; &#39;NOX&#39; &#39;RM&#39; &#39;AGE&#39; &#39;DIS&#39; &#39;RAD&#39; &#39;TAX&#39; &#39;PTRATIO&#39; ## &#39;B&#39; &#39;LSTAT&#39;] 特徴量同士の積を求めたりして、新しい特徴量を導出することを特徴量エンジニアリングと呼ぶ。 boston_housingに対し、重複ありで2つの特徴量の積を求め、データセットの拡張を試みる。 作業が面倒なので既に拡張したものがmglearn.datasets.load_extended_boston()で読み込めます。 X, y = mglearn.datasets.load_extended_boston() print(X.shape) ## (506, 104) 2.3.2 \\(k\\)-最近傍法 備えよう。 2.3.3 線形モデル 2.3.3.1 線形モデルによる回帰 線形モデルによる予測式は… \\[\\hat{y} = w[0]\\times x[0] + w[1]\\times x[1] + ... + w[p]\\times x[p] + b\\] \\(\\hat{y}\\)は予測値で、\\(w\\)と\\(b\\)はモデルのパラメータ。\\(x\\)はある一つのデータポイントの特徴量。 予測値は、データポイントを適当に重み付けしたもの、と見ることもできる。 waveに線形回帰を適用してプロットしてみよう。 mglearn.plots.plot_linear_regression_wave() ## w[0]: 0.393906 b: -0.031804 ## ## /Users/rito/myenv/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to &#39;gelss&#39; driver. ## warnings.warn(mesg, RuntimeWarning) plt.show() 線形モデルを利用した回帰にはいろいろなアルゴリズムがあって、それぞれ以下の点で異なっている。 どのようにパラメータ\\(w\\)と\\(b\\)を学習するか。 モデルの複雑さをどのように制御するのか。 2.3.3.2 線形回帰(通常最小二乗法) 予測値と真値の平均二乗誤差 (mean squared error) を最小にするようなパラメータを求める。 線形回帰には複雑さを制御するパラメータがない。できない。 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression X, y = mglearn.datasets.make_wave(n_samples = 60) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42) lr = LinearRegression().fit(X_train, y_train) \\(w\\)は係数 (coefficient)と呼ばれ、coef_に格納される。 \\(b\\)は切片 (intercept)と呼ばれ、intercept_に格納される。 print(lr.coef_) ## [0.39390555] print(lr.intercept_) ## -0.03180434302675976 訓練データから得られた属性にアンダースコアを付けるのはscikit-learnの慣習である。 coef_は特徴量1つに対して1つの値をもつNumPy配列となる。 線形回帰の性能は決定係数\\(R^2\\)として求められる。 print(lr.score(X_train, y_train)) ## 0.6700890315075756 print(lr.score(X_test, y_test)) ## 0.65933685968637 ここで訓練セットとテストセットの\\(R^2\\)があんまり違わないのは（予測性能はともかく）過剰適合していないことを示している。通常、特徴量が多いほど過剰適合のリスクが高まる。拡張したboston_housingで確認してみよう。 X, y = mglearn.datasets.load_extended_boston() X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0) lr = LinearRegression().fit(X_train, y_train) \\(R^2\\)を訓練セットとテストセットで比較してみよう。 print(lr.score(X_train, y_train)) ## 0.9523526436864234 print(lr.score(X_test, y_test)) ## 0.6057754892935417 両者に乖離が見られるのは、過剰適合している可能性がある。 モデルの複雑さを制御できれば良いのだが、線形回帰にはそのためのパラメータがない。パラメータを導入する方法としてリッジ回帰がある。 2.3.3.3 リッジ回帰 "]
]
