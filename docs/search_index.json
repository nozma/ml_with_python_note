[
["index.html", "Pythonで始める機械学習の学習 まえおき", " Pythonで始める機械学習の学習 R. Ito 2018-03-22 まえおき やるぞい(๑•̀ㅂ•́)و✧ "],
["方針とか.html", "方針とか", " 方針とか Pythonではじめる機械学習 ―scikit-learnで学ぶ特徴量エンジニアリングと機械学習の基礎の備忘録です。 テキストのコードは見た目重視で冗長なので、どんどん省略していきます 。 そんな風に思っていた時期もありました。 mglearnによって過度にコードが省略されている部分や、それに起因してR Markdown上では動かない部分もあるため、適宜amueller/introduction_to_ml_with_python: Notebooks and code for the book “Introduction to Machine Learning with Python”等を参照して不足部分を補っています。 がんばりすぎない。 "],
["実行環境とか.html", "実行環境とか", " 実行環境とか サンプルコード実行用Jupyter notebook 書籍のサンプルコードが動く&amp;日本語が使えるように調整したJupyter notebookのDockerイメージをnozma/ml-python-notebookで公開しています(nozma/ml-python-notebook - Docker Hub)。 docker run -p 8888:8888 nozma/ml-python-notebook などとやればJupyter notebookが起動すると思います。 この文章を執筆しているRStudio この文章自体はR Studioとbookdownパッケージを用いて執筆しており、こちらの環境はnozma/ml-python-notebook-rで公開しています(nozma/ml-python-notebook-r - Docker Hub)。 Dockerがセットアップされている環境で docker run -p 8787:8787 nozma/ml-python-notebook-r とし、 http://localhost:8787 にアクセスするとR Studio Serverの起動画面が表示されます。ユーザー名、パスワードはいずれもrstudioです。 R Markdown中でPythonを使用するためには、Pythonのengine.pathを明示的にpython3と指定してやる必要があります。このテキストでは、.Rmdファイルに、以下のコードチャンクを設置してこの設定を行っています。 ```{r setup, echo=FALSE} knitr::opts_chunk$set( engine.path = list(python = &quot;/usr/bin/python3&quot;), collapse = TRUE, comment = &quot; ##&quot; ) ``` また、日本語フォントとしてIPAexGothicをインストールしてあります。matplotlibで使用する場合は、以下のコードチャンクをファイル冒頭などに記述してください。 ```python matplotlib.rc(&#39;font&#39;, family=&#39;IPAexGothic&#39;) # 日本語プロット設定 ``` R sessionInfo utils::sessionInfo() ## R version 3.4.3 (2017-11-30) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 9 (stretch) ## ## Matrix products: default ## BLAS: /usr/lib/openblas-base/libblas.so.3 ## LAPACK: /usr/lib/libopenblasp-r0.2.19.so ## ## locale: ## [1] LC_CTYPE=ja_JP.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=ja_JP.UTF-8 LC_COLLATE=ja_JP.UTF-8 ## [5] LC_MONETARY=ja_JP.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=ja_JP.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=ja_JP.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.4.3 backports_1.1.2 bookdown_0.7 magrittr_1.5 ## [5] rprojroot_1.3-2 tools_3.4.3 htmltools_0.3.6 yaml_2.1.18 ## [9] Rcpp_0.12.16 stringi_1.1.7 rmarkdown_1.9 knitr_1.20 ## [13] methods_3.4.3 xfun_0.1 stringr_1.3.0 digest_0.6.15 ## [17] evaluate_0.10.1 Python環境 import sys print(sys.version) ## 3.5.3 (default, Jan 19 2017, 14:11:04) ## [GCC 6.3.0 20170118] from pip.utils import get_installed_distributions [print(d) for d in get_installed_distributions()] ## wcwidth 0.1.7 ## traitlets 4.3.2 ## simplegeneric 0.8.1 ## scipy 1.0.0 ## scikit-learn 0.19.1 ## pytz 2018.3 ## python-dateutil 2.7.0 ## pyparsing 2.2.0 ## Pygments 2.2.0 ## ptyprocess 0.5.2 ## prompt-toolkit 1.0.15 ## Pillow 5.0.0 ## pickleshare 0.7.4 ## pexpect 4.4.0 ## parso 0.1.1 ## pandas 0.22.0 ## numpy 1.14.2 ## mglearn 0.1.6 ## matplotlib 2.2.2 ## kiwisolver 1.0.1 ## jedi 0.11.1 ## ipython 6.2.1 ## ipython-genutils 0.2.0 ## graphviz 0.8.2 ## decorator 4.2.1 ## cycler 0.10.0 "],
["1-はじめに.html", "1 はじめに", " 1 はじめに 真の前置き的なやつ。 "],
["1-1-なぜ機械学習なのか.html", "1.1 なぜ機械学習なのか", " 1.1 なぜ機械学習なのか if〜then〜のエキスパートシステムには限界がある。 ロジックがドメイン固有のものになり、タスクが変化したら作り直さないといけない。 人間のエキスパートの思考に対する深い理解が必要。 顔認識とかはちょっと無理。 1.1.1 機械学習で解決可能な問題 世の中には教師のある学習と教師のない学習がある。 教師あり学習 入力と出力のペアを与えてアルゴリズムを学習させる。 未知の入力に対する出力を予測する。 データセットを集めることが大変な場合もある。 教師なし学習 出力のないデータセットでアルゴリズムを学習させる。 トピック解析やクラスタリング、異常検知など。 サンプルと特徴量。 サンプル 個々のエンティティまたは量。 特徴量 エンティティの持つ特性を表現する列。 情報量のないデータからは学習できないことを覚えておくこと。 1.1.2 タスクを知り、データを知る よくデータを理解すること そのデータでそもそも問題を解決できるのか？ どんな機械学習の問題に置き換えるべきか？ データの数は十分か？ アルゴリズムは問題の一部にすぎない。全体を心に留めておくこと。 "],
["1-2-なぜpythonなのか.html", "1.2 なぜPythonなのか？", " 1.2 なぜPythonなのか？ つよい 汎用言語の強さとドメイン特価スクリプト言語の強さがある。 Jupyter Notebookが使える データ解析はインタラクティブな過程。 "],
["1-3-scikit-learn.html", "1.3 scikit-learn", " 1.3 scikit-learn Pythonで機械学習といったらこれ オープンソース ユーザーガイドも読んでおこうな http://scikit-learn.org/stable/user_guide.html 1.3.1 インストール $ pip install numpy scipy matplotlib ipython scikit-learn pandas pillow この他、章によってはgraphvizが必要な部分もある。 "],
["1-4-必要なライブラリとツール.html", "1.4 必要なライブラリとツール", " 1.4 必要なライブラリとツール Jupyter Notebook ブラウザ上でMarkdownとコード混ぜて書けるやつ。 NumPy 配列計算ﾒｯﾁｬﾊﾔｲやつ SciPy 科学技術計算できるやつ matplotlib プロットするやつ pandas Rのデータフレーム的な感じでデータ扱えるやつ mglearn 「こちらに予め調理したものがあります」的に教科書の例試せるやつ "],
["1-5-python-2-vs-python-3.html", "1.5 Python 2 vs. Python 3", " 1.5 Python 2 vs. Python 3 3でやれ。 "],
["1-6-最初のアプリケーション-アイリスのクラス分類.html", "1.6 最初のアプリケーション: アイリスのクラス分類", " 1.6 最初のアプリケーション: アイリスのクラス分類 みんな大好きiris。 以下の記事が詳しい。 irisの正体 (R Advent Calendar 2012 6日目) - どんな鳥も 1.6.1 データを読む scikit-learnのdatasetsモジュール いろんなサンプルデータセットが入ってる。 sklearn.datasets.load_iris()でirisが返ってくるので適当に受けよう。 from sklearn.datasets import load_iris iris_dataset = load_iris() load_iris()はBunchクラスのオブジェクトを返す。これはディクショナリみたいに扱える。値にはiris_dataset['data']以外にiris_dataset.dataみたいにアクセスしてもいい。 print(iris_dataset.keys()) ## dict_keys([&#39;target_names&#39;, &#39;data&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;target&#39;]) キーDESCRの中にデータセットの説明が入っている。 print(iris_dataset.DESCR) ## Iris Plants Database ## ==================== ## ## Notes ## ----- ## Data Set Characteristics: ## :Number of Instances: 150 (50 in each of three classes) ## :Number of Attributes: 4 numeric, predictive attributes and the class ## :Attribute Information: ## - sepal length in cm ## - sepal width in cm ## - petal length in cm ## - petal width in cm ## - class: ## - Iris-Setosa ## - Iris-Versicolour ## - Iris-Virginica ## :Summary Statistics: ## ## ============== ==== ==== ======= ===== ==================== ## Min Max Mean SD Class Correlation ## ============== ==== ==== ======= ===== ==================== ## sepal length: 4.3 7.9 5.84 0.83 0.7826 ## sepal width: 2.0 4.4 3.05 0.43 -0.4194 ## petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) ## petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ## ============== ==== ==== ======= ===== ==================== ## ## :Missing Attribute Values: None ## :Class Distribution: 33.3% for each of 3 classes. ## :Creator: R.A. Fisher ## :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) ## :Date: July, 1988 ## ## This is a copy of UCI ML iris datasets. ## http://archive.ics.uci.edu/ml/datasets/Iris ## ## The famous Iris database, first used by Sir R.A Fisher ## ## This is perhaps the best known database to be found in the ## pattern recognition literature. Fisher&#39;s paper is a classic in the field and ## is referenced frequently to this day. (See Duda &amp; Hart, for example.) The ## data set contains 3 classes of 50 instances each, where each class refers to a ## type of iris plant. One class is linearly separable from the other 2; the ## latter are NOT linearly separable from each other. ## ## References ## ---------- ## - Fisher,R.A. &quot;The use of multiple measurements in taxonomic problems&quot; ## Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to ## Mathematical Statistics&quot; (John Wiley, NY, 1950). ## - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis. ## (Q327.D83) John Wiley &amp; Sons. ISBN 0-471-22361-1. See page 218. ## - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System ## Structure and Classification Rule for Recognition in Partially Exposed ## Environments&quot;. IEEE Transactions on Pattern Analysis and Machine ## Intelligence, Vol. PAMI-2, No. 1, 67-71. ## - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;. IEEE Transactions ## on Information Theory, May 1972, 431-433. ## - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al&quot;s AUTOCLASS II ## conceptual clustering system finds 3 classes in the data. ## - Many, many more ... targetは符号化されていて、対応する名前はtarget_namesに入っている。 print(iris_dataset.target) ## [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## 2 2] print(iris_dataset.target_names) ## [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] 要するにtargetでtarget_namesを参照すると実際の目的変数の構造が見える。 print(iris_dataset.target_names[iris_dataset.target]) ## [&#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; &#39;setosa&#39; ## &#39;setosa&#39; &#39;setosa&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; &#39;versicolor&#39; ## &#39;versicolor&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39; ## &#39;virginica&#39; &#39;virginica&#39; &#39;virginica&#39;] 特徴量はデータ件数×特徴量数のNumPy配列としてdataに入っている。 print(iris_dataset.data) ## [[5.1 3.5 1.4 0.2] ## [4.9 3. 1.4 0.2] ## [4.7 3.2 1.3 0.2] ## [4.6 3.1 1.5 0.2] ## [5. 3.6 1.4 0.2] ## [5.4 3.9 1.7 0.4] ## [4.6 3.4 1.4 0.3] ## [5. 3.4 1.5 0.2] ## [4.4 2.9 1.4 0.2] ## [4.9 3.1 1.5 0.1] ## [5.4 3.7 1.5 0.2] ## [4.8 3.4 1.6 0.2] ## [4.8 3. 1.4 0.1] ## [4.3 3. 1.1 0.1] ## [5.8 4. 1.2 0.2] ## [5.7 4.4 1.5 0.4] ## [5.4 3.9 1.3 0.4] ## [5.1 3.5 1.4 0.3] ## [5.7 3.8 1.7 0.3] ## [5.1 3.8 1.5 0.3] ## [5.4 3.4 1.7 0.2] ## [5.1 3.7 1.5 0.4] ## [4.6 3.6 1. 0.2] ## [5.1 3.3 1.7 0.5] ## [4.8 3.4 1.9 0.2] ## [5. 3. 1.6 0.2] ## [5. 3.4 1.6 0.4] ## [5.2 3.5 1.5 0.2] ## [5.2 3.4 1.4 0.2] ## [4.7 3.2 1.6 0.2] ## [4.8 3.1 1.6 0.2] ## [5.4 3.4 1.5 0.4] ## [5.2 4.1 1.5 0.1] ## [5.5 4.2 1.4 0.2] ## [4.9 3.1 1.5 0.1] ## [5. 3.2 1.2 0.2] ## [5.5 3.5 1.3 0.2] ## [4.9 3.1 1.5 0.1] ## [4.4 3. 1.3 0.2] ## [5.1 3.4 1.5 0.2] ## [5. 3.5 1.3 0.3] ## [4.5 2.3 1.3 0.3] ## [4.4 3.2 1.3 0.2] ## [5. 3.5 1.6 0.6] ## [5.1 3.8 1.9 0.4] ## [4.8 3. 1.4 0.3] ## [5.1 3.8 1.6 0.2] ## [4.6 3.2 1.4 0.2] ## [5.3 3.7 1.5 0.2] ## [5. 3.3 1.4 0.2] ## [7. 3.2 4.7 1.4] ## [6.4 3.2 4.5 1.5] ## [6.9 3.1 4.9 1.5] ## [5.5 2.3 4. 1.3] ## [6.5 2.8 4.6 1.5] ## [5.7 2.8 4.5 1.3] ## [6.3 3.3 4.7 1.6] ## [4.9 2.4 3.3 1. ] ## [6.6 2.9 4.6 1.3] ## [5.2 2.7 3.9 1.4] ## [5. 2. 3.5 1. ] ## [5.9 3. 4.2 1.5] ## [6. 2.2 4. 1. ] ## [6.1 2.9 4.7 1.4] ## [5.6 2.9 3.6 1.3] ## [6.7 3.1 4.4 1.4] ## [5.6 3. 4.5 1.5] ## [5.8 2.7 4.1 1. ] ## [6.2 2.2 4.5 1.5] ## [5.6 2.5 3.9 1.1] ## [5.9 3.2 4.8 1.8] ## [6.1 2.8 4. 1.3] ## [6.3 2.5 4.9 1.5] ## [6.1 2.8 4.7 1.2] ## [6.4 2.9 4.3 1.3] ## [6.6 3. 4.4 1.4] ## [6.8 2.8 4.8 1.4] ## [6.7 3. 5. 1.7] ## [6. 2.9 4.5 1.5] ## [5.7 2.6 3.5 1. ] ## [5.5 2.4 3.8 1.1] ## [5.5 2.4 3.7 1. ] ## [5.8 2.7 3.9 1.2] ## [6. 2.7 5.1 1.6] ## [5.4 3. 4.5 1.5] ## [6. 3.4 4.5 1.6] ## [6.7 3.1 4.7 1.5] ## [6.3 2.3 4.4 1.3] ## [5.6 3. 4.1 1.3] ## [5.5 2.5 4. 1.3] ## [5.5 2.6 4.4 1.2] ## [6.1 3. 4.6 1.4] ## [5.8 2.6 4. 1.2] ## [5. 2.3 3.3 1. ] ## [5.6 2.7 4.2 1.3] ## [5.7 3. 4.2 1.2] ## [5.7 2.9 4.2 1.3] ## [6.2 2.9 4.3 1.3] ## [5.1 2.5 3. 1.1] ## [5.7 2.8 4.1 1.3] ## [6.3 3.3 6. 2.5] ## [5.8 2.7 5.1 1.9] ## [7.1 3. 5.9 2.1] ## [6.3 2.9 5.6 1.8] ## [6.5 3. 5.8 2.2] ## [7.6 3. 6.6 2.1] ## [4.9 2.5 4.5 1.7] ## [7.3 2.9 6.3 1.8] ## [6.7 2.5 5.8 1.8] ## [7.2 3.6 6.1 2.5] ## [6.5 3.2 5.1 2. ] ## [6.4 2.7 5.3 1.9] ## [6.8 3. 5.5 2.1] ## [5.7 2.5 5. 2. ] ## [5.8 2.8 5.1 2.4] ## [6.4 3.2 5.3 2.3] ## [6.5 3. 5.5 1.8] ## [7.7 3.8 6.7 2.2] ## [7.7 2.6 6.9 2.3] ## [6. 2.2 5. 1.5] ## [6.9 3.2 5.7 2.3] ## [5.6 2.8 4.9 2. ] ## [7.7 2.8 6.7 2. ] ## [6.3 2.7 4.9 1.8] ## [6.7 3.3 5.7 2.1] ## [7.2 3.2 6. 1.8] ## [6.2 2.8 4.8 1.8] ## [6.1 3. 4.9 1.8] ## [6.4 2.8 5.6 2.1] ## [7.2 3. 5.8 1.6] ## [7.4 2.8 6.1 1.9] ## [7.9 3.8 6.4 2. ] ## [6.4 2.8 5.6 2.2] ## [6.3 2.8 5.1 1.5] ## [6.1 2.6 5.6 1.4] ## [7.7 3. 6.1 2.3] ## [6.3 3.4 5.6 2.4] ## [6.4 3.1 5.5 1.8] ## [6. 3. 4.8 1.8] ## [6.9 3.1 5.4 2.1] ## [6.7 3.1 5.6 2.4] ## [6.9 3.1 5.1 2.3] ## [5.8 2.7 5.1 1.9] ## [6.8 3.2 5.9 2.3] ## [6.7 3.3 5.7 2.5] ## [6.7 3. 5.2 2.3] ## [6.3 2.5 5. 1.9] ## [6.5 3. 5.2 2. ] ## [6.2 3.4 5.4 2.3] ## [5.9 3. 5.1 1.8]] dataの各列は1種類の特徴量に対応するが、具体的になんという特徴量なのかはfeature_namesに入っている。 print(iris_dataset.feature_names) ## [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] NumPy配列の扱い方を簡単に。配列の形状(次元)はshapeで取得できる。 print(iris_dataset.data.shape) ## (150, 4) 普通の配列みたいにスライスできる。 print(iris_dataset.data[:5]) ## [[5.1 3.5 1.4 0.2] ## [4.9 3. 1.4 0.2] ## [4.7 3.2 1.3 0.2] ## [4.6 3.1 1.5 0.2] ## [5. 3.6 1.4 0.2]] 1.6.2 成功度合いの測定: 訓練データとテストデータ 未知の測定値からアヤメの品種を予測するみたいなことやりたい。 未知の入力に対する予測能力 is 汎化能力。 こいつを最大化するのが目的。 訓練に使ったデータはモデルの評価に使えない。 そのデータに対してモデルを最適化しているので、良い性能が出るのは当然。 一部だけ訓練に使って残りをテスト用に取っておけばよいのでは？ → ホールドアウト法 訓練に使うデータセット: 訓練データ or 訓練セット。 テストに使うデータセット: テストデータ or テストセット。 scikit-learnでホールドアウト法をやるならmodel_selection.train_test_split。 scikit-learn的な慣習 (入力)データはXで表す データラベル(=入力に対応する出力)はyで表す 入力は行列で出力はベクトルなので、\\(f(\\textbf{X}) = \\textbf{y}\\)ということらしい。 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( iris_dataset.data, iris_dataset.target, random_state = 0 ) 引数random_stateは乱数種を固定する。再現性確保のため。 デフォルトで訓練:テスト=3:1に分解する。 print(X_train.shape) ## (112, 4) print(y_train.shape) ## (112,) print(X_test.shape) ## (38, 4) print(y_test.shape) ## (38,) 1.6.3 最初にすべきこと: データを良く観察する まずは散布図を作れ 多変量ならペアプロット(散布図行列)を作れ pandasにペアプロット作成関数があるので、pandas.DataFrameに変換して作業するとよい。 ## DataFrameへの変換 import pandas as pd iris_dataframe = pd.DataFrame(X_train, columns = iris_dataset.feature_names) 注: テキストで指定しているオプションの大半は外観調整のためのものなので、なくてもいい。 import matplotlib.pyplot as plt import mglearn ## プロット # pandas.scatter_matrixはdeprecated pd.plotting.scatter_matrix( iris_dataframe, # データの指定 c = y_train, # データポイントの色を出力=品種に対応付ける figsize = (15, 15),# 画像出力サイズの指定(なくてもいい) marker = &#39;o&#39;, # ポイントマーカーの指定(なくてもいい) hist_kwds = {&#39;bins&#39;: 20}, # ヒストグラムを作る関数に渡す引数の指定(とりあえずなくてもいい) s = 60, # データポイントのサイズ？(なくてもいい) alpha = .8, # 透過度調整(なくてもいい) cmap = mglearn.cm3 # 配色設定(カラーマップ、なくてもいい) ) ## 表示 plt.show() 1.6.4 最初のモデル: k-最近傍法 距離的に近いやつは仲間でいいんじゃね？に基づくアルゴリズム。 scikit-learnのアルゴリズムを使うためには、アルゴリズムに対応するオブジェクトのインスタンスを生成する必要がある。 from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors = 1) # インスタンス生成時にパラメータを指定できるものもある モデルに訓練セットを適合させるためには、fitメソッドを呼び出して訓練セットを渡すだけで良い。 knn.fit(X_train, y_train) 1.6.5 予測を行う 予測はpredictメソッドで行う。適当にデータを生成して予測してみよう。予測結果は符号化された値になるが、iris_dataset.target_namesを使うと実際のラベル名が分かる。 import numpy as np X_new = np.array([[5, 2.9, 1, 0.2]]) print(knn.predict(X_new)) ## [0] print(iris_dataset.target_names[knn.predict(X_new)]) ## [&#39;setosa&#39;] 適当な数字から予測したから正解なのかどうか分からない！ そのためのテストデータ。 1.6.6 モデルの評価 精度 (accuracy): テストデータのラベルを正しく判別できた割合。 テストデータを使って予測を行い、正解と同じラベルがどれだけあるか、をカウントする。 # 予測 y_pred = knn.predict(X_test) # 比較 print(y_pred == y_test) ## [ True True True True True True True True True True True True ## True True True True True True True True True True True True ## True True True True True True True True True True True True ## True False] pythonは数値計算の際Trueは1、Falseは0として扱うので、y_pred == y_testの平均値がそのまま精度になる。 print(np.mean(y_pred == y_test)) ## 0.9736842105263158 予測から精度計算まで一発でやってくれるメソッドとしてscoreもある。 print(knn.score(X_test, y_test)) ## 0.9736842105263158 "],
["2-教師あり学習-1.html", "2 教師あり学習 (1)", " 2 教師あり学習 (1) 備えます。 import numpy as np import scipy as sp import pandas as pd import matplotlib.pyplot as plt import matplotlib matplotlib.rc(&#39;font&#39;, family=&#39;IPAexGothic&#39;) # 日本語プロット設定 import mglearn "],
["2-1-クラス分類と回帰.html", "2.1 クラス分類と回帰", " 2.1 クラス分類と回帰 教師あり学習はさらに2つに分けられる。 クラス分類: クラスラベルを予測する問題。 2クラス分類 (binary classification): Yes/Noみたいな2択。 片方を陽性 (positive)、もう片方を陰性 (negative)とする場合がしばしばある。 他クラス分類 (multiclass classification): もっと選択肢多いやつ。 回帰: 連続値を予測する問題。 2つを区別するのは出力が連続かどうか。入力はどちらの問題でも連続の場合も離散的な場合もある。 "],
["2-2-汎化過剰適合適合不足.html", "2.2 汎化、過剰適合、適合不足", " 2.2 汎化、過剰適合、適合不足 汎化能力: 未知のデータ(訓練に使ってないデータ)に対する正しい値を予測する能力。 過剰適合: 訓練データはめっちゃ正確に予測できるけど新しいデータはてんでダメという状態。 適合不足: 訓練データすらちゃんと予測できてないという状態。 一般的にはモデルを複雑にするほど訓練データに適合していく。適合不足でなく、過剰適合にならない適度なモデルの複雑さの時に汎化能力が最大になる。そこを目指そう。 2.2.1 モデルの複雑さとデータセットの大きさ モデルが複雑でも、データセットが大きければ過剰適合を避けられる。 適度な複雑さのモデルと十分に大きなデータセットを使うことが成功のポイント。 "],
["2-3-教師あり機械学習アルゴリズム.html", "2.3 教師あり機械学習アルゴリズム", " 2.3 教師あり機械学習アルゴリズム 2.3.1 サンプルデータセット 人工的な単純なデータセットと、実世界の割と複雑なデータセットを使う。 2.3.1.1 人工的な単純なデータセット 単純なデータセットはmglearnで生成する。 forge: mglearn.datasets.make_forge()で生成する2クラス分類向けデータ。 2つの特徴量と1つの2値目的変数をもつ。 X, y = mglearn.datasets.make_forge() mglearn.discrete_scatter(X[:, 0], X[:, 1], y) plt.legend([&quot;Class 0&quot;, &quot;Class 1&quot;], loc = 4) # 凡例 plt.xlabel(&quot;第1特徴量&quot;) plt.ylabel(&quot;第2特徴量&quot;) wave: mglearn.datasets.make_waveで生成する回帰向けデータ。 1つの特徴量と1つの目的変数を持つ。 X, y = mglearn.datasets.make_wave(n_samples = 40) plt.plot(X, y, &#39;o&#39;) plt.xlabel(&quot;特徴量&quot;) plt.ylabel(&quot;目的変数&quot;) 2.3.1.2 実データ 実データはscikit-learnに入ってるものを使う。第1章でも説明したBunchクラスになっている。 cancer: ウィスコンシン乳癌データセット 目的変数は良性(benign)と悪性(malignant)の2値。 特徴量は30。 データポイントは569点。 from sklearn.datasets import load_breast_cancer cancer = load_breast_cancer() print(cancer.keys()) ## dict_keys([&#39;target&#39;, &#39;target_names&#39;, &#39;data&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;]) print(cancer.data.shape) ## (569, 30) print(cancer.target_names) ## [&#39;malignant&#39; &#39;benign&#39;] print(np.bincount(cancer.target)) ## [212 357] boston_housing: 1970年代のボストン近郊の住宅価格。 住宅価格の中央値が目的変数。 特徴量は13。 データポイントは506点。 from sklearn.datasets import load_boston boston = load_boston() print(boston.data.shape) ## (506, 13) print(boston.feature_names) ## [&#39;CRIM&#39; &#39;ZN&#39; &#39;INDUS&#39; &#39;CHAS&#39; &#39;NOX&#39; &#39;RM&#39; &#39;AGE&#39; &#39;DIS&#39; &#39;RAD&#39; &#39;TAX&#39; &#39;PTRATIO&#39; ## &#39;B&#39; &#39;LSTAT&#39;] 特徴量同士の積を求めたりして、新しい特徴量を導出することを特徴量エンジニアリングと呼ぶ。 boston_housingに対し、重複ありで2つの特徴量の積を求め、データセットの拡張を試みる。 作業が面倒なので既に拡張したものがmglearn.datasets.load_extended_boston()で読み込めます。 X, y = mglearn.datasets.load_extended_boston() print(X.shape) ## (506, 104) "],
["2-4-アルゴリズム1-k-最近傍法.html", "2.4 アルゴリズム1 \\(k\\)-最近傍法", " 2.4 アルゴリズム1 \\(k\\)-最近傍法 a.k.a. \\(k\\)-NN 近いやつは大体おんなじ。 2.4.1 \\(k\\)-最近傍法によるクラス分類 \\(k\\)は参考にする近傍点の個数。 1-NNの例。 mglearn.plots.plot_knn_classification(n_neighbors=1) 3-NNの例 近傍点が複数のときは多数決で決める。 mglearn.plots.plot_knn_classification(n_neighbors=3) scikit-learnでやる。 from sklearn.model_selection import train_test_split X, y = mglearn.datasets.make_forge() X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) from sklearn.neighbors import KNeighborsClassifier clf = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train) print(clf.score(X_test, y_test)) ## 0.8571428571428571 2.4.2 KNeighborsClassifierの解析 特徴量が2つしかなければ、散布図が描ける。 散布図上のあらゆる箇所についてもしその場所に点があったらと考えて判別ができる。 つまり、特徴量がつくる平面を分類クラスで塗り分けることができる。 境界線を決定境界と呼ぶ。 fig, axes = plt.subplots(1, 3, figsize = (10, 3)) for n, ax in zip([1, 3, 9], axes): clf = KNeighborsClassifier(n_neighbors = n).fit(X, y) mglearn.plots.plot_2d_separator(clf, X, fill=True, eps = 0.5, ax = ax, alpha = .4) mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax = ax) ax.set_title(&quot;{} neighbor(s)&quot;.format(n)) ax.set_xlabel(&quot;特徴量 0&quot;) ax.set_ylabel(&quot;特徴量 1&quot;) axes[0].legend(loc=3) 近傍点が多いほど境界がなめらか = モデルは単純になる。 近傍点1 = 最も複雑なモデル 近傍点数 = データ数 -&gt; ただの多数決 ということは近傍点数の数を増やしていくと、どこかで汎化能力のピークが…？ cancerデータセットで試してみる。 from sklearn.datasets import load_breast_cancer cancer = load_breast_cancer() X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify = cancer.target, random_state = 66 ) training_accuracy = [] test_accuracy = [] n_settings = range(1, 11) for n in n_settings: clf = KNeighborsClassifier(n_neighbors = n).fit(X_train, y_train) training_accuracy.append(clf.score(X_train, y_train)) test_accuracy.append(clf.score(X_test, y_test)) plt.plot(n_settings, training_accuracy, label = &quot;訓練セット精度&quot;) plt.plot(n_settings, test_accuracy, label = &quot;テストセット精度&quot;) plt.ylabel(&quot;精度&quot;) plt.xlabel(&quot;近傍点数&quot;) plt.legend() 2.4.3 \\(k\\)-近傍回帰 kNNは回帰もできる。 1-NNでは近傍点の値が新しい観測値に対応する値だと考える。 mglearn.plots.plot_knn_regression(n_neighbors = 1) 近傍点が複数の時は平均値を使う。 mglearn.plots.plot_knn_regression(n_neighbors = 3) scikit-learnでは、KNeighborsRegressorクラスに実装されてる。 from sklearn.neighbors import KNeighborsRegressor X, y = mglearn.datasets.make_wave(n_samples = 40) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) reg = KNeighborsRegressor(n_neighbors = 3).fit(X_train, y_train) print(reg.score(X_test, y_test)) ## 0.8344172446249604 2.4.4 KNeighborsRegressorの解析 1次元のデータセットに対する予測値は、近傍点数\\(k\\)に対してどのように変化するか？ # プロット先を3つ作る fig, axes = plt.subplots(1, 3, figsize = (15, 4)) # -3〜3までの間にデータポイントを1000点作る line = np.linspace(-3, 3, 1000).reshape(-1, 1) for n_neighbors, ax in zip([1, 3, 9], axes): reg = KNeighborsRegressor(n_neighbors = n_neighbors) reg.fit(X_train, y_train) ax.plot(line, reg.predict(line)) ax.plot(X_train, y_train, &#39;^&#39;) ax.plot(X_test, y_test, &#39;v&#39;) ax.set_title( &quot;{} 近傍点\\n 訓練スコア: {:.2f} テストスコア{:.2f}&quot;.format( n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test))) ax.set_xlabel(&quot;特徴量&quot;) ax.set_ylabel(&quot;目的変数&quot;) axes[0].legend([&quot;モデルによる予測値&quot;, &quot;訓練データ&quot;, &quot;テストデータ&quot;], loc=&quot;best&quot;) \\(k=1\\)の場合は予測値が全ての訓練データを通るので、モデルが不安定になる。 近傍点を増やしていくと予測は滑らかになるが、その反面訓練データへの適合度が下がる。 2.4.5 利点と欠点とパラメータ 利点 モデルが理解しやすい。 あまり調整しなくても性能が出やすい。 モデル構築は高速 欠点 訓練セットが大きくなると予測が遅くなる。 実際に使う前には前処理を行うことが重要。 疎なデータセット(特徴量の多くが0である)に対しては十分な性能が出にくい。 上記の理由から、kNNは実際に使われることは少ない。 "],
["2-5-アルゴリズム2-線形モデル.html", "2.5 アルゴリズム2 線形モデル", " 2.5 アルゴリズム2 線形モデル 2.5.1 線形モデルによる回帰 線形モデルによる予測式は… \\[\\hat{y} = w[0]\\times x[0] + w[1]\\times x[1] + ... + w[p]\\times x[p] + b\\] \\(\\hat{y}\\)は予測値で、\\(w\\)と\\(b\\)はモデルのパラメータ。\\(x\\)はある一つのデータポイントの特徴量。 予測値は、データポイントを適当に重み付けしたもの、と見ることもできる。 waveに線形回帰を適用してプロットしてみよう。 mglearn.plots.plot_linear_regression_wave() ## w[0]: 0.393906 b: -0.031804 線形モデルを利用した回帰にはいろいろなアルゴリズムがあって、それぞれ以下の点で異なっている。 どのようにパラメータ\\(w\\)と\\(b\\)を学習するか。 モデルの複雑さをどのように制御するのか。 2.5.2 線形回帰(通常最小二乗法) 予測値と真値の平均二乗誤差 (mean squared error) を最小にするようなパラメータを求める。 線形回帰には複雑さを制御するパラメータがない。できない。 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression X, y = mglearn.datasets.make_wave(n_samples = 60) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42) lr = LinearRegression().fit(X_train, y_train) \\(w\\)は係数 (coefficient)と呼ばれ、coef_に格納される。 \\(b\\)は切片 (intercept)と呼ばれ、intercept_に格納される。 print(lr.coef_) ## [0.39390555] print(lr.intercept_) ## -0.03180434302675973 訓練データから得られた属性にアンダースコアを付けるのはscikit-learnの慣習である。 coef_は特徴量1つに対して1つの値をもつNumPy配列となる。 線形回帰の性能は決定係数\\(R^2\\)として求められる。 print(lr.score(X_train, y_train)) ## 0.6700890315075756 print(lr.score(X_test, y_test)) ## 0.6593368596863701 ここで訓練セットとテストセットの\\(R^2\\)があんまり違わないのは（予測性能はともかく）過剰適合していないことを示している。通常、特徴量が多いほど過剰適合のリスクが高まる。拡張したboston_housingで確認してみよう。 X, y = mglearn.datasets.load_extended_boston() X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0) lr = LinearRegression().fit(X_train, y_train) \\(R^2\\)を訓練セットとテストセットで比較してみよう。 print(lr.score(X_train, y_train)) ## 0.9523526436864239 print(lr.score(X_test, y_test)) ## 0.6057754892935757 両者に乖離が見られるのは、過剰適合している可能性がある。 モデルの複雑さを制御できれば良いのだが、線形回帰にはそのためのパラメータがない。パラメータを導入する方法としてリッジ回帰がある。 2.5.3 リッジ回帰 係数が多いからモデルが複雑になる。 係数が0＝その係数を考慮しない。 係数が小さければモデルは単純になるのでは🤔 極端な話係数が全部ゼロなら入力に関わらず一定の値(平均とか)を出力するモデルになる。 係数ベクトルの長さを最小化しよう！→リッジ回帰 from sklearn.linear_model import Ridge ridge = Ridge().fit(X_train, y_train) # データは拡張Boston housingのまま print(ridge.score(X_train, y_train)) ## 0.8860578560395836 print(ridge.score(X_test, y_test)) ## 0.7527139600306947 訓練セットへの予測能力が下がったけどテストセットへの予測能力が上がった！ モデルを単純にすることで汎化能力が上がっている。 リッジ回帰におけるモデルの単純さを制御するパラメータ: \\(\\alpha\\) 大きいほど制約が強い = モデルが単純になる sklearnのデフォルトは1.0 何が良いかはデータ次第で、自動的には調整されない（後で多分チューニング方法が出て来る）。 ### alphaを10倍にしてみる パラメータはオブジェクト生成時に指定 ridge10 = Ridge(alpha = 10).fit(X_train, y_train) print(ridge10.score(X_train, y_train)) ## 0.7883461511233252 print(ridge10.score(X_test, y_test)) ### alphaを0.1倍にしてみる パラメータはオブジェクト生成時に指定 ## 0.6358967327447733 ridge01 = Ridge(alpha = .1).fit(X_train, y_train) print(ridge01.score(X_train, y_train)) ## 0.9285782082010734 print(ridge01.score(X_test, y_test)) ## 0.7717933688844941 \\(\\alpha\\)の大きさと係数の関係をプロットしてみる。\\(\\alpha\\)が大きいほど係数の絶対値は小さくなるはず… plt.plot(ridge.coef_, &#39;s&#39;, label=&quot;Ridge alpha=1&quot;) plt.plot(ridge10.coef_, &#39;^&#39;, label=&quot;Ridge alpha=10&quot;) plt.plot(ridge01.coef_, &#39;v&#39;, label=&quot;Ridge alpha=0.1&quot;) plt.plot(lr.coef_, &#39;o&#39;, label=&quot;LinearRegression&quot;) plt.xlabel(&quot;係数のインデックス&quot;) plt.ylabel(&quot;係数の値&quot;) plt.hlines(0, 0, len(lr.coef_)) plt.ylim(-25, 25) plt.legend() データサイズを増やしていくとスコアはどのように変化するか？ 学習曲線 (learning curve): モデルの性能をデータセットサイズとの関係で表したもの。 リッジ回帰は正則化の影響で常に線形回帰より訓練データへの適合が低い。 テストセットへの適合はデータセットサイズが小さいうちはリッジ回帰の方が優れる。 データセットサイズが大きくなると、リッジ回帰と線形回帰の差はなくなる。 データセットサイズが大きくなると、(単純なモデルでは)過剰適合することが難しくなる。 mglearn.plots.plot_ridge_n_samples() plt.xlabel(&quot;訓練セットのサイズ&quot;) plt.ylabel(&quot;スコア(R²)&quot;) plt.legend(labels=[&quot;リッジ 訓練セット&quot;, &quot;リッジ テストセット&quot;, &quot;線形回帰 訓練セット&quot;, &quot;線形回帰 テストセット&quot;]) 2.5.4 Lasso Ridgeとは異なる形で係数に制約をかける線形回帰。 L1正則化: L1ノルム、つまり係数の絶対値の和に制約をかける。 いくつかの係数が完全に0になる場合があるという点がRidgeと大きく異なる。 係数が完全に0=係数を除外しているということなので、自動的な変数選択ともみなせる。 変数が減ればモデルを解釈しやすくなるという利点もある。 Lassoをboston_housingに適用する。 from sklearn.linear_model import Lasso lasso = Lasso().fit(X_train, y_train) print(&quot;訓練データスコア: {:.2f}&quot;.format(lasso.score(X_train, y_train))) ## 訓練データスコア: 0.29 print(&quot;テストデータスコア: {:.2f}&quot;.format(lasso.score(X_test, y_test))) ## テストデータスコア: 0.21 print(&quot;選択された特徴量数: {}&quot;.format(np.sum(lasso.coef_ != 0))) ## 選択された特徴量数: 4 スコアが非常に悪いのは、パラメータを全くチューニングしていないことによる。 Lassoには複雑さの度合いを制御するパラメータalphaがある。alphaのデフォルトは1.0で、小さくするほど複雑なモデルになる。 alphaを手動で減らす際には、合わせてmax_iterを増やしてやる必要がある。 lasso001 = Lasso(alpha = 0.01, max_iter=100000).fit(X_train, y_train) print(&quot;訓練データスコア: {:.2f}&quot;.format(lasso001.score(X_train, y_train))) ## 訓練データスコア: 0.90 print(&quot;テストデータスコア: {:.2f}&quot;.format(lasso001.score(X_test, y_test))) ## テストデータスコア: 0.77 print(&quot;選択された特徴量数: {}&quot;.format(np.sum(lasso001.coef_ != 0))) ## 選択された特徴量数: 33 alphaを小さくしすぎると過剰適合する。 lasso00001 = Lasso(alpha = 0.0001, max_iter=100000).fit(X_train, y_train) print(&quot;訓練データスコア: {:.2f}&quot;.format(lasso00001.score(X_train, y_train))) ## 訓練データスコア: 0.95 print(&quot;テストデータスコア: {:.2f}&quot;.format(lasso00001.score(X_test, y_test))) ## テストデータスコア: 0.64 print(&quot;選択された特徴量数: {}&quot;.format(np.sum(lasso00001.coef_ != 0))) ## 選択された特徴量数: 94 Ridgeでやったように係数の大きさをプロットしてみよう。 plt.plot(lasso.coef_, &#39;s&#39;, label = &quot;Lasso alpha = 1&quot;) plt.plot(lasso001.coef_, &#39;^&#39;, label = &quot;Lasso alpha = 0.01&quot;) plt.plot(lasso00001.coef_, &#39;v&#39;, label = &quot;Lasso alpha = 0.0001&quot;) plt.plot(ridge01.coef_, &#39;o&#39;, label = &quot;Ridge alpha = 0.1&quot;) plt.legend(ncol = 2, loc = (0, 1.05)) plt.ylim = (-25, 25) plt.xlabel(&quot;係数のインデックス&quot;) plt.ylabel(&quot;係数の大きさ&quot;) 合わせてプロットしたRidge(\\(\\alpha=0.1\\))は、Lasso(\\(\\alpha=0.01\\))と同じくらいの性能であるが、Ridgeでは大きさが小さいながらも係数の値は0にはなっていないものが多いのに対して、Lassoでは大きさが0の係数が目立つ。 実際にはまずRidgeを試すと良い。 係数がたくさんあって重要なのはそのうちの幾つか少数であると予想されるのであれば、Lassoを試すと良い。 RidgeとLassoのペナルティを組合せたものとしてElasticNetがある。結果は良好であるが、チューニングすべきパラメータが増えるという欠点がある。 2.5.5 クラス分類のための線形モデル 線形モデルでクラス分類を行う場合は以下の式を用いる。 \\[\\hat{y} = w[0]\\times x[0] + w[1]\\times x[1] + \\dots + w[p]\\times x[p] + b &gt; 0\\] 出力\\(y\\)が0を超えるかどうかで判別する。 出力\\(y\\)は特徴量の線形関数であり、2つのクラスを直線や平面、超平面で分割する決定境界となる。 線形モデルを学習するアルゴリズムは以下の観点から分類される。 どのような尺度で訓練データへの適合度を測るか。 正則化を行うか。行うならどのような方法か。 ロジスティック回帰と線形サポートベクターマシンは一般的な線形クラスアルゴリズムである。 LogisticRegressionとLinearSVCによりforgeを分類する決定境界を可視化する。 from sklearn.linear_model import LogisticRegression from sklearn.svm import LinearSVC X, y = mglearn.datasets.make_forge() fig, axes = plt.subplots(1, 2, figsize = (10, 3)) for model, ax in zip([LinearSVC(), LogisticRegression()], axes): clf = model.fit(X, y) mglearn.plots.plot_2d_separator(clf, X, fill = False, eps = 0.5, ax = ax, alpha = 0.7) mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax = ax) ax.set_title(&quot;{}&quot;.format(clf.__class__.__name__)) ax.set_xlabel(&quot;特徴量 0&quot;) ax.set_ylabel(&quot;特徴量 1&quot;) axes[0].legend() 2つのクラス分類器はいずれも正則化パラメータCを持つ。Cは大きいほど正則化が弱くなる。 Cがは小さいとデータポイントの多数派に適合しようとするが、大きくすると個々のデータポイントを正確に分類しようとする。 mglearn.plots.plot_linear_svc_regularization() 上記の例では、Cを大きくすると誤分類した少数の点に決定境界が大きく影響されていることがわかる。 低次元の場合は線形分類は制約が強いように思えるが、次元数が大きくなるとモデルは強力になり、むしろ過剰適合をいかに避けるかがポイントになる。 cancerにLogisticRegressionを適用してみる。 from sklearn.datasets import load_breast_cancer cancer = load_breast_cancer() X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify = cancer.target, random_state = 42 ) logreg = LogisticRegression().fit(X_train, y_train) print(&quot;テストセットスコア: {:.3f}&quot;.format(logreg.score(X_train, y_train))) ## テストセットスコア: 0.953 print(&quot;訓練セットスコア: {:.3f}&quot;.format(logreg.score(X_test, y_test))) ## 訓練セットスコア: 0.958 訓練セットとテストセットのスコアが近い場合は適合不足を疑う。 パラメータCを大きくしてモデルの複雑さを上げる。 logreg100 = LogisticRegression(C=100).fit(X_train, y_train) print(&quot;テストセットスコア: {:.3f}&quot;.format(logreg100.score(X_train, y_train))) ## テストセットスコア: 0.967 print(&quot;訓練セットスコア: {:.3f}&quot;.format(logreg100.score(X_test, y_test))) ## 訓練セットスコア: 0.965 精度が上がった。今度は逆にパラメータCを小さくしてみる。 logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train) print(&quot;テストセットスコア: {:.3f}&quot;.format(logreg001.score(X_train, y_train))) ## テストセットスコア: 0.934 print(&quot;訓練セットスコア: {:.3f}&quot;.format(logreg001.score(X_test, y_test))) ## 訓練セットスコア: 0.930 精度が下がった。最後に、3つのパターンについて係数を可視化してみる。 plt.plot(logreg.coef_.T, &#39;o&#39;, label = &quot;C=1&quot;) plt.plot(logreg100.coef_.T, &#39;^&#39;, label = &quot;C=100&quot;) plt.plot(logreg001.coef_.T, &#39;v&#39;, label = &quot;C=0.01&quot;) plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90) plt.hlines(0, 0, cancer.data.shape[1]) plt.xlabel(&quot;特徴量&quot;) plt.ylabel(&quot;係数の大きさ&quot;) plt.legend() デフォルトではLogisticRegressionはL2正則化を行う。 penalty=&quot;l1&quot;の指定でL1正則化に切り替えることができる。より単純なモデルが欲しければこちらを試すと良い。 for C, marker in zip([0.001, 1, 100], [&#39;o&#39;, &#39;^&#39;, &#39;v&#39;]): lr_l1 = LogisticRegression(C = C, penalty = &quot;l1&quot;).fit(X_train, y_train) print(&quot;訓練セットに対する精度(C={:.3f}): {:.2f}&quot;.format(C, lr_l1.score(X_train, y_train))) print(&quot;テストセットに対する精度(C={:.3f}): {:.2f}&quot;.format(C, lr_l1.score(X_test, y_test))) plt.plot(lr_l1.coef_.T, marker, label = &quot;C={:.3f}&quot;.format(C)) ## 訓練セットに対する精度(C=0.001): 0.91 ## テストセットに対する精度(C=0.001): 0.92 ## 訓練セットに対する精度(C=1.000): 0.96 ## テストセットに対する精度(C=1.000): 0.96 ## 訓練セットに対する精度(C=100.000): 0.99 ## テストセットに対する精度(C=100.000): 0.98 plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation = 90) plt.hlines(0, 0, cancer.data.shape[1]) plt.xlabel(&quot;特徴量&quot;) plt.ylabel(&quot;係数の大きさ&quot;) plt.legend(loc = 3) 2.5.6 線形モデルによる多クラス分類 大抵の線形クラス分類は2クラス分類にしか対応しておらず、そのままでは多クラスに拡張することはできない。 ロジスティック回帰は例外 拡張するための方法として1対その他(one-vs.-rest)アプローチがある。 1つのクラスとその他のクラスという2クラス分類に対してモデルを学習させる。 データポイントに対しては全ての2クラス分類を実行する。 一番高いスコアのクラス分類器の分類結果を予測結果とする。 クラスごとに2クラス分類が存在するということなので、クラスごとに以下の式で表す確信度が存在し、確信度が最も大きいクラスがクラスラベルとなる。 \\[ w[0] \\times x[0] + w[1] \\times x[1] + \\dots + w[p] \\times x[p] + b\\] 多クラスロジスティック回帰と1対多アプローチは多少異なるが、1クラスあたり係数ベクトルと切片ができるという点は共通している。 3クラス分類に対して1対多アプローチを試す。データはガウス分布からサンプリングした2次元データセットとする。 from sklearn.datasets import make_blobs X, y = make_blobs(random_state = 42) mglearn.discrete_scatter(X[:, 0], X[:, 1], y) plt.xlabel(&quot;特徴量0&quot;) plt.ylabel(&quot;特徴量1&quot;) plt.legend([&quot;クラス0&quot;, &quot;クラス1&quot;, &quot;クラス2&quot;]) このデータセットでLinearSVCを学習させる。 linear_svm = LinearSVC().fit(X, y) print(&quot;係数ベクトルの形状&quot;, linear_svm.coef_.shape) ## 係数ベクトルの形状 (3, 2) print(&quot;切片ベクトルの形状&quot;, linear_svm.intercept_.shape) ## 切片ベクトルの形状 (3,) 係数ベクトルの形状が3行2列ということは、各行に各クラスに対応する2次元の係数ベクトルが格納されているということである。 切片ベクトルはクラスの数に対応している。 上記2点をまとめると、3つのクラス分類器が得られているということである。 3つのクラス分類器が作る決定境界を可視化する。 mglearn.discrete_scatter(X[:, 0], X[:, 1], y) line = np.linspace(-15, 15) for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, [&#39;b&#39;, &#39;r&#39;, &#39;g&#39;]): plt.plot(line, -(line * coef[0] + intercept) / coef[1], c = color) plt.xlabel(&quot;特徴量0&quot;) plt.ylabel(&quot;特徴量1&quot;) plt.legend([&#39;クラス0&#39;, &#39;クラス1&#39;, &#39;クラス2&#39;, &#39;クラス0の決定境界&#39;, &#39;クラス1の決定境界&#39;, &#39;クラス2の決定境界&#39;], loc = (1.01, 0.3)) 決定境界が作る領域の中には複数のクラスが属する部分(左、右上、右下の三角形領域)と、すべてのクラスが属さない部分(中央の三角)がある。この内部では、クラス分類式の値が一番大きいクラスが予測されるクラスとなる。 例えば、中央の三角であれば対応する決定境界が最も近いクラスに分類される。 上記のルールに従って、多クラス分類における最終的な決定境界を示す。 mglearn.plots.plot_2d_classification(linear_svm, X, fill = True, alpha = .7) mglearn.discrete_scatter(X[:, 0], X[:, 1], y) line = np.linspace(-15, 15) for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, [&#39;b&#39;, &#39;r&#39;, &#39;g&#39;]): plt.plot(line, -(line * coef[0] + intercept) / coef[1], c = color) plt.legend([&quot;クラス0&quot;, &quot;クラス1&quot;, &quot;クラス2&quot;, &quot;クラス0の決定境界&quot;, &quot;クラス1の決定境界&quot;, &quot;クラス2の決定境界&quot;], loc = (1.01, 0.3)) plt.xlabel(&quot;特徴量0&quot;) plt.ylabel(&quot;特徴量1&quot;) 2.5.7 利点、欠点、パラメータ 線形モデルの主要なパラメータ 回帰モデル: alpha 大きいと単純なモデル LinearSVCとLogisticRegression: C 小さいと単純なモデル alpha、Cは対数スケールで調整する。 正則化を行う場合はL1かL2かも重要なポイント。 一部のパラメータが重要と予想される: L1 パラメータを限定できるので、モデルを説明しやすくなる。 特にそのようなこだわりがない: L2 線形モデルの利点 訓練、予測ともに高速。 大きなデータセットでも疎なデータセットでも上手く動く。 非常に大きなデータセットへの対処は2通りある。 LogisticRegressionとRidgeにsolver='sag'オプションを指定する。 SGDClassifierクラスとSGDRegressorクラスの利用を検討する。 線形モデルの欠点 予測手法は理解しやすい反面、係数がなぜその値になっているのかは必ずしも自明ではない。 特に係数間に相関がある場合。 "],
["2-6-アルゴリズム3-ナイーブベイズクラス分類器.html", "2.6 アルゴリズム3 ナイーブベイズクラス分類器", " 2.6 アルゴリズム3 ナイーブベイズクラス分類器 線形モデルよりさらに高速に訓練ができる。 汎化性能は劣る場合がある。 scikit-learnに実装されているナイーブベイズクラス分類器は3種。 GaussianNB: 任意の連続値データに適用できる。 BernoulliNB: 2値データを仮定している。 MultinomialNB: カウントデータを仮定している。 カウントデータ…文章中の単語の出現回数など、個々の特徴量の値が何かのカウントであるもの。 BernoulliNBは特徴量毎に非ゼロの場合をカウントする。 import numpy as np X = np.array([[0, 1, 0, 1], [1, 0, 1, 1], [0, 0, 0, 1], [1, 0, 1, 0]]) y = np.array([0, 1, 0, 1]) counts = {} for label in np.unique(y): counts[label] = X[y == label].sum(axis=0) print(&quot;非ゼロの特徴量のカウント:\\n{}&quot;.format(counts)) ## 非ゼロの特徴量のカウント: ## {0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])} MultinominalNBはクラスごとの個々の特徴量の平均値を考慮に入れる。 GaussianNBは平均値に加えて標準偏差も考慮する。 予測式の形は線形モデルと同じになるが、coef_は\\(w\\)とは若干意味が異なる。 2.6.1 利点、欠点、パラメータ MultinomialNBとBernoulliNBは唯一のパラメータとしてalphaを持つ。 alphaを大きくするとモデルの複雑さが減るが、alphaを変化させてもアルゴリズムの性能はそれほど変化しない。しかし、場合によっては精度を多少上げることができる。 GaussianNBは高次元データに対して用いる場合が多い。 MultinomialNBはBernoulliNBより若干性能が良く、特に非ゼロ特徴量が多数ある場合に強い。 "],
["2-7-アルゴリズム4-決定木.html", "2.7 アルゴリズム4 決定木", " 2.7 アルゴリズム4 決定木 このセクションのためにはgraphvizをインストールしておく必要がある。 pip install graphviz以外に、別途OSに応じた方法でgraphvizをインストール。 ubuntuならばsudo apt-get install graphviz。 回帰にも分類にも使える。 Yes/Noで答えられる質問で出来た木を構成する。 mglearn.plots.plot_animal_tree() 2.7.1 決定木の構築 2つの特徴量、2つのクラスを持つデータセットtwo_moonを使用する。 2つの特徴量のなす平面上で2つのクラスが半月を組合せたように分布している。 from sklearn.datasets import make_moons from mglearn.tools import discrete_scatter X, y = make_moons(n_samples=100, noise=0.25, random_state=3) plt.figure() ax = plt.gca() discrete_scatter(X[:, 0], X[:, 1], y, ax=ax) ax.set_xticks(()) ax.set_yticks(()) 木の構築は、データセットの分割の繰り返しである。分割された部分を葉と呼ぶ。 分割によりテストが1段階増える(e.g. X[1]は0.06以上であるか？) 各ステップで分割は情報量が最も多くなるように（最もクラスを分割するように）行われる。 分割はテストによってデータセットが完全に分類できるようになるまで進む。 1つの葉に1種類のクラスや値しか含まない状態になった木を純粋(pure)と呼ぶ。 以下にtwo_moonから純粋な決定木を作成する過程を示す。 for i, max_depth in enumerate([1, 2, 9]): fig, ax = plt.subplots(1, 2, figsize = (12, 4), subplot_kw={&#39;xticks&#39;: (), &#39;yticks&#39;: ()}) tree = mglearn.plot_interactive_tree.plot_tree(X, y, max_depth = max_depth, ax = ax[0]) ax[1].imshow(mglearn.plot_interactive_tree.tree_image(tree)) plt.show() plt.close() 決定木はターゲットがクラスではなく連続値になっても同じように機能するので、回帰にも使える。 2.7.2 決定木の複雑さの制御 純粋になるまで分割を続けるとルールが複雑になりすぎ、容易に過剰適合してしまう。 過剰適合を防ぐ戦略は2つある。 事前枝刈り: 構築過程で木の生成を止める。単に枝刈りとも。 木の深さを制限する方法、葉の最大値を制限する方法、葉に含まれるデータ点の最小数を制限する方法がある。 scikit-learnには事前枝刈りしか実装されていない。 事後枝刈り: 木を構築してから情報量の少ない枝を削除する。 scikit-learnの決定木の実装 回帰: DecisionTreeRegressorクラス 分類: DecisionTreeClassifierクラス 以下ではcancerデータに対して決定木を作成し、枝刈りの効果を確認する。まずはデフォルトの設定で訓練セットに対して木を構築する。デフォルトでは葉が純粋になるまで分類する。 from sklearn.tree import DecisionTreeClassifier cancer = load_breast_cancer() X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state = 42 ) tree = DecisionTreeClassifier(random_state = 0) # 内部でタイブレークの判定に使う乱数を固定している tree.fit(X_train, y_train) print(&quot;訓練セットに対する精度:{:.3f}&quot;.format(tree.score(X_train, y_train))) ## 訓練セットに対する精度:1.000 print(&quot;テストセットに対する精度:{:.3f}&quot;.format(tree.score(X_test, y_test))) ## テストセットに対する精度:0.937 葉が純粋になるまで分割しているので、訓練セットに対する精度は当然1になる。 テストセットに対する制度は線形モデルの例で見た時より若干低い。 次に、枝刈りの例として木の深さを4に固定してみる。 tree = DecisionTreeClassifier(max_depth = 4, random_state = 0) tree.fit(X_train, y_train) print(&quot;訓練セットに対する精度:{:.3f}&quot;.format(tree.score(X_train, y_train))) ## 訓練セットに対する精度:0.988 print(&quot;テストセットに対する精度:{:.3f}&quot;.format(tree.score(X_test, y_test))) ## テストセットに対する精度:0.951 訓練セットに対する精度と引き換えに、汎化性能が向上していることが分かる。 2.7.3 決定木の解析 木の可視化のために、まずはtreeモジュールのexport_graphviz関数でグラフを書き出す。 出力はグラフに対応するファイル形式である.dot形式のファイル。 from sklearn.tree import export_graphviz export_graphviz( tree, out_file = &quot;output/tree.dot&quot;, class_names = [&quot;malignant&quot;, &quot;benign&quot;], feature_names = cancer.feature_names, impurity = False, filled = True ) .dotファイルの可視化はgraphvizモジュールで行う import graphviz from PIL import Image with open(&quot;output/tree.dot&quot;) as f: dot_graph = f.read() g = graphviz.Source(dot_graph) g.format = &quot;png&quot; g.render(&quot;output/tree.gv&quot;) img = np.array(Image.open(&quot;output/tree.gv.png&quot;)) plt.imshow(img) plt.axis(&#39;off&#39;) 可視化した木は多くの情報を含むが、特に大部分のデータが通るパスはどこか？に注目すると良い。 2.7.4 決定木の特徴量の重要性 決定木全体を確認し、把握するのは大変な作業なので、以下のような情報が使用される場合がある。 特徴量の重要度 (feature importance) 個々の特徴量はそれぞれの判断に対してどの程度重要なのか？ 1に近いほど重要。1であればその特徴量だけで完全に判別ができるということ。 0に近いほど重要ではない 特徴量の重要度はフィット済みオブジェクトの.feature_importance_に格納されている。 print(tree.feature_importances_) ## [0. 0. 0. 0. 0. 0. ## 0. 0. 0. 0. 0.01019737 0.04839825 ## 0. 0. 0.0024156 0. 0. 0. ## 0. 0. 0.72682851 0.0458159 0. 0. ## 0.0141577 0. 0.018188 0.1221132 0.01188548 0. ] このままではわかりにくいのでプロットしてみる。 n_features = cancer.data.shape[1] plt.barh(range(n_features), tree.feature_importances_, align = &#39;center&#39;) plt.yticks(np.arange(n_features), cancer.feature_names) plt.xlabel(&quot;特徴量の重要度&quot;) plt.ylabel(&quot;特徴量&quot;) 特徴量の重要度が高い特徴量は重要だが、逆は必ずしも成り立たないという点に注意が必要である。 特徴量間に強い相関があり、いずれかの特徴量で十分説明出来てしまう場合は、残りの特徴量がたまたま採用されないということがありうる。 特徴量の重要度は係数と異なって常に正であり、その特徴量が大きいとクラスがどれになるのかは直接は分からない。 上記の例ではworst radiusは少なくとも重要だが、他に重要な特徴量がある可能性は除外できないし、worst radiusの値と良性・悪性の関係がどのようになっているのかも自明ではない。 そもそも、特徴量とクラスの関係は必ずしも単純とは限らない。例えば次のような2つの特徴量からなる2クラス分類問題を考えてみる。この例は、クラスを分けるルールは単純で明確だが、クラス1はクラス0の中に分布しているので、一定の大小関係だけでは分類できない。 tree = mglearn.plots.plot_tree_not_monotone() ## Feature importances: [0. 1.] tree.format = &quot;png&quot; tree.render(&quot;output/not_monotone.gv&quot;) img = np.array(Image.open(&quot;output/not_monotone.gv.png&quot;)) plt.imshow(img) plt.axis(&#39;off&#39;) 決定木による分類の議論は決定木による回帰にも当てはまる。 決定木による回帰では、外挿 (extrapolate)ができない点に注意する。 決定木は外挿ができないという点について、RAM価格の推移データセットを使って例を示そう。 import os ram_prices = pd.read_csv(os.path.join(mglearn.datasets.DATA_PATH, &quot;ram_price.csv&quot;)) plt.semilogy(ram_prices.date, ram_prices.price) plt.xlabel(&quot;年&quot;) plt.ylabel(&quot;1Mバイトあたりの価格($)&quot;) 関係を直線的にするために、価格を対数変換しているという点に注意してもらいたい。この種の変換は線形回帰を行う際に重要となる。 データセットに対し、線形回帰と回帰木を適用する。ここでは、2000年より前のデータを訓練セットとし、2000年以降のデータをテストセットとする。つまり、過去のデータから将来を予測する。 from sklearn.tree import DecisionTreeRegressor data_train = ram_prices[ram_prices.date &lt; 2000] data_test = ram_prices[ram_prices.date &gt;= 2000] X_train = data_train.date[:, np.newaxis] y_train = np.log(data_train.price) #対数変換 # モデルに訓練データをフィットさせる tree = DecisionTreeRegressor().fit(X_train, y_train) linear_reg = LinearRegression().fit(X_train, y_train) # 2000年以降も含めた全てのデータポイントに対して予測を行う X_all = ram_prices.date[:, np.newaxis] pred_tree = tree.predict(X_all) pred_lr = linear_reg.predict(X_all) price_tree = np.exp(pred_tree) #対数変換を解除 price_lr = np.exp(pred_lr) ## プロット plt.semilogy(data_train.date, data_train.price, label = &quot;訓練データ&quot;) plt.semilogy(data_test.date, data_test.price, label = &quot;テストデータ&quot;) plt.semilogy(ram_prices.date, price_tree, label = &quot;回帰木&quot;) plt.semilogy(ram_prices.date, price_lr, label = &quot;線形回帰&quot;) plt.legend() plt.tight_layout() plt.show() plt.close() 線形回帰は2000年以降の値も予測出来ているのに対して、回帰木は単に2000年の値を返すだけになっている。 2.7.5 長所、短所、パラメータ 決定木のパラメータは事前枝刈りに関するパラメータである。 大抵の場合はmax_depth、max_leaf_nodes、min_samples_leafのいずれか1つの指定で十分である。 決定木は容易に可視化可能であり、理解しやすい。 決定木の分割は特徴量毎に行われるため、特徴量を正規化したり標準化したりする必要はない。 特徴量の最大の欠点は事前枝刈りを行ったとしても過剰適合しやすく、汎化性能が低くなりやすいという点である。 "],
["3-教師あり学習-2.html", "3 教師あり学習 (2)", " 3 教師あり学習 (2) 長くなってきたので章を分けました。 "],
["3-1-アルゴリズム5-決定木のアンサンブル法.html", "3.1 アルゴリズム5 決定木のアンサンブル法", " 3.1 アルゴリズム5 決定木のアンサンブル法 アンサンブル法 (Ensembles): 複数の機械学習モデルを組合せてより強力なモデルを構築する手法。 ランダムフォレストと勾配ブースティングが有名。 3.1.1 ランダムフォレスト 決定木の過剰適合しやすいという欠点に対する対処法の一つ。 少しずつ異なる決定木をたくさん作って、その平均をとるという手法。 木が沢山あるので森 3.1.1.1 ランダムフォレストの構築 "]
]
